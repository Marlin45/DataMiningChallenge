{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4250e169-533f-4d81-9bf1-bb741afffd62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.layers import Dense, LeakyReLU, ReLU, Conv1D\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from imblearn.over_sampling import SMOTE,SVMSMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37dd6331-8ad0-4381-bc36-6f28bc515f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_tests_ensamble(clfs, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "        returns the test auroc\n",
    "    \"\"\"\n",
    "    pas1 = []\n",
    "    pas2 = []\n",
    "    for clf in clfs:\n",
    "        # for X_train, X_test in zip(X_trains, X_tests):\n",
    "        # fit each classifier on each data set variation\n",
    "        t = clf.fit(X_train, y_train)\n",
    "        pas1.append(t.predict_proba(X_test)[:, 1])\n",
    "        pas2.append(t.predict_proba(X_train)[:, 1])\n",
    "\n",
    "    pa1 = np.mean(pas1, axis=0)\n",
    "    pa2 = np.mean(pas2, axis=0)\n",
    "\n",
    "    print(pa1.shape, pa2.shape)\n",
    "\n",
    "    # pred3 = trained.predict(X_val)\n",
    "    # pa3 = trained.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # IMPORTANT JUST SETTING YPRED TO 0 RN CUZ I DONT CARE ABOUT IT, SHOULD FIX LATER\n",
    "    met1 = get_metrics('ensamble' + ' X_test', y_test, [0]*y_test.shape[0], pa1)\n",
    "    met2 = get_metrics('ensamble' + ' X_train', y_train, [0]*y_train.shape[0], pa2)\n",
    "    # met3 = get_metrics('ensamble' + ' X_val', y_val, pred3, pa3)\n",
    "\n",
    "    print(f\"({'ensamble'}) (X_test) accuracy, precision, recall, specificity, AUROC\", met1)\n",
    "    print(f\"({'ensamble'}) (X_train) accuracy, precision, recall, specificity, AUROC\", met2)\n",
    "\n",
    "    return met1[4]\n",
    "    # print(f\"({'ensamble'}) (X_val) accuracy, precision, recall, specificity, AUROC\", met3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d536fb85-86cd-4c28-be94-fe028015bcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics(name, y_test, y_pred, proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fn + fp)\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    specificity = tn / (tn+fp)\n",
    "    roc_auc = roc_auc_score(y_test, proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, proba, pos_label=1)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, precision, recall, specificity, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f2b079b-ba7d-483e-adbb-494a97323b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_demo_data(y_train, y_test):\n",
    "    X = pd.read_csv('train/train_demos.csv')\n",
    "    X = X.set_index('patient_id')\n",
    "    cat = ['gender', 'insurance', 'marital_status', 'ethnicity']\n",
    "    # ONE HOT\n",
    "    enc = OneHotEncoder()\n",
    "    X_encoded = enc.fit_transform(X[cat])\n",
    "    X_encoded = pd.DataFrame.sparse.from_spmatrix(X_encoded)\n",
    "    X_encoded.index = X.index\n",
    "\n",
    "    # LABEL\n",
    "    # encoder = LabelEncoder()\n",
    "    # X_encoded = X.copy()\n",
    "    # for var in cat:\n",
    "    #     X_encoded[var] = encoder.fit_transform(X_encoded[var])\n",
    "\n",
    "    \n",
    "    X = pd.concat([X.drop(cat, axis=1), X_encoded], axis=1)\n",
    "    X['admittime'] = X.apply(lambda x: time.mktime(pd.Timestamp(x['admittime']).timetuple()), axis=1)\n",
    "    X['admittime'] = X['admittime'] - X['admittime'].min()\n",
    "\n",
    "    X_train = split(X, y_train.index)\n",
    "    X_test = split(X, y_test.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[['age', 'admittime']])\n",
    "    X_train[['age', 'admittime']] = scaler.transform(X_train[['age', 'admittime']])\n",
    "    X_test[['age', 'admittime']] = scaler.transform(X_test[['age', 'admittime']])\n",
    "\n",
    "    X_train.columns = X_train.columns.astype('str')\n",
    "    X_test.columns = X_test.columns.astype('str')\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d73a4f57-2db8-4baf-8d69-b387dde4bfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_signs_data(y_train, y_test):\n",
    "    # CURRENTLY DROPS TIME COL\n",
    "    signs = pd.read_csv('train/train_signs.csv')\n",
    "    signs['charttime'] = pd.to_datetime(signs['charttime'])\n",
    "    \n",
    "    \n",
    "    first_time_row = signs.groupby('patient_id')['charttime'].first()\n",
    "    signs['firsttime'] = signs['patient_id'].map(first_time_row)\n",
    "    # Sets the index as the time from the first reading so all patients start at 0 and go toward 24 hours\n",
    "    signs['timediff'] = pd.to_numeric(signs['charttime'] - signs['firsttime'])\n",
    "    signs = signs.drop(['charttime','firsttime'],axis=1)\n",
    "    \n",
    "    aggs = signs.groupby('patient_id').agg(['mean', 'min', 'max', 'first', 'last'])\n",
    "    X_train = split(aggs, y_train.index)\n",
    "    X_test = split(aggs, y_test.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features = X_train.columns\n",
    "    id = X_train.index\n",
    "    id_test = X_test.index\n",
    "\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_train.columns = ['_'.join(x) for x in features]\n",
    "    X_train.index = id\n",
    "    X_test.columns = ['_'.join(x) for x in features]\n",
    "    X_test.index = id_test\n",
    "\n",
    "    # columns with more than 10% null values, drop these (10 columns, 2 metrics)\n",
    "    drop_cols = X_train.columns[X_train.isna().sum() / X_train.shape[0] > .1] # should be just train set\n",
    "    X_train = X_train.drop(columns=drop_cols)\n",
    "    X_test = X_test.drop(columns=drop_cols)\n",
    "\n",
    "    # is this the best way to do it??\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_train.mean()) # note train mean\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4bba4e8-9b9b-4550-9492-6324e734097e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#UNUSED\n",
    "def DAE_dimensionality_reducer(X_train,X_test,dim=30):\n",
    "    \"\"\"\n",
    "        Reduces the dimension of X to whatever dim is given using Denoising Auto Encoder, this can lead to worse performance if dim is choosen poorly\n",
    "    \"\"\"\n",
    "    # Specify how much noise to add\n",
    "    level_of_noise=0.0\n",
    "\n",
    "    # Add random noise based on sampling from Gaussian distribution\n",
    "    X_train_noisy = X_train + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "    X_test_noisy = X_test + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    # Defines network layers encoder - middle - decoder, middle is the undercomplete layer that should be sampled \n",
    "    visible = Input(shape=(input_size,), name='Input-Layer') # Specify input shape\n",
    "    e = Dense(units=input_size, name='Encoder-Layer')(visible)\n",
    "    e = LeakyReLU(name='Encoder-Layer-Activation')(e)\n",
    "    m = Dense(units=dim, activation='linear', kernel_regularizer=keras.regularizers.L1L2(l1=1e-1,l2=1e-1), name='Middle-Hidden-Layer')(e)\n",
    "    d = Dense(units=input_size, name='Decoder-Layer')(m)\n",
    "    d = LeakyReLU(name='Decoder-Layer-Activation')(d)\n",
    "    output = Dense(units=input_size, activation='sigmoid', name='Output-Layer')(d)\n",
    "\n",
    "    # Define denoising autoencoder model\n",
    "    model = Model(inputs=visible, outputs=output, name='Denoising-Autoencoder-Model')\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    # Compile denoising autoencoder model\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "\n",
    "    # Fit the Denoising autoencoder model to reconstruct original images\n",
    "    history = model.fit(X_train_noisy, X_train, epochs=40, batch_size=32, verbose=0, validation_split=0.2)\n",
    "\n",
    "    # Plot a loss chart\n",
    "    display(model.summary())\n",
    "    fig, ax = plt.subplots(figsize=(16,9), dpi=300)\n",
    "    plt.title(label='DAE Model loss by Epoch', loc='center')\n",
    "    ax.plot(history.history['loss'], label='Training Data', color='black')\n",
    "    ax.plot(history.history['val_loss'], label='Test Data', color='red')\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "    plt.xticks(ticks=np.arange(len(history.history['loss'])), labels=np.arange(1, len(history.history['loss'])+1))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Once model is trained we sample the middle layer as that is the lower dimensional representation\n",
    "    dimensionality_reducing_model = Model(inputs=model.input, outputs=[model.layers[3].output])\n",
    "    X_train_reduced = dimensionality_reducing_model.predict(X_train)\n",
    "\n",
    "    return pd.DataFrame(dimensionality_reducing_model.predict(X_train),index=X_train.index),  pd.DataFrame(dimensionality_reducing_model.predict(X_test),index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5859375d-00b2-4a4d-b993-c8e097f0f2a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNUSED\n",
    "def RNN_DAE_dimensionality_reducer(X_train,X_test,dim=30,epochs=300):\n",
    "    \"\"\"\n",
    "        Reduces the dimension of X to whatever dim is given using Denoising Auto Encoder, this can lead to worse performance if dim is choosen poorly\n",
    "    \"\"\"\n",
    "    # Specify how much noise to add\n",
    "    level_of_noise=0.5\n",
    "\n",
    "    # Add random noise based on sampling from Gaussian distribution\n",
    "    X_train_noisy = X_train + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "\n",
    "    timesteps = X_train.shape[1]\n",
    "    input_dim = X_train.shape[2] \n",
    "    latent_dim = dim\n",
    "\n",
    "    inputs = keras.Input(shape=(timesteps, input_dim))\n",
    "    encoded = layers.LSTM(latent_dim)(inputs)\n",
    "    middle = layers.Dense(latent_dim)(encoded)\n",
    "\n",
    "    decoded = layers.RepeatVector(timesteps)(middle)\n",
    "    decoded = layers.LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "    sequence_autoencoder = keras.Model(inputs, decoded,name='RNN-DAE')\n",
    "    encoder = keras.Model(inputs, middle,name=\"Encoder\")\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    sequence_autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "    history = sequence_autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=128, verbose=1, validation_split=0.2)\n",
    "    \n",
    "    # Plot a loss chart\n",
    "    display(sequence_autoencoder.summary())\n",
    "    fig, ax = plt.subplots(figsize=(16,9), dpi=300)\n",
    "    plt.title(label='DAE Model loss by Epoch', loc='center')\n",
    "    ax.plot(history.history['loss'], label='Training Data', color='black')\n",
    "    ax.plot(history.history['val_loss'], label='Test Data', color='red')\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "    plt.xticks(ticks=np.arange(len(history.history['loss'])), labels=np.arange(1, len(history.history['loss'])+1))\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return encoder.predict(X_train), encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d609843-14fb-419f-9bb9-e96ffc56b540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test(balance_train=False):\n",
    "    # returns the train test split labels\n",
    "    y = pd.read_csv('train/train_labels.csv')\n",
    "    y = y.set_index('patient_id')\n",
    "    y_train = y.sample(n=int(y.shape[0] * .8))\n",
    "    y_test = y.drop(y_train.index)\n",
    "    if balance_train:\n",
    "        # RUN THIS CELL IF YOU WANT AN EVEN DATA CLASS BALANCE\n",
    "        keep_ids = y_train[y_train['label'] == 0].sample(n=y_train['label'].sum()).index\n",
    "        ys = y_train[y_train['label'] == 1].index\n",
    "        y_train = y_train.loc[[*keep_ids, *ys]]\n",
    "    return y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcf0703e-8c98-4cc7-9d81-19b58cd9a440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_test_create_samples(balance_train=False):\n",
    "    # returns the train test split labels\n",
    "    y = pd.read_csv('train/train_labels.csv')\n",
    "    y = y.set_index('patient_id')\n",
    "    y_train = y.sample(n=int(y.shape[0] * .8))\n",
    "    y_test = y.drop(y_train.index)\n",
    "    if balance_train:\n",
    "        num_ones = y_train[y_train['label'] == 1].count()\n",
    "        num_zeroes = y_train[y_train['label'] == 0].count()\n",
    "        \n",
    "        y_train = pd.concat([y_train,y_train[y_train['label'] == 1].sample(n=(num_zeroes-num_ones).iloc[0],replace=True)])\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9787675c-c16c-4c36-afb9-a86f3d0deaba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split(X, index):\n",
    "    return X.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f0bc61e-7449-4dd3-9945-d83bab54df2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_radiology_data(y_train, y_test):\n",
    "    df = pd.read_csv('train/train_radiology.csv')\n",
    "    df = df.groupby('patient_id').agg({'text': ['sum']})\n",
    "\n",
    "    \n",
    "    X_train = split(df, y_train.index)\n",
    "    X_test = split(df, y_test.index)    \n",
    "    \n",
    "    # vectorizer = \n",
    "    # (sublinear_tf=True, max_df=.5, min_df=5, max_features=100, stop_words=\"english\")\n",
    "    vec_train = vectorizer.fit_transform(X_train['text']['sum'])\n",
    "    vec_test = vectorizer.transform(X_test['text']['sum'])\n",
    "    X_train = pd.concat([X_train.drop(columns=['text']), pd.DataFrame(vec_train.toarray(), index=y_train.index)], axis=1)\n",
    "    X_test = pd.concat([X_test.drop(columns=['text']), pd.DataFrame(vec_test.toarray(), index=y_test.index)], axis=1)\n",
    "    \n",
    "    # X_train[('charttime','first')] = pd.to_datetime(X_train[('charttime','first')])\n",
    "    # X_train[('charttime','last')] = pd.to_datetime(X_train[('charttime','last')])\n",
    "    # X_test[('charttime','first')] = pd.to_datetime(X_test[('charttime','first')])\n",
    "    # X_test[('charttime','last')] = pd.to_datetime(X_test[('charttime','last')])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features = X_train.columns\n",
    "    id = X_train.index\n",
    "    id_test = X_test.index\n",
    "\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_train.columns = features\n",
    "    X_train.index = id\n",
    "    X_test.columns = features\n",
    "    X_test.index = id_test\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b5442d6-4006-49ac-9a2d-f4fd9b8e82ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sign_data_to_rnn(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe of sign data to a numpy array grouped by patients * cols -> patients * timestep * cols\n",
    "    \"\"\"\n",
    "    df['charttime'] = pd.to_datetime(df['charttime'])\n",
    "\n",
    "    #Creates new column 'firsttime' which is the time of first row for each patient\n",
    "    first_time_row = df.groupby('patient_id')['charttime'].first()\n",
    "    df['firsttime'] = df['patient_id'].map(first_time_row)\n",
    "    # Sets the index as the time from the first reading so all patients start at 0 and go toward 24 hours\n",
    "    df = df.set_index(df['charttime'] - df['firsttime'])\n",
    "    df = df.drop(['charttime','firsttime'],axis=1)\n",
    "    # Resamples data so all patients have exactly 24 hours\n",
    "    df = df.groupby('patient_id').resample('h').mean()\n",
    "    df = df.reindex(pd.MultiIndex.from_product([df.index.levels[0],pd.timedelta_range(start='00:00:00', end='23:00:00', freq='6h')]))\n",
    "    df = df.groupby(level=['patient_id']).ffill().bfill()\n",
    "    # Fills NA with zero, na here means a patient that never had a certain measurment taken\n",
    "    df[df.isna()] = 0\n",
    "    \n",
    "    num_patients = len(df.index.levels[0])\n",
    "    num_cols = len(df.columns)\n",
    "    d = np.array(df)\n",
    "    index = df.index.levels[0]\n",
    "    # patients * cols (30) * hours (24)\n",
    "    d=np.reshape(d,(num_patients,num_cols,-1))\n",
    "    d = np.swapaxes(d,1,2)\n",
    "    return d,index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7e72033-0960-42b4-a538-06c10d7988d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rnn_prep_signs(y_train,y_test):\n",
    "    \"\"\"\n",
    "    reads train_signs and turns it into time series data with 30 columns and 24 hours for each person\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv('train/train_signs.csv')\n",
    "    df = df.set_index('patient_id')\n",
    "    \n",
    "    X_train = split(df, y_train.index).reset_index()\n",
    "    X_test = split(df, y_test.index).reset_index()\n",
    "    X_train,index_train = sign_data_to_rnn(X_train)\n",
    "    X_test,index_test = sign_data_to_rnn(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    timesteps = X_train.shape[1]\n",
    "    dim = X_train.shape[2]\n",
    "    X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0]*X_train.shape[1],-1))\n",
    "    X_test = scaler.transform(X_test.reshape(-1,dim))\n",
    "\n",
    "\n",
    "    X_train = X_train.reshape(-1,timesteps,dim)\n",
    "    X_test = X_test.reshape(-1,timesteps,dim)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e4b1eb0-ab1c-4c7b-a043-95e46b5c03a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6367686627493092"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts, simple_preprocess\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def prep_rad_doc_vec(y_train,y_test):\n",
    "    df = pd.read_csv('train/train_radiology.csv')\n",
    "    df = df.groupby('patient_id').agg({'text': ['sum']})\n",
    "    X_train = split(df, y_train.index)\n",
    "    X_test = split(df, y_test.index)    \n",
    "    train_corpus = [TaggedDocument(simple_preprocess(doc), [X_train.index[i]]) for i, doc in enumerate(X_train['text']['sum'])]\n",
    "    model = Doc2Vec(train_corpus, vector_size=30, window=1, min_count=100, workers=4,epochs=100,dm=1)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus,total_examples=model.corpus_count,epochs=model.epochs)\n",
    "    keys = model.dv.index_to_key\n",
    "    X_train = pd.DataFrame({key : model.dv[key] for key in keys}).transpose()\n",
    "    X_test = pd.DataFrame({patient_id:model.infer_vector(simple_preprocess(X_test['text']['sum'].loc[patient_id])) for i,patient_id in enumerate(X_test.index)}).transpose()\n",
    "    return X_train,X_test\n",
    "\n",
    "X_train, X_test = prep_rad_doc_vec(y_train,y_test)\n",
    "# rad,rad_test = prep_radiology_data(y_train,y_test)\n",
    "# X_train = pd.concat([rad_train,X_train], axis=1)\n",
    "# X_test = pd.concat([rad_test,X_test], axis=1)\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train,y_train)\n",
    "\n",
    "roc_auc_score(y_test,svc.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1a8ed-a936-44f0-9c2f-efc786001b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_test(balance_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "649f5d0b-06b5-491d-b919-825da7ec44da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rad_train, rad_test \u001b[38;5;241m=\u001b[39m prep_radiology_data(y_train, y_test)\n\u001b[1;32m      2\u001b[0m rad_doc_train,rad_doc_test \u001b[38;5;241m=\u001b[39m prep_rad_doc_vec(y_train,y_test)\n\u001b[1;32m      3\u001b[0m demo_train, demo_test \u001b[38;5;241m=\u001b[39m prep_demo_data(y_train, y_test)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mprep_radiology_data\u001b[0;34m(y_train, y_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test \u001b[38;5;241m=\u001b[39m split(df, y_test\u001b[38;5;241m.\u001b[39mindex)    \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# vectorizer = \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# (sublinear_tf=True, max_df=.5, min_df=5, max_features=100, stop_words=\"english\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m vec_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m vec_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), pd\u001b[38;5;241m.\u001b[39mDataFrame(vec_train\u001b[38;5;241m.\u001b[39mtoarray(), index\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mindex)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "rad_train, rad_test = prep_radiology_data(y_train, y_test)\n",
    "rad_doc_train,rad_doc_test = prep_rad_doc_vec(y_train,y_test)\n",
    "demo_train, demo_test = prep_demo_data(y_train, y_test)\n",
    "sign_train,sign_test = prep_signs_data(y_train,y_test)\n",
    "\n",
    "X_train = pd.concat([rad_train,sign_train, demo_train,rad_doc_train], axis=1)\n",
    "X_test = pd.concat([rad_test,sign_test,  demo_test,rad_doc_test], axis=1)\n",
    "X_just_rad_train = rad_train\n",
    "X_jst_rad_test = rad_test\n",
    "X_train.columns = X_train.columns.astype('str')\n",
    "X_test.columns = X_test.columns.astype('str')\n",
    "print(rad_train.shape,demo_train.shape,sign_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "52a937dd-4321-4524-acc7-83bf8b10f6fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Doesn't improve performance, we will continue undersampling\n",
    "# sm = SVMSMOTE(random_state=42)\n",
    "# from imblearn.over_sampling import ADASYN\n",
    "# sm = ADASYN(random_state=42)\n",
    "# X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "# X_train_sm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ecfa1ae2-e9d3-4728-af9f-d6596eb8cdf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2742,) (2064,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/var/folders/qd/q50f4_v16n7fp25tfl78t3ph0000gn/T/ipykernel_99467/1136690420.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (tp+fp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/q50f4_v16n7fp25tfl78t3ph0000gn/T/ipykernel_99467/1136690420.py:4: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision = tp / (tp+fp)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ensamble) (X_test) accuracy, precision, recall, specificity, AUROC (0.9008023340627279, nan, 0.0, 1.0, 0.8430682900690641)\n",
      "(ensamble) (X_train) accuracy, precision, recall, specificity, AUROC (0.5, nan, 0.0, 1.0, 0.9533156060332915)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8430682900690641"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# svm\n",
    "svc = svm.SVC(kernel='rbf', probability=True)\n",
    "logreg = LogisticRegression()\n",
    "# run_tests(svc, 'SVC', X_train, y_train, X_test, y_test)\n",
    "# run_tests(logreg, 'Logistic', X_train, y_train, X_test, y_test)\n",
    "\n",
    "run_tests_ensamble([svc, logreg], X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47efded-6bd9-4687-b522-00f822a350e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d336b91-75bf-4ca5-b5cf-9f9610e24eaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sign_train, sign_test \u001b[38;5;241m=\u001b[39m prep_signs_data(y_train, y_test)\n\u001b[0;32m----> 2\u001b[0m rad_train, rad_test \u001b[38;5;241m=\u001b[39m prep_radiology_data(y_train, y_test)\n\u001b[1;32m      3\u001b[0m demo_train, demo_test \u001b[38;5;241m=\u001b[39m prep_demo_data(y_train, y_test)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sign_train\u001b[38;5;241m.\u001b[39mshape,rad_train\u001b[38;5;241m.\u001b[39mshape,demo_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m, in \u001b[0;36mprep_radiology_data\u001b[0;34m(y_train, y_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test \u001b[38;5;241m=\u001b[39m split(df, y_test\u001b[38;5;241m.\u001b[39mindex)    \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# vectorizer = \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# (sublinear_tf=True, max_df=.5, min_df=5, max_features=100, stop_words=\"english\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m vec_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m vec_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), pd\u001b[38;5;241m.\u001b[39mDataFrame(vec_train\u001b[38;5;241m.\u001b[39mtoarray(), index\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mindex)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sign_train, sign_test = prep_signs_data(y_train, y_test)\n",
    "rad_train, rad_test = prep_radiology_data(y_train, y_test)\n",
    "demo_train, demo_test = prep_demo_data(y_train, y_test)\n",
    "print(sign_train.shape,rad_train.shape,demo_train.shape)\n",
    "# rad_train, rad_test = DAE_dimensionality_reducer(rad_train,rad_test,dim=100)\n",
    "\n",
    "X_train = pd.concat([sign_train, rad_train, demo_train], axis=1)\n",
    "X_test = pd.concat([sign_test, rad_test, demo_test], axis=1)\n",
    "\n",
    "X_train.columns = X_train.columns.astype('str')\n",
    "X_test.columns = X_test.columns.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d9e1ef2-c000-4318-9ffe-da30c8c6c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN(X_train,y_train,neurons):\n",
    "    input_size = X_train.shape[1]\n",
    "\n",
    "    model = keras.Sequential([\n",
    "            layers.Dense(input_size, activation=\"tanh\", name='Input-Layer'),\n",
    "            layers.Dense(units=96,activation=\"tanh\", kernel_regularizer=keras.regularizers.L1L2(l1=1e-2, l2=1e-2),name='Hidden-Layer-1'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(units=1,activation='sigmoid',name='Output')\n",
    "    ])\n",
    "\n",
    "    for lr in [1e-4,1e-6]:\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        callbacks = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5) \n",
    "        model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(),metrics=[keras.metrics.AUC()])\n",
    "        model.fit(X_train,y_train,validation_split=0.1,epochs=1000,callbacks=[callbacks])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b48277e9-26ee-475f-b92d-53e11177dff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_k_NN(k,X,y):\n",
    "    \"\"\"\n",
    "    Trains k neural networks over different folds of the provided training data\n",
    "    \n",
    "    Returns a list of trained models, and saves them to files at models/model{i}.keras where i is the model number\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for i in range(k):\n",
    "        y_train = y.sample(n=int(y.shape[0] * .9))\n",
    "        y_val = y.drop(y_train.index)\n",
    "        X_train = X.loc[y_train.index]\n",
    "        X_val= X.loc[y_val.index]\n",
    "        input_size = X_train.shape[1]\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(input_size, activation=\"tanh\", name='Input-Layer'),\n",
    "            layers.Dense(units=96,activation=\"tanh\", kernel_regularizer=keras.regularizers.L1L2(l1=1e-2, l2=1e-2),name='Hidden-Layer-1'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(units=1,activation='sigmoid',name='Output')\n",
    "        ])\n",
    "        for lr in [1e-4,1e-5,1e-6]:\n",
    "            opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "            callbacks = keras.callbacks.EarlyStopping(monitor='val_loss',patience=5) \n",
    "            model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(),metrics=[keras.metrics.AUC()])\n",
    "            model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=1000,callbacks=[callbacks],verbose=0)\n",
    "            model.save(f'models/model{i}.keras')\n",
    "        models.append(model)\n",
    "    return models\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b194825c-7f5f-4e23-a05b-2e2bfbdaea65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = train_k_NN(10,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f9bea584-b7fb-4f70-bfca-1eec4524fb17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN0:  0.8480960103996754\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN1:  0.8438493726451841\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN2:  0.8477850153242822\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN3:  0.8488952677434353\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 990us/step\n",
      "NN4:  0.8436161263386394\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN5:  0.847539329214722\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step\n",
      "NN6:  0.8435725870280846\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN7:  0.8458692856598616\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step\n",
      "NN8:  0.8457355577774426\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "NN9:  0.8475175595594445\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    model = keras.models.load_model(f'models/model{k}.keras')\n",
    "    nn_predict = model.predict(X_test)\n",
    "    print(f\"NN{k}: \",roc_auc_score(y_test,nn_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5ca6d98-f554-48f8-b343-aa1c0b2118ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_test(balance_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "211b3747-4bb3-4f9a-b09a-20f83d214dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rad_train, rad_test \u001b[38;5;241m=\u001b[39m prep_radiology_data(y_train, y_test)\n\u001b[1;32m      2\u001b[0m demo_train, demo_test \u001b[38;5;241m=\u001b[39m prep_demo_data(y_train, y_test)\n\u001b[1;32m      3\u001b[0m sign_train,sign_test \u001b[38;5;241m=\u001b[39m prep_signs_data(y_train,y_test)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mprep_radiology_data\u001b[0;34m(y_train, y_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m X_test \u001b[38;5;241m=\u001b[39m split(df, y_test\u001b[38;5;241m.\u001b[39mindex)    \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# vectorizer = \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# (sublinear_tf=True, max_df=.5, min_df=5, max_features=100, stop_words=\"english\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m vec_train \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m vec_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]), pd\u001b[38;5;241m.\u001b[39mDataFrame(vec_train\u001b[38;5;241m.\u001b[39mtoarray(), index\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mindex)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "rad_train, rad_test = prep_radiology_data(y_train, y_test)\n",
    "demo_train, demo_test = prep_demo_data(y_train, y_test)\n",
    "sign_train,sign_test = prep_signs_data(y_train,y_test)\n",
    "\n",
    "\n",
    "X_train = pd.concat([rad_train,sign_train, demo_train], axis=1)\n",
    "X_test = pd.concat([rad_test,sign_test,  demo_test], axis=1)\n",
    "\n",
    "# X_train,X_test = DAE_dimensionality_reducer(X_train,X_test)\n",
    "\n",
    "X_train.columns = X_train.columns.astype('str')\n",
    "X_test.columns = X_test.columns.astype('str')\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0fc3998-549d-4a73-89a6-85bb501999a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC()])\n\u001b[0;32m---> 15\u001b[0m     history\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mfit(X_train,y_train,validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,callbacks\u001b[38;5;241m=\u001b[39m[callbacks]))\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# input_size = X_train.shape[1]\n",
    "history = []\n",
    "model = keras.Sequential([\n",
    "        # layers.Dense(256, activation=\"tanh\", name='Input-Layer', kernel_regularizer=keras.regularizers.L1L2(l1=1e-2,l2=1e-2)),\n",
    "        # layers.Dropout(0.5),\n",
    "        layers.Dense(units=90,activation=\"tanh\",name='Hidden-Layer-1',kernel_regularizer=keras.regularizers.L1L2(l1=1e-3,l2=1e-3)),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(units=1,activation='sigmoid',name='Output')\n",
    "])\n",
    "\n",
    "for lr in [1e-4,1e-5,1e-6]:\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    callbacks = keras.callbacks.EarlyStopping(monitor='val_loss',patience=3) \n",
    "    model.compile(optimizer=opt, loss=keras.losses.BinaryCrossentropy(),metrics=[keras.metrics.AUC()])\n",
    "    history.append(model.fit(X_train,y_train,validation_split=0.1,epochs=1000,callbacks=[callbacks]))\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8955f0e-18aa-4e47-bc46-3a58acda7b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(X_train, y_train)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"train\")\n",
    "model.evaluate(X_train, y_train)\n",
    "print(\"prediction\")\n",
    "model.evaluate(X_test, y_test)\n",
    "display(model.summary())\n",
    "for h in history:\n",
    "    fig, ax = plt.subplots(figsize=(16,9), dpi=300)\n",
    "    plt.title(label='Model loss by Epoch', loc='center')\n",
    "    ax.plot(h.history['loss'], label='Training Data', color='black')\n",
    "    ax.plot(h.history['val_loss'], label='Test Data', color='red')\n",
    "    ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "    plt.xticks(ticks=np.arange(len(h.history['loss'])), labels=np.arange(1, len(h.history['loss'])+1))\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "306168e4-21ac-4ef2-9992-fe8886de62a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.predict(X_test) > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bb8d2cf0-3908-4e8d-a704-da3dd2ba8b00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    276\n",
       "dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test > 0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a7da7871-d4cd-4d7d-94f3-d1203d3de26c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN:  0.833063216434003\n",
      "SVC:  0.8447993039004276\n",
      "LOGREG:  0.8297037793667006\n",
      "ENSEMBLE:  0.8460318541217418\n",
      "Without NN, 0.8461105436386335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nn_predict = model.predict(X_test)\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', probability=True)\n",
    "fit_svc = svc.fit(X_train, y_train)\n",
    "svc_predict = fit_svc.predict_proba(X_test)[:, 1].reshape(-1,1)\n",
    "svc_train_predict = fit_svc.predict_proba(X_train)[:, 1].reshape(-1,1)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "fit_logreg = logreg.fit(X_train, y_train)\n",
    "logreg_predict = fit_logreg.predict_proba(X_test)[:, 1].reshape(-1,1)\n",
    "logreg_train_predict = fit_logreg.predict_proba(X_train)[:, 1].reshape(-1,1)\n",
    "\n",
    "\n",
    "ensemble_predict = (logreg_predict + nn_predict + svc_predict) / 3\n",
    "ensemble_nnless_predict = (logreg_predict  + svc_predict) / 2\n",
    "print(\"NN: \",roc_auc_score(y_test,nn_predict))\n",
    "print(\"SVC: \",roc_auc_score(y_test,svc_predict))\n",
    "print(\"LOGREG: \",roc_auc_score(y_test,logreg_predict))\n",
    "print(\"ENSEMBLE: \",roc_auc_score(y_test,ensemble_predict))\n",
    "print(\"Without NN,\" ,roc_auc_score(y_test,ensemble_nnless_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "231f5c64-dd29-432e-a7f6-81fb1f3cc866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add all of the models predictions to the input data and use this to train a final model\n",
    "X_train_w_models = pd.DataFrame(index=y_train.index)\n",
    "X_test_w_models = pd.DataFrame(index=y_test.index)\n",
    "X_train_w_models['svc'] = svc_train_predict\n",
    "X_train_w_models['logreg'] = logreg_train_predict\n",
    "X_test_w_models['svc'] = svc_predict\n",
    "X_test_w_models['logreg'] = logreg_predict\n",
    "for i in range(10):\n",
    "    model = keras.models.load_model(f'models/model{i}.keras')\n",
    "    X_train_w_models[f'NN{i}'] = model.predict(X_train,verbose=0)\n",
    "    X_test_w_models[f'NN{i}'] = model.predict(X_test,verbose=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "0ea812c5-d086-41c0-9b5e-2ff52226f830",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input-Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden-Layer-1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input-Layer (\u001b[38;5;33mDense\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden-Layer-1 (\u001b[38;5;33mDense\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_25 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - auc_46: 0.5089 - loss: 2.2541 - val_auc_46: 0.0000e+00 - val_loss: 2.2390\n",
      "Epoch 2/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.5091 - loss: 2.2490 - val_auc_46: 0.0000e+00 - val_loss: 2.2266\n",
      "Epoch 3/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.5282 - loss: 2.2359 - val_auc_46: 0.0000e+00 - val_loss: 2.2141\n",
      "Epoch 4/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5270 - loss: 2.2303 - val_auc_46: 0.0000e+00 - val_loss: 2.2037\n",
      "Epoch 5/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5217 - loss: 2.2229 - val_auc_46: 0.0000e+00 - val_loss: 2.1945\n",
      "Epoch 6/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5548 - loss: 2.2102 - val_auc_46: 0.0000e+00 - val_loss: 2.1827\n",
      "Epoch 7/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5342 - loss: 2.2084 - val_auc_46: 0.0000e+00 - val_loss: 2.1719\n",
      "Epoch 8/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5442 - loss: 2.1977 - val_auc_46: 0.0000e+00 - val_loss: 2.1633\n",
      "Epoch 9/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5489 - loss: 2.1899 - val_auc_46: 0.0000e+00 - val_loss: 2.1532\n",
      "Epoch 10/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5711 - loss: 2.1796 - val_auc_46: 0.0000e+00 - val_loss: 2.1435\n",
      "Epoch 11/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5588 - loss: 2.1767 - val_auc_46: 0.0000e+00 - val_loss: 2.1343\n",
      "Epoch 12/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5587 - loss: 2.1658 - val_auc_46: 0.0000e+00 - val_loss: 2.1247\n",
      "Epoch 13/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5706 - loss: 2.1598 - val_auc_46: 0.0000e+00 - val_loss: 2.1157\n",
      "Epoch 14/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6060 - loss: 2.1444 - val_auc_46: 0.0000e+00 - val_loss: 2.1045\n",
      "Epoch 15/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6078 - loss: 2.1412 - val_auc_46: 0.0000e+00 - val_loss: 2.0968\n",
      "Epoch 16/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5870 - loss: 2.1352 - val_auc_46: 0.0000e+00 - val_loss: 2.0877\n",
      "Epoch 17/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6250 - loss: 2.1208 - val_auc_46: 0.0000e+00 - val_loss: 2.0781\n",
      "Epoch 18/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6215 - loss: 2.1145 - val_auc_46: 0.0000e+00 - val_loss: 2.0699\n",
      "Epoch 19/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5987 - loss: 2.1118 - val_auc_46: 0.0000e+00 - val_loss: 2.0609\n",
      "Epoch 20/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.5943 - loss: 2.1071 - val_auc_46: 0.0000e+00 - val_loss: 2.0523\n",
      "Epoch 21/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6357 - loss: 2.0915 - val_auc_46: 0.0000e+00 - val_loss: 2.0458\n",
      "Epoch 22/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6195 - loss: 2.0882 - val_auc_46: 0.0000e+00 - val_loss: 2.0374\n",
      "Epoch 23/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6049 - loss: 2.0844 - val_auc_46: 0.0000e+00 - val_loss: 2.0278\n",
      "Epoch 24/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6240 - loss: 2.0731 - val_auc_46: 0.0000e+00 - val_loss: 2.0198\n",
      "Epoch 25/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6364 - loss: 2.0614 - val_auc_46: 0.0000e+00 - val_loss: 2.0114\n",
      "Epoch 26/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6747 - loss: 2.0466 - val_auc_46: 0.0000e+00 - val_loss: 2.0048\n",
      "Epoch 27/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6200 - loss: 2.0530 - val_auc_46: 0.0000e+00 - val_loss: 1.9966\n",
      "Epoch 28/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6220 - loss: 2.0496 - val_auc_46: 0.0000e+00 - val_loss: 1.9879\n",
      "Epoch 29/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6404 - loss: 2.0376 - val_auc_46: 0.0000e+00 - val_loss: 1.9813\n",
      "Epoch 30/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6596 - loss: 2.0261 - val_auc_46: 0.0000e+00 - val_loss: 1.9740\n",
      "Epoch 31/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6909 - loss: 2.0123 - val_auc_46: 0.0000e+00 - val_loss: 1.9661\n",
      "Epoch 32/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6578 - loss: 2.0116 - val_auc_46: 0.0000e+00 - val_loss: 1.9586\n",
      "Epoch 33/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6408 - loss: 2.0138 - val_auc_46: 0.0000e+00 - val_loss: 1.9517\n",
      "Epoch 34/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6800 - loss: 1.9942 - val_auc_46: 0.0000e+00 - val_loss: 1.9440\n",
      "Epoch 35/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6838 - loss: 1.9866 - val_auc_46: 0.0000e+00 - val_loss: 1.9358\n",
      "Epoch 36/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.6715 - loss: 1.9850 - val_auc_46: 0.0000e+00 - val_loss: 1.9292\n",
      "Epoch 37/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6970 - loss: 1.9707 - val_auc_46: 0.0000e+00 - val_loss: 1.9224\n",
      "Epoch 38/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7039 - loss: 1.9654 - val_auc_46: 0.0000e+00 - val_loss: 1.9153\n",
      "Epoch 39/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6871 - loss: 1.9625 - val_auc_46: 0.0000e+00 - val_loss: 1.9092\n",
      "Epoch 40/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7088 - loss: 1.9497 - val_auc_46: 0.0000e+00 - val_loss: 1.9010\n",
      "Epoch 41/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6811 - loss: 1.9487 - val_auc_46: 0.0000e+00 - val_loss: 1.8929\n",
      "Epoch 42/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6865 - loss: 1.9407 - val_auc_46: 0.0000e+00 - val_loss: 1.8867\n",
      "Epoch 43/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.6899 - loss: 1.9373 - val_auc_46: 0.0000e+00 - val_loss: 1.8788\n",
      "Epoch 44/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7192 - loss: 1.9234 - val_auc_46: 0.0000e+00 - val_loss: 1.8721\n",
      "Epoch 45/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7323 - loss: 1.9137 - val_auc_46: 0.0000e+00 - val_loss: 1.8657\n",
      "Epoch 46/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7319 - loss: 1.9052 - val_auc_46: 0.0000e+00 - val_loss: 1.8569\n",
      "Epoch 47/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7091 - loss: 1.9038 - val_auc_46: 0.0000e+00 - val_loss: 1.8503\n",
      "Epoch 48/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7136 - loss: 1.8974 - val_auc_46: 0.0000e+00 - val_loss: 1.8439\n",
      "Epoch 49/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7343 - loss: 1.8853 - val_auc_46: 0.0000e+00 - val_loss: 1.8363\n",
      "Epoch 50/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7571 - loss: 1.8739 - val_auc_46: 0.0000e+00 - val_loss: 1.8289\n",
      "Epoch 51/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7382 - loss: 1.8738 - val_auc_46: 0.0000e+00 - val_loss: 1.8224\n",
      "Epoch 52/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7415 - loss: 1.8649 - val_auc_46: 0.0000e+00 - val_loss: 1.8165\n",
      "Epoch 53/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7485 - loss: 1.8583 - val_auc_46: 0.0000e+00 - val_loss: 1.8086\n",
      "Epoch 54/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7583 - loss: 1.8479 - val_auc_46: 0.0000e+00 - val_loss: 1.8026\n",
      "Epoch 55/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7750 - loss: 1.8397 - val_auc_46: 0.0000e+00 - val_loss: 1.7959\n",
      "Epoch 56/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7415 - loss: 1.8432 - val_auc_46: 0.0000e+00 - val_loss: 1.7880\n",
      "Epoch 57/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7807 - loss: 1.8249 - val_auc_46: 0.0000e+00 - val_loss: 1.7813\n",
      "Epoch 58/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7683 - loss: 1.8231 - val_auc_46: 0.0000e+00 - val_loss: 1.7743\n",
      "Epoch 59/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.7583 - loss: 1.8197 - val_auc_46: 0.0000e+00 - val_loss: 1.7684\n",
      "Epoch 60/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7667 - loss: 1.8101 - val_auc_46: 0.0000e+00 - val_loss: 1.7625\n",
      "Epoch 61/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7834 - loss: 1.8000 - val_auc_46: 0.0000e+00 - val_loss: 1.7554\n",
      "Epoch 62/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7563 - loss: 1.8022 - val_auc_46: 0.0000e+00 - val_loss: 1.7486\n",
      "Epoch 63/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7747 - loss: 1.7900 - val_auc_46: 0.0000e+00 - val_loss: 1.7423\n",
      "Epoch 64/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8055 - loss: 1.7773 - val_auc_46: 0.0000e+00 - val_loss: 1.7358\n",
      "Epoch 65/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7860 - loss: 1.7767 - val_auc_46: 0.0000e+00 - val_loss: 1.7297\n",
      "Epoch 66/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.7740 - loss: 1.7735 - val_auc_46: 0.0000e+00 - val_loss: 1.7234\n",
      "Epoch 67/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.7600 - loss: 1.7719 - val_auc_46: 0.0000e+00 - val_loss: 1.7157\n",
      "Epoch 68/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8015 - loss: 1.7542 - val_auc_46: 0.0000e+00 - val_loss: 1.7092\n",
      "Epoch 69/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8072 - loss: 1.7486 - val_auc_46: 0.0000e+00 - val_loss: 1.7021\n",
      "Epoch 70/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8040 - loss: 1.7413 - val_auc_46: 0.0000e+00 - val_loss: 1.6961\n",
      "Epoch 71/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8199 - loss: 1.7329 - val_auc_46: 0.0000e+00 - val_loss: 1.6896\n",
      "Epoch 72/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8189 - loss: 1.7273 - val_auc_46: 0.0000e+00 - val_loss: 1.6840\n",
      "Epoch 73/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8147 - loss: 1.7207 - val_auc_46: 0.0000e+00 - val_loss: 1.6792\n",
      "Epoch 74/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8265 - loss: 1.7103 - val_auc_46: 0.0000e+00 - val_loss: 1.6722\n",
      "Epoch 75/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8131 - loss: 1.7099 - val_auc_46: 0.0000e+00 - val_loss: 1.6669\n",
      "Epoch 76/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8084 - loss: 1.7043 - val_auc_46: 0.0000e+00 - val_loss: 1.6607\n",
      "Epoch 77/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8269 - loss: 1.6965 - val_auc_46: 0.0000e+00 - val_loss: 1.6548\n",
      "Epoch 78/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8177 - loss: 1.6931 - val_auc_46: 0.0000e+00 - val_loss: 1.6485\n",
      "Epoch 79/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8339 - loss: 1.6820 - val_auc_46: 0.0000e+00 - val_loss: 1.6427\n",
      "Epoch 80/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8059 - loss: 1.6840 - val_auc_46: 0.0000e+00 - val_loss: 1.6369\n",
      "Epoch 81/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8459 - loss: 1.6657 - val_auc_46: 0.0000e+00 - val_loss: 1.6301\n",
      "Epoch 82/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8221 - loss: 1.6688 - val_auc_46: 0.0000e+00 - val_loss: 1.6246\n",
      "Epoch 83/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8419 - loss: 1.6559 - val_auc_46: 0.0000e+00 - val_loss: 1.6187\n",
      "Epoch 84/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8246 - loss: 1.6568 - val_auc_46: 0.0000e+00 - val_loss: 1.6128\n",
      "Epoch 85/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8280 - loss: 1.6518 - val_auc_46: 0.0000e+00 - val_loss: 1.6069\n",
      "Epoch 86/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8396 - loss: 1.6419 - val_auc_46: 0.0000e+00 - val_loss: 1.6015\n",
      "Epoch 87/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8264 - loss: 1.6381 - val_auc_46: 0.0000e+00 - val_loss: 1.5954\n",
      "Epoch 88/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8619 - loss: 1.6174 - val_auc_46: 0.0000e+00 - val_loss: 1.5885\n",
      "Epoch 89/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8228 - loss: 1.6288 - val_auc_46: 0.0000e+00 - val_loss: 1.5840\n",
      "Epoch 90/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8447 - loss: 1.6144 - val_auc_46: 0.0000e+00 - val_loss: 1.5779\n",
      "Epoch 91/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8662 - loss: 1.6037 - val_auc_46: 0.0000e+00 - val_loss: 1.5729\n",
      "Epoch 92/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8492 - loss: 1.6045 - val_auc_46: 0.0000e+00 - val_loss: 1.5664\n",
      "Epoch 93/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8488 - loss: 1.5965 - val_auc_46: 0.0000e+00 - val_loss: 1.5618\n",
      "Epoch 94/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8626 - loss: 1.5882 - val_auc_46: 0.0000e+00 - val_loss: 1.5555\n",
      "Epoch 95/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8529 - loss: 1.5849 - val_auc_46: 0.0000e+00 - val_loss: 1.5499\n",
      "Epoch 96/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8598 - loss: 1.5760 - val_auc_46: 0.0000e+00 - val_loss: 1.5439\n",
      "Epoch 97/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8726 - loss: 1.5653 - val_auc_46: 0.0000e+00 - val_loss: 1.5384\n",
      "Epoch 98/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8618 - loss: 1.5654 - val_auc_46: 0.0000e+00 - val_loss: 1.5331\n",
      "Epoch 99/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8645 - loss: 1.5618 - val_auc_46: 0.0000e+00 - val_loss: 1.5272\n",
      "Epoch 100/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8523 - loss: 1.5569 - val_auc_46: 0.0000e+00 - val_loss: 1.5207\n",
      "Epoch 101/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8847 - loss: 1.5428 - val_auc_46: 0.0000e+00 - val_loss: 1.5161\n",
      "Epoch 102/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8649 - loss: 1.5425 - val_auc_46: 0.0000e+00 - val_loss: 1.5103\n",
      "Epoch 103/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8760 - loss: 1.5332 - val_auc_46: 0.0000e+00 - val_loss: 1.5044\n",
      "Epoch 104/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8812 - loss: 1.5259 - val_auc_46: 0.0000e+00 - val_loss: 1.4991\n",
      "Epoch 105/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8696 - loss: 1.5278 - val_auc_46: 0.0000e+00 - val_loss: 1.4927\n",
      "Epoch 106/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8709 - loss: 1.5177 - val_auc_46: 0.0000e+00 - val_loss: 1.4870\n",
      "Epoch 107/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8798 - loss: 1.5106 - val_auc_46: 0.0000e+00 - val_loss: 1.4817\n",
      "Epoch 108/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8609 - loss: 1.5102 - val_auc_46: 0.0000e+00 - val_loss: 1.4763\n",
      "Epoch 109/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8899 - loss: 1.4950 - val_auc_46: 0.0000e+00 - val_loss: 1.4700\n",
      "Epoch 110/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8633 - loss: 1.5010 - val_auc_46: 0.0000e+00 - val_loss: 1.4653\n",
      "Epoch 111/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.8920 - loss: 1.4871 - val_auc_46: 0.0000e+00 - val_loss: 1.4597\n",
      "Epoch 112/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8709 - loss: 1.4861 - val_auc_46: 0.0000e+00 - val_loss: 1.4553\n",
      "Epoch 113/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8746 - loss: 1.4815 - val_auc_46: 0.0000e+00 - val_loss: 1.4497\n",
      "Epoch 114/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8813 - loss: 1.4702 - val_auc_46: 0.0000e+00 - val_loss: 1.4448\n",
      "Epoch 115/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8838 - loss: 1.4666 - val_auc_46: 0.0000e+00 - val_loss: 1.4389\n",
      "Epoch 116/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9017 - loss: 1.4521 - val_auc_46: 0.0000e+00 - val_loss: 1.4330\n",
      "Epoch 117/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8859 - loss: 1.4572 - val_auc_46: 0.0000e+00 - val_loss: 1.4278\n",
      "Epoch 118/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8879 - loss: 1.4489 - val_auc_46: 0.0000e+00 - val_loss: 1.4225\n",
      "Epoch 119/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8875 - loss: 1.4443 - val_auc_46: 0.0000e+00 - val_loss: 1.4173\n",
      "Epoch 120/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8885 - loss: 1.4387 - val_auc_46: 0.0000e+00 - val_loss: 1.4121\n",
      "Epoch 121/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8946 - loss: 1.4278 - val_auc_46: 0.0000e+00 - val_loss: 1.4067\n",
      "Epoch 122/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8941 - loss: 1.4249 - val_auc_46: 0.0000e+00 - val_loss: 1.4006\n",
      "Epoch 123/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8961 - loss: 1.4194 - val_auc_46: 0.0000e+00 - val_loss: 1.3965\n",
      "Epoch 124/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9037 - loss: 1.4081 - val_auc_46: 0.0000e+00 - val_loss: 1.3904\n",
      "Epoch 125/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8887 - loss: 1.4134 - val_auc_46: 0.0000e+00 - val_loss: 1.3849\n",
      "Epoch 126/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8960 - loss: 1.4023 - val_auc_46: 0.0000e+00 - val_loss: 1.3806\n",
      "Epoch 127/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8970 - loss: 1.3964 - val_auc_46: 0.0000e+00 - val_loss: 1.3742\n",
      "Epoch 128/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8960 - loss: 1.3926 - val_auc_46: 0.0000e+00 - val_loss: 1.3699\n",
      "Epoch 129/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9080 - loss: 1.3827 - val_auc_46: 0.0000e+00 - val_loss: 1.3642\n",
      "Epoch 130/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8951 - loss: 1.3860 - val_auc_46: 0.0000e+00 - val_loss: 1.3585\n",
      "Epoch 131/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8862 - loss: 1.3839 - val_auc_46: 0.0000e+00 - val_loss: 1.3549\n",
      "Epoch 132/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8938 - loss: 1.3740 - val_auc_46: 0.0000e+00 - val_loss: 1.3490\n",
      "Epoch 133/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9032 - loss: 1.3622 - val_auc_46: 0.0000e+00 - val_loss: 1.3443\n",
      "Epoch 134/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8905 - loss: 1.3629 - val_auc_46: 0.0000e+00 - val_loss: 1.3397\n",
      "Epoch 135/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8956 - loss: 1.3554 - val_auc_46: 0.0000e+00 - val_loss: 1.3338\n",
      "Epoch 136/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8959 - loss: 1.3491 - val_auc_46: 0.0000e+00 - val_loss: 1.3285\n",
      "Epoch 137/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8945 - loss: 1.3471 - val_auc_46: 0.0000e+00 - val_loss: 1.3242\n",
      "Epoch 138/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.8951 - loss: 1.3400 - val_auc_46: 0.0000e+00 - val_loss: 1.3196\n",
      "Epoch 139/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8882 - loss: 1.3398 - val_auc_46: 0.0000e+00 - val_loss: 1.3137\n",
      "Epoch 140/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8907 - loss: 1.3335 - val_auc_46: 0.0000e+00 - val_loss: 1.3092\n",
      "Epoch 141/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9115 - loss: 1.3173 - val_auc_46: 0.0000e+00 - val_loss: 1.3045\n",
      "Epoch 142/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9165 - loss: 1.3106 - val_auc_46: 0.0000e+00 - val_loss: 1.2995\n",
      "Epoch 143/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9180 - loss: 1.3042 - val_auc_46: 0.0000e+00 - val_loss: 1.2945\n",
      "Epoch 144/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9265 - loss: 1.2943 - val_auc_46: 0.0000e+00 - val_loss: 1.2898\n",
      "Epoch 145/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9026 - loss: 1.3046 - val_auc_46: 0.0000e+00 - val_loss: 1.2846\n",
      "Epoch 146/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.8987 - loss: 1.2995 - val_auc_46: 0.0000e+00 - val_loss: 1.2791\n",
      "Epoch 147/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9138 - loss: 1.2866 - val_auc_46: 0.0000e+00 - val_loss: 1.2735\n",
      "Epoch 148/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9113 - loss: 1.2819 - val_auc_46: 0.0000e+00 - val_loss: 1.2692\n",
      "Epoch 149/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9078 - loss: 1.2796 - val_auc_46: 0.0000e+00 - val_loss: 1.2649\n",
      "Epoch 150/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9169 - loss: 1.2696 - val_auc_46: 0.0000e+00 - val_loss: 1.2596\n",
      "Epoch 151/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9146 - loss: 1.2709 - val_auc_46: 0.0000e+00 - val_loss: 1.2550\n",
      "Epoch 152/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9207 - loss: 1.2587 - val_auc_46: 0.0000e+00 - val_loss: 1.2495\n",
      "Epoch 153/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9081 - loss: 1.2616 - val_auc_46: 0.0000e+00 - val_loss: 1.2454\n",
      "Epoch 154/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9123 - loss: 1.2548 - val_auc_46: 0.0000e+00 - val_loss: 1.2404\n",
      "Epoch 155/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9200 - loss: 1.2447 - val_auc_46: 0.0000e+00 - val_loss: 1.2363\n",
      "Epoch 156/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9181 - loss: 1.2416 - val_auc_46: 0.0000e+00 - val_loss: 1.2313\n",
      "Epoch 157/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9162 - loss: 1.2368 - val_auc_46: 0.0000e+00 - val_loss: 1.2272\n",
      "Epoch 158/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9227 - loss: 1.2285 - val_auc_46: 0.0000e+00 - val_loss: 1.2216\n",
      "Epoch 159/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9164 - loss: 1.2261 - val_auc_46: 0.0000e+00 - val_loss: 1.2170\n",
      "Epoch 160/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - auc_46: 0.9105 - loss: 1.2269 - val_auc_46: 0.0000e+00 - val_loss: 1.2125\n",
      "Epoch 161/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9196 - loss: 1.2159 - val_auc_46: 0.0000e+00 - val_loss: 1.2085\n",
      "Epoch 162/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9163 - loss: 1.2128 - val_auc_46: 0.0000e+00 - val_loss: 1.2030\n",
      "Epoch 163/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9123 - loss: 1.2133 - val_auc_46: 0.0000e+00 - val_loss: 1.1984\n",
      "Epoch 164/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9107 - loss: 1.2086 - val_auc_46: 0.0000e+00 - val_loss: 1.1937\n",
      "Epoch 165/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9179 - loss: 1.2009 - val_auc_46: 0.0000e+00 - val_loss: 1.1895\n",
      "Epoch 166/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9102 - loss: 1.1990 - val_auc_46: 0.0000e+00 - val_loss: 1.1849\n",
      "Epoch 167/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9297 - loss: 1.1844 - val_auc_46: 0.0000e+00 - val_loss: 1.1798\n",
      "Epoch 168/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9166 - loss: 1.1854 - val_auc_46: 0.0000e+00 - val_loss: 1.1761\n",
      "Epoch 169/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9190 - loss: 1.1801 - val_auc_46: 0.0000e+00 - val_loss: 1.1716\n",
      "Epoch 170/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9171 - loss: 1.1782 - val_auc_46: 0.0000e+00 - val_loss: 1.1654\n",
      "Epoch 171/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9136 - loss: 1.1745 - val_auc_46: 0.0000e+00 - val_loss: 1.1613\n",
      "Epoch 172/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9120 - loss: 1.1727 - val_auc_46: 0.0000e+00 - val_loss: 1.1573\n",
      "Epoch 173/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9199 - loss: 1.1615 - val_auc_46: 0.0000e+00 - val_loss: 1.1530\n",
      "Epoch 174/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9117 - loss: 1.1647 - val_auc_46: 0.0000e+00 - val_loss: 1.1485\n",
      "Epoch 175/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9203 - loss: 1.1548 - val_auc_46: 0.0000e+00 - val_loss: 1.1444\n",
      "Epoch 176/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9212 - loss: 1.1467 - val_auc_46: 0.0000e+00 - val_loss: 1.1399\n",
      "Epoch 177/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9229 - loss: 1.1383 - val_auc_46: 0.0000e+00 - val_loss: 1.1355\n",
      "Epoch 178/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9284 - loss: 1.1364 - val_auc_46: 0.0000e+00 - val_loss: 1.1307\n",
      "Epoch 179/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9220 - loss: 1.1376 - val_auc_46: 0.0000e+00 - val_loss: 1.1275\n",
      "Epoch 180/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9244 - loss: 1.1309 - val_auc_46: 0.0000e+00 - val_loss: 1.1228\n",
      "Epoch 181/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9131 - loss: 1.1313 - val_auc_46: 0.0000e+00 - val_loss: 1.1184\n",
      "Epoch 182/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9328 - loss: 1.1134 - val_auc_46: 0.0000e+00 - val_loss: 1.1139\n",
      "Epoch 183/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9259 - loss: 1.1148 - val_auc_46: 0.0000e+00 - val_loss: 1.1093\n",
      "Epoch 184/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9273 - loss: 1.1073 - val_auc_46: 0.0000e+00 - val_loss: 1.1059\n",
      "Epoch 185/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9173 - loss: 1.1085 - val_auc_46: 0.0000e+00 - val_loss: 1.1009\n",
      "Epoch 186/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9230 - loss: 1.1022 - val_auc_46: 0.0000e+00 - val_loss: 1.0966\n",
      "Epoch 187/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9314 - loss: 1.0898 - val_auc_46: 0.0000e+00 - val_loss: 1.0925\n",
      "Epoch 188/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9211 - loss: 1.0959 - val_auc_46: 0.0000e+00 - val_loss: 1.0883\n",
      "Epoch 189/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9198 - loss: 1.0917 - val_auc_46: 0.0000e+00 - val_loss: 1.0845\n",
      "Epoch 190/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9266 - loss: 1.0871 - val_auc_46: 0.0000e+00 - val_loss: 1.0799\n",
      "Epoch 191/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9286 - loss: 1.0787 - val_auc_46: 0.0000e+00 - val_loss: 1.0754\n",
      "Epoch 192/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9362 - loss: 1.0668 - val_auc_46: 0.0000e+00 - val_loss: 1.0714\n",
      "Epoch 193/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9229 - loss: 1.0715 - val_auc_46: 0.0000e+00 - val_loss: 1.0679\n",
      "Epoch 194/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9369 - loss: 1.0585 - val_auc_46: 0.0000e+00 - val_loss: 1.0632\n",
      "Epoch 195/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9163 - loss: 1.0685 - val_auc_46: 0.0000e+00 - val_loss: 1.0594\n",
      "Epoch 196/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9288 - loss: 1.0571 - val_auc_46: 0.0000e+00 - val_loss: 1.0558\n",
      "Epoch 197/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9235 - loss: 1.0575 - val_auc_46: 0.0000e+00 - val_loss: 1.0515\n",
      "Epoch 198/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9240 - loss: 1.0509 - val_auc_46: 0.0000e+00 - val_loss: 1.0486\n",
      "Epoch 199/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9334 - loss: 1.0388 - val_auc_46: 0.0000e+00 - val_loss: 1.0441\n",
      "Epoch 200/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9295 - loss: 1.0412 - val_auc_46: 0.0000e+00 - val_loss: 1.0396\n",
      "Epoch 201/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9149 - loss: 1.0449 - val_auc_46: 0.0000e+00 - val_loss: 1.0367\n",
      "Epoch 202/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9261 - loss: 1.0358 - val_auc_46: 0.0000e+00 - val_loss: 1.0324\n",
      "Epoch 203/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9348 - loss: 1.0208 - val_auc_46: 0.0000e+00 - val_loss: 1.0289\n",
      "Epoch 204/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9158 - loss: 1.0305 - val_auc_46: 0.0000e+00 - val_loss: 1.0246\n",
      "Epoch 205/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9379 - loss: 1.0166 - val_auc_46: 0.0000e+00 - val_loss: 1.0198\n",
      "Epoch 206/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9216 - loss: 1.0224 - val_auc_46: 0.0000e+00 - val_loss: 1.0169\n",
      "Epoch 207/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9320 - loss: 1.0131 - val_auc_46: 0.0000e+00 - val_loss: 1.0121\n",
      "Epoch 208/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9355 - loss: 1.0020 - val_auc_46: 0.0000e+00 - val_loss: 1.0088\n",
      "Epoch 209/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9303 - loss: 1.0014 - val_auc_46: 0.0000e+00 - val_loss: 1.0049\n",
      "Epoch 210/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - auc_46: 0.9257 - loss: 1.0020 - val_auc_46: 0.0000e+00 - val_loss: 1.0020\n",
      "Epoch 211/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9198 - loss: 1.0013 - val_auc_46: 0.0000e+00 - val_loss: 0.9982\n",
      "Epoch 212/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9345 - loss: 0.9871 - val_auc_46: 0.0000e+00 - val_loss: 0.9950\n",
      "Epoch 213/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9396 - loss: 0.9774 - val_auc_46: 0.0000e+00 - val_loss: 0.9911\n",
      "Epoch 214/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9319 - loss: 0.9842 - val_auc_46: 0.0000e+00 - val_loss: 0.9867\n",
      "Epoch 215/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9358 - loss: 0.9770 - val_auc_46: 0.0000e+00 - val_loss: 0.9842\n",
      "Epoch 216/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9373 - loss: 0.9695 - val_auc_46: 0.0000e+00 - val_loss: 0.9792\n",
      "Epoch 217/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9267 - loss: 0.9773 - val_auc_46: 0.0000e+00 - val_loss: 0.9754\n",
      "Epoch 218/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9334 - loss: 0.9697 - val_auc_46: 0.0000e+00 - val_loss: 0.9720\n",
      "Epoch 219/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9294 - loss: 0.9670 - val_auc_46: 0.0000e+00 - val_loss: 0.9678\n",
      "Epoch 220/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9382 - loss: 0.9559 - val_auc_46: 0.0000e+00 - val_loss: 0.9652\n",
      "Epoch 221/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9329 - loss: 0.9597 - val_auc_46: 0.0000e+00 - val_loss: 0.9612\n",
      "Epoch 222/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9353 - loss: 0.9531 - val_auc_46: 0.0000e+00 - val_loss: 0.9572\n",
      "Epoch 223/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9343 - loss: 0.9536 - val_auc_46: 0.0000e+00 - val_loss: 0.9546\n",
      "Epoch 224/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9286 - loss: 0.9469 - val_auc_46: 0.0000e+00 - val_loss: 0.9504\n",
      "Epoch 225/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9378 - loss: 0.9358 - val_auc_46: 0.0000e+00 - val_loss: 0.9480\n",
      "Epoch 226/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9315 - loss: 0.9372 - val_auc_46: 0.0000e+00 - val_loss: 0.9441\n",
      "Epoch 227/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9331 - loss: 0.9364 - val_auc_46: 0.0000e+00 - val_loss: 0.9409\n",
      "Epoch 228/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9331 - loss: 0.9316 - val_auc_46: 0.0000e+00 - val_loss: 0.9372\n",
      "Epoch 229/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9327 - loss: 0.9290 - val_auc_46: 0.0000e+00 - val_loss: 0.9340\n",
      "Epoch 230/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9437 - loss: 0.9152 - val_auc_46: 0.0000e+00 - val_loss: 0.9310\n",
      "Epoch 231/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9383 - loss: 0.9177 - val_auc_46: 0.0000e+00 - val_loss: 0.9273\n",
      "Epoch 232/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9294 - loss: 0.9183 - val_auc_46: 0.0000e+00 - val_loss: 0.9237\n",
      "Epoch 233/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9321 - loss: 0.9104 - val_auc_46: 0.0000e+00 - val_loss: 0.9206\n",
      "Epoch 234/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9329 - loss: 0.9105 - val_auc_46: 0.0000e+00 - val_loss: 0.9177\n",
      "Epoch 235/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9291 - loss: 0.9105 - val_auc_46: 0.0000e+00 - val_loss: 0.9135\n",
      "Epoch 236/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9292 - loss: 0.9079 - val_auc_46: 0.0000e+00 - val_loss: 0.9097\n",
      "Epoch 237/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9349 - loss: 0.9013 - val_auc_46: 0.0000e+00 - val_loss: 0.9072\n",
      "Epoch 238/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9521 - loss: 0.8802 - val_auc_46: 0.0000e+00 - val_loss: 0.9031\n",
      "Epoch 239/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9429 - loss: 0.8863 - val_auc_46: 0.0000e+00 - val_loss: 0.9001\n",
      "Epoch 240/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9316 - loss: 0.8929 - val_auc_46: 0.0000e+00 - val_loss: 0.8967\n",
      "Epoch 241/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9475 - loss: 0.8738 - val_auc_46: 0.0000e+00 - val_loss: 0.8937\n",
      "Epoch 242/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9366 - loss: 0.8816 - val_auc_46: 0.0000e+00 - val_loss: 0.8903\n",
      "Epoch 243/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9351 - loss: 0.8809 - val_auc_46: 0.0000e+00 - val_loss: 0.8874\n",
      "Epoch 244/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9413 - loss: 0.8681 - val_auc_46: 0.0000e+00 - val_loss: 0.8839\n",
      "Epoch 245/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9341 - loss: 0.8740 - val_auc_46: 0.0000e+00 - val_loss: 0.8806\n",
      "Epoch 246/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9358 - loss: 0.8693 - val_auc_46: 0.0000e+00 - val_loss: 0.8776\n",
      "Epoch 247/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9402 - loss: 0.8593 - val_auc_46: 0.0000e+00 - val_loss: 0.8749\n",
      "Epoch 248/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9330 - loss: 0.8635 - val_auc_46: 0.0000e+00 - val_loss: 0.8713\n",
      "Epoch 249/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9433 - loss: 0.8546 - val_auc_46: 0.0000e+00 - val_loss: 0.8677\n",
      "Epoch 250/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9347 - loss: 0.8615 - val_auc_46: 0.0000e+00 - val_loss: 0.8652\n",
      "Epoch 251/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9311 - loss: 0.8583 - val_auc_46: 0.0000e+00 - val_loss: 0.8619\n",
      "Epoch 252/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9297 - loss: 0.8590 - val_auc_46: 0.0000e+00 - val_loss: 0.8582\n",
      "Epoch 253/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9403 - loss: 0.8438 - val_auc_46: 0.0000e+00 - val_loss: 0.8554\n",
      "Epoch 254/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9425 - loss: 0.8381 - val_auc_46: 0.0000e+00 - val_loss: 0.8529\n",
      "Epoch 255/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9372 - loss: 0.8382 - val_auc_46: 0.0000e+00 - val_loss: 0.8495\n",
      "Epoch 256/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9362 - loss: 0.8349 - val_auc_46: 0.0000e+00 - val_loss: 0.8469\n",
      "Epoch 257/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9416 - loss: 0.8288 - val_auc_46: 0.0000e+00 - val_loss: 0.8442\n",
      "Epoch 258/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9476 - loss: 0.8183 - val_auc_46: 0.0000e+00 - val_loss: 0.8414\n",
      "Epoch 259/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9462 - loss: 0.8181 - val_auc_46: 0.0000e+00 - val_loss: 0.8381\n",
      "Epoch 260/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9386 - loss: 0.8239 - val_auc_46: 0.0000e+00 - val_loss: 0.8348\n",
      "Epoch 261/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9416 - loss: 0.8134 - val_auc_46: 0.0000e+00 - val_loss: 0.8311\n",
      "Epoch 262/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9381 - loss: 0.8153 - val_auc_46: 0.0000e+00 - val_loss: 0.8284\n",
      "Epoch 263/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9464 - loss: 0.8079 - val_auc_46: 0.0000e+00 - val_loss: 0.8246\n",
      "Epoch 264/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9454 - loss: 0.8046 - val_auc_46: 0.0000e+00 - val_loss: 0.8218\n",
      "Epoch 265/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9477 - loss: 0.7999 - val_auc_46: 0.0000e+00 - val_loss: 0.8192\n",
      "Epoch 266/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9476 - loss: 0.7973 - val_auc_46: 0.0000e+00 - val_loss: 0.8157\n",
      "Epoch 267/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9408 - loss: 0.7978 - val_auc_46: 0.0000e+00 - val_loss: 0.8129\n",
      "Epoch 268/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9389 - loss: 0.7998 - val_auc_46: 0.0000e+00 - val_loss: 0.8103\n",
      "Epoch 269/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9397 - loss: 0.7951 - val_auc_46: 0.0000e+00 - val_loss: 0.8078\n",
      "Epoch 270/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9408 - loss: 0.7885 - val_auc_46: 0.0000e+00 - val_loss: 0.8047\n",
      "Epoch 271/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9376 - loss: 0.7916 - val_auc_46: 0.0000e+00 - val_loss: 0.8019\n",
      "Epoch 272/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9402 - loss: 0.7883 - val_auc_46: 0.0000e+00 - val_loss: 0.7994\n",
      "Epoch 273/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9449 - loss: 0.7751 - val_auc_46: 0.0000e+00 - val_loss: 0.7963\n",
      "Epoch 274/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9367 - loss: 0.7839 - val_auc_46: 0.0000e+00 - val_loss: 0.7935\n",
      "Epoch 275/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9442 - loss: 0.7704 - val_auc_46: 0.0000e+00 - val_loss: 0.7907\n",
      "Epoch 276/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9482 - loss: 0.7717 - val_auc_46: 0.0000e+00 - val_loss: 0.7875\n",
      "Epoch 277/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9401 - loss: 0.7760 - val_auc_46: 0.0000e+00 - val_loss: 0.7849\n",
      "Epoch 278/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9451 - loss: 0.7643 - val_auc_46: 0.0000e+00 - val_loss: 0.7818\n",
      "Epoch 279/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9401 - loss: 0.7642 - val_auc_46: 0.0000e+00 - val_loss: 0.7803\n",
      "Epoch 280/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9382 - loss: 0.7658 - val_auc_46: 0.0000e+00 - val_loss: 0.7769\n",
      "Epoch 281/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9394 - loss: 0.7670 - val_auc_46: 0.0000e+00 - val_loss: 0.7747\n",
      "Epoch 282/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9402 - loss: 0.7581 - val_auc_46: 0.0000e+00 - val_loss: 0.7728\n",
      "Epoch 283/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9468 - loss: 0.7505 - val_auc_46: 0.0000e+00 - val_loss: 0.7693\n",
      "Epoch 284/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9375 - loss: 0.7564 - val_auc_46: 0.0000e+00 - val_loss: 0.7670\n",
      "Epoch 285/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9512 - loss: 0.7379 - val_auc_46: 0.0000e+00 - val_loss: 0.7645\n",
      "Epoch 286/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9457 - loss: 0.7442 - val_auc_46: 0.0000e+00 - val_loss: 0.7622\n",
      "Epoch 287/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9386 - loss: 0.7501 - val_auc_46: 0.0000e+00 - val_loss: 0.7595\n",
      "Epoch 288/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9319 - loss: 0.7545 - val_auc_46: 0.0000e+00 - val_loss: 0.7572\n",
      "Epoch 289/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9487 - loss: 0.7316 - val_auc_46: 0.0000e+00 - val_loss: 0.7546\n",
      "Epoch 290/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9499 - loss: 0.7260 - val_auc_46: 0.0000e+00 - val_loss: 0.7518\n",
      "Epoch 291/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9472 - loss: 0.7341 - val_auc_46: 0.0000e+00 - val_loss: 0.7494\n",
      "Epoch 292/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9469 - loss: 0.7262 - val_auc_46: 0.0000e+00 - val_loss: 0.7470\n",
      "Epoch 293/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9314 - loss: 0.7406 - val_auc_46: 0.0000e+00 - val_loss: 0.7449\n",
      "Epoch 294/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9391 - loss: 0.7301 - val_auc_46: 0.0000e+00 - val_loss: 0.7421\n",
      "Epoch 295/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9440 - loss: 0.7204 - val_auc_46: 0.0000e+00 - val_loss: 0.7394\n",
      "Epoch 296/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9505 - loss: 0.7168 - val_auc_46: 0.0000e+00 - val_loss: 0.7381\n",
      "Epoch 297/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9442 - loss: 0.7116 - val_auc_46: 0.0000e+00 - val_loss: 0.7347\n",
      "Epoch 298/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9379 - loss: 0.7178 - val_auc_46: 0.0000e+00 - val_loss: 0.7323\n",
      "Epoch 299/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9402 - loss: 0.7202 - val_auc_46: 0.0000e+00 - val_loss: 0.7301\n",
      "Epoch 300/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9386 - loss: 0.7193 - val_auc_46: 0.0000e+00 - val_loss: 0.7284\n",
      "Epoch 301/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9411 - loss: 0.7089 - val_auc_46: 0.0000e+00 - val_loss: 0.7247\n",
      "Epoch 302/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9512 - loss: 0.6978 - val_auc_46: 0.0000e+00 - val_loss: 0.7228\n",
      "Epoch 303/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9373 - loss: 0.7081 - val_auc_46: 0.0000e+00 - val_loss: 0.7207\n",
      "Epoch 304/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9399 - loss: 0.7071 - val_auc_46: 0.0000e+00 - val_loss: 0.7181\n",
      "Epoch 305/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9448 - loss: 0.6957 - val_auc_46: 0.0000e+00 - val_loss: 0.7168\n",
      "Epoch 306/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9338 - loss: 0.7078 - val_auc_46: 0.0000e+00 - val_loss: 0.7141\n",
      "Epoch 307/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9372 - loss: 0.6999 - val_auc_46: 0.0000e+00 - val_loss: 0.7124\n",
      "Epoch 308/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9485 - loss: 0.6893 - val_auc_46: 0.0000e+00 - val_loss: 0.7105\n",
      "Epoch 309/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9414 - loss: 0.6972 - val_auc_46: 0.0000e+00 - val_loss: 0.7083\n",
      "Epoch 310/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9489 - loss: 0.6819 - val_auc_46: 0.0000e+00 - val_loss: 0.7068\n",
      "Epoch 311/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9420 - loss: 0.6880 - val_auc_46: 0.0000e+00 - val_loss: 0.7042\n",
      "Epoch 312/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9522 - loss: 0.6759 - val_auc_46: 0.0000e+00 - val_loss: 0.7022\n",
      "Epoch 313/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9504 - loss: 0.6743 - val_auc_46: 0.0000e+00 - val_loss: 0.6998\n",
      "Epoch 314/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9471 - loss: 0.6807 - val_auc_46: 0.0000e+00 - val_loss: 0.6976\n",
      "Epoch 315/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9569 - loss: 0.6636 - val_auc_46: 0.0000e+00 - val_loss: 0.6947\n",
      "Epoch 316/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9528 - loss: 0.6630 - val_auc_46: 0.0000e+00 - val_loss: 0.6931\n",
      "Epoch 317/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9435 - loss: 0.6749 - val_auc_46: 0.0000e+00 - val_loss: 0.6920\n",
      "Epoch 318/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9471 - loss: 0.6698 - val_auc_46: 0.0000e+00 - val_loss: 0.6891\n",
      "Epoch 319/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9532 - loss: 0.6576 - val_auc_46: 0.0000e+00 - val_loss: 0.6880\n",
      "Epoch 320/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9401 - loss: 0.6731 - val_auc_46: 0.0000e+00 - val_loss: 0.6856\n",
      "Epoch 321/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9386 - loss: 0.6705 - val_auc_46: 0.0000e+00 - val_loss: 0.6848\n",
      "Epoch 322/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9479 - loss: 0.6668 - val_auc_46: 0.0000e+00 - val_loss: 0.6817\n",
      "Epoch 323/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9470 - loss: 0.6607 - val_auc_46: 0.0000e+00 - val_loss: 0.6814\n",
      "Epoch 324/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9512 - loss: 0.6529 - val_auc_46: 0.0000e+00 - val_loss: 0.6794\n",
      "Epoch 325/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9506 - loss: 0.6511 - val_auc_46: 0.0000e+00 - val_loss: 0.6768\n",
      "Epoch 326/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9527 - loss: 0.6454 - val_auc_46: 0.0000e+00 - val_loss: 0.6744\n",
      "Epoch 327/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9455 - loss: 0.6492 - val_auc_46: 0.0000e+00 - val_loss: 0.6734\n",
      "Epoch 328/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9431 - loss: 0.6542 - val_auc_46: 0.0000e+00 - val_loss: 0.6718\n",
      "Epoch 329/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9412 - loss: 0.6547 - val_auc_46: 0.0000e+00 - val_loss: 0.6692\n",
      "Epoch 330/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9457 - loss: 0.6477 - val_auc_46: 0.0000e+00 - val_loss: 0.6681\n",
      "Epoch 331/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9408 - loss: 0.6523 - val_auc_46: 0.0000e+00 - val_loss: 0.6654\n",
      "Epoch 332/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9532 - loss: 0.6390 - val_auc_46: 0.0000e+00 - val_loss: 0.6643\n",
      "Epoch 333/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9495 - loss: 0.6349 - val_auc_46: 0.0000e+00 - val_loss: 0.6626\n",
      "Epoch 334/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9534 - loss: 0.6383 - val_auc_46: 0.0000e+00 - val_loss: 0.6607\n",
      "Epoch 335/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9499 - loss: 0.6335 - val_auc_46: 0.0000e+00 - val_loss: 0.6583\n",
      "Epoch 336/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9476 - loss: 0.6367 - val_auc_46: 0.0000e+00 - val_loss: 0.6563\n",
      "Epoch 337/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9547 - loss: 0.6289 - val_auc_46: 0.0000e+00 - val_loss: 0.6537\n",
      "Epoch 338/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9523 - loss: 0.6262 - val_auc_46: 0.0000e+00 - val_loss: 0.6525\n",
      "Epoch 339/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9565 - loss: 0.6209 - val_auc_46: 0.0000e+00 - val_loss: 0.6500\n",
      "Epoch 340/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9472 - loss: 0.6261 - val_auc_46: 0.0000e+00 - val_loss: 0.6478\n",
      "Epoch 341/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9500 - loss: 0.6265 - val_auc_46: 0.0000e+00 - val_loss: 0.6470\n",
      "Epoch 342/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9586 - loss: 0.6107 - val_auc_46: 0.0000e+00 - val_loss: 0.6452\n",
      "Epoch 343/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9526 - loss: 0.6215 - val_auc_46: 0.0000e+00 - val_loss: 0.6425\n",
      "Epoch 344/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9507 - loss: 0.6176 - val_auc_46: 0.0000e+00 - val_loss: 0.6419\n",
      "Epoch 345/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9469 - loss: 0.6242 - val_auc_46: 0.0000e+00 - val_loss: 0.6397\n",
      "Epoch 346/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9552 - loss: 0.6038 - val_auc_46: 0.0000e+00 - val_loss: 0.6391\n",
      "Epoch 347/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9515 - loss: 0.6136 - val_auc_46: 0.0000e+00 - val_loss: 0.6371\n",
      "Epoch 348/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9507 - loss: 0.6096 - val_auc_46: 0.0000e+00 - val_loss: 0.6359\n",
      "Epoch 349/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9601 - loss: 0.5950 - val_auc_46: 0.0000e+00 - val_loss: 0.6340\n",
      "Epoch 350/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9542 - loss: 0.6059 - val_auc_46: 0.0000e+00 - val_loss: 0.6323\n",
      "Epoch 351/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9551 - loss: 0.6009 - val_auc_46: 0.0000e+00 - val_loss: 0.6305\n",
      "Epoch 352/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - auc_46: 0.9415 - loss: 0.6187 - val_auc_46: 0.0000e+00 - val_loss: 0.6300\n",
      "Epoch 353/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9642 - loss: 0.5855 - val_auc_46: 0.0000e+00 - val_loss: 0.6284\n",
      "Epoch 354/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9449 - loss: 0.6105 - val_auc_46: 0.0000e+00 - val_loss: 0.6266\n",
      "Epoch 355/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9501 - loss: 0.6018 - val_auc_46: 0.0000e+00 - val_loss: 0.6254\n",
      "Epoch 356/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9510 - loss: 0.6001 - val_auc_46: 0.0000e+00 - val_loss: 0.6234\n",
      "Epoch 357/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9474 - loss: 0.6024 - val_auc_46: 0.0000e+00 - val_loss: 0.6216\n",
      "Epoch 358/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9394 - loss: 0.6140 - val_auc_46: 0.0000e+00 - val_loss: 0.6198\n",
      "Epoch 359/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - auc_46: 0.9466 - loss: 0.5978 - val_auc_46: 0.0000e+00 - val_loss: 0.6188\n",
      "Epoch 360/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9509 - loss: 0.5929 - val_auc_46: 0.0000e+00 - val_loss: 0.6167\n",
      "Epoch 361/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9475 - loss: 0.5963 - val_auc_46: 0.0000e+00 - val_loss: 0.6158\n",
      "Epoch 362/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9512 - loss: 0.5938 - val_auc_46: 0.0000e+00 - val_loss: 0.6135\n",
      "Epoch 363/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9523 - loss: 0.5901 - val_auc_46: 0.0000e+00 - val_loss: 0.6122\n",
      "Epoch 364/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9543 - loss: 0.5858 - val_auc_46: 0.0000e+00 - val_loss: 0.6099\n",
      "Epoch 365/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9522 - loss: 0.5856 - val_auc_46: 0.0000e+00 - val_loss: 0.6080\n",
      "Epoch 366/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9453 - loss: 0.5916 - val_auc_46: 0.0000e+00 - val_loss: 0.6071\n",
      "Epoch 367/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9600 - loss: 0.5770 - val_auc_46: 0.0000e+00 - val_loss: 0.6050\n",
      "Epoch 368/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9547 - loss: 0.5796 - val_auc_46: 0.0000e+00 - val_loss: 0.6040\n",
      "Epoch 369/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9550 - loss: 0.5775 - val_auc_46: 0.0000e+00 - val_loss: 0.6022\n",
      "Epoch 370/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9517 - loss: 0.5835 - val_auc_46: 0.0000e+00 - val_loss: 0.6013\n",
      "Epoch 371/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9467 - loss: 0.5811 - val_auc_46: 0.0000e+00 - val_loss: 0.6001\n",
      "Epoch 372/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9554 - loss: 0.5725 - val_auc_46: 0.0000e+00 - val_loss: 0.5993\n",
      "Epoch 373/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9511 - loss: 0.5778 - val_auc_46: 0.0000e+00 - val_loss: 0.5981\n",
      "Epoch 374/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9560 - loss: 0.5672 - val_auc_46: 0.0000e+00 - val_loss: 0.5967\n",
      "Epoch 375/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9480 - loss: 0.5773 - val_auc_46: 0.0000e+00 - val_loss: 0.5955\n",
      "Epoch 376/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9573 - loss: 0.5673 - val_auc_46: 0.0000e+00 - val_loss: 0.5935\n",
      "Epoch 377/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9544 - loss: 0.5692 - val_auc_46: 0.0000e+00 - val_loss: 0.5925\n",
      "Epoch 378/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9472 - loss: 0.5786 - val_auc_46: 0.0000e+00 - val_loss: 0.5910\n",
      "Epoch 379/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9475 - loss: 0.5733 - val_auc_46: 0.0000e+00 - val_loss: 0.5890\n",
      "Epoch 380/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9374 - loss: 0.5806 - val_auc_46: 0.0000e+00 - val_loss: 0.5883\n",
      "Epoch 381/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9646 - loss: 0.5468 - val_auc_46: 0.0000e+00 - val_loss: 0.5873\n",
      "Epoch 382/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9551 - loss: 0.5585 - val_auc_46: 0.0000e+00 - val_loss: 0.5857\n",
      "Epoch 383/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9490 - loss: 0.5687 - val_auc_46: 0.0000e+00 - val_loss: 0.5844\n",
      "Epoch 384/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9542 - loss: 0.5582 - val_auc_46: 0.0000e+00 - val_loss: 0.5829\n",
      "Epoch 385/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9488 - loss: 0.5692 - val_auc_46: 0.0000e+00 - val_loss: 0.5816\n",
      "Epoch 386/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9529 - loss: 0.5555 - val_auc_46: 0.0000e+00 - val_loss: 0.5809\n",
      "Epoch 387/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9524 - loss: 0.5555 - val_auc_46: 0.0000e+00 - val_loss: 0.5793\n",
      "Epoch 388/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9505 - loss: 0.5602 - val_auc_46: 0.0000e+00 - val_loss: 0.5782\n",
      "Epoch 389/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9540 - loss: 0.5535 - val_auc_46: 0.0000e+00 - val_loss: 0.5766\n",
      "Epoch 390/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9578 - loss: 0.5482 - val_auc_46: 0.0000e+00 - val_loss: 0.5753\n",
      "Epoch 391/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9508 - loss: 0.5619 - val_auc_46: 0.0000e+00 - val_loss: 0.5749\n",
      "Epoch 392/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9477 - loss: 0.5634 - val_auc_46: 0.0000e+00 - val_loss: 0.5733\n",
      "Epoch 393/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9502 - loss: 0.5575 - val_auc_46: 0.0000e+00 - val_loss: 0.5729\n",
      "Epoch 394/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9507 - loss: 0.5573 - val_auc_46: 0.0000e+00 - val_loss: 0.5715\n",
      "Epoch 395/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9599 - loss: 0.5410 - val_auc_46: 0.0000e+00 - val_loss: 0.5704\n",
      "Epoch 396/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9424 - loss: 0.5587 - val_auc_46: 0.0000e+00 - val_loss: 0.5700\n",
      "Epoch 397/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9587 - loss: 0.5434 - val_auc_46: 0.0000e+00 - val_loss: 0.5682\n",
      "Epoch 398/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9567 - loss: 0.5398 - val_auc_46: 0.0000e+00 - val_loss: 0.5683\n",
      "Epoch 399/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9540 - loss: 0.5465 - val_auc_46: 0.0000e+00 - val_loss: 0.5670\n",
      "Epoch 400/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9519 - loss: 0.5459 - val_auc_46: 0.0000e+00 - val_loss: 0.5659\n",
      "Epoch 401/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9500 - loss: 0.5423 - val_auc_46: 0.0000e+00 - val_loss: 0.5654\n",
      "Epoch 402/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9534 - loss: 0.5466 - val_auc_46: 0.0000e+00 - val_loss: 0.5645\n",
      "Epoch 403/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9532 - loss: 0.5400 - val_auc_46: 0.0000e+00 - val_loss: 0.5634\n",
      "Epoch 404/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9594 - loss: 0.5344 - val_auc_46: 0.0000e+00 - val_loss: 0.5620\n",
      "Epoch 405/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9622 - loss: 0.5299 - val_auc_46: 0.0000e+00 - val_loss: 0.5614\n",
      "Epoch 406/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9576 - loss: 0.5351 - val_auc_46: 0.0000e+00 - val_loss: 0.5605\n",
      "Epoch 407/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9535 - loss: 0.5393 - val_auc_46: 0.0000e+00 - val_loss: 0.5594\n",
      "Epoch 408/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9649 - loss: 0.5224 - val_auc_46: 0.0000e+00 - val_loss: 0.5578\n",
      "Epoch 409/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9543 - loss: 0.5371 - val_auc_46: 0.0000e+00 - val_loss: 0.5575\n",
      "Epoch 410/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9517 - loss: 0.5329 - val_auc_46: 0.0000e+00 - val_loss: 0.5568\n",
      "Epoch 411/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9590 - loss: 0.5255 - val_auc_46: 0.0000e+00 - val_loss: 0.5554\n",
      "Epoch 412/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9491 - loss: 0.5422 - val_auc_46: 0.0000e+00 - val_loss: 0.5549\n",
      "Epoch 413/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9537 - loss: 0.5289 - val_auc_46: 0.0000e+00 - val_loss: 0.5533\n",
      "Epoch 414/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9560 - loss: 0.5296 - val_auc_46: 0.0000e+00 - val_loss: 0.5529\n",
      "Epoch 415/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9659 - loss: 0.5171 - val_auc_46: 0.0000e+00 - val_loss: 0.5517\n",
      "Epoch 416/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9568 - loss: 0.5280 - val_auc_46: 0.0000e+00 - val_loss: 0.5515\n",
      "Epoch 417/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9581 - loss: 0.5218 - val_auc_46: 0.0000e+00 - val_loss: 0.5511\n",
      "Epoch 418/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9552 - loss: 0.5289 - val_auc_46: 0.0000e+00 - val_loss: 0.5504\n",
      "Epoch 419/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9523 - loss: 0.5303 - val_auc_46: 0.0000e+00 - val_loss: 0.5491\n",
      "Epoch 420/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9510 - loss: 0.5332 - val_auc_46: 0.0000e+00 - val_loss: 0.5484\n",
      "Epoch 421/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9564 - loss: 0.5243 - val_auc_46: 0.0000e+00 - val_loss: 0.5473\n",
      "Epoch 422/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9576 - loss: 0.5202 - val_auc_46: 0.0000e+00 - val_loss: 0.5467\n",
      "Epoch 423/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9607 - loss: 0.5168 - val_auc_46: 0.0000e+00 - val_loss: 0.5464\n",
      "Epoch 424/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9582 - loss: 0.5237 - val_auc_46: 0.0000e+00 - val_loss: 0.5452\n",
      "Epoch 425/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9530 - loss: 0.5263 - val_auc_46: 0.0000e+00 - val_loss: 0.5446\n",
      "Epoch 426/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9611 - loss: 0.5212 - val_auc_46: 0.0000e+00 - val_loss: 0.5445\n",
      "Epoch 427/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9515 - loss: 0.5320 - val_auc_46: 0.0000e+00 - val_loss: 0.5432\n",
      "Epoch 428/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9586 - loss: 0.5189 - val_auc_46: 0.0000e+00 - val_loss: 0.5418\n",
      "Epoch 429/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9609 - loss: 0.5134 - val_auc_46: 0.0000e+00 - val_loss: 0.5412\n",
      "Epoch 430/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9644 - loss: 0.5063 - val_auc_46: 0.0000e+00 - val_loss: 0.5408\n",
      "Epoch 431/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9577 - loss: 0.5219 - val_auc_46: 0.0000e+00 - val_loss: 0.5399\n",
      "Epoch 432/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9567 - loss: 0.5226 - val_auc_46: 0.0000e+00 - val_loss: 0.5383\n",
      "Epoch 433/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9591 - loss: 0.5043 - val_auc_46: 0.0000e+00 - val_loss: 0.5379\n",
      "Epoch 434/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9594 - loss: 0.5130 - val_auc_46: 0.0000e+00 - val_loss: 0.5371\n",
      "Epoch 435/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9511 - loss: 0.5237 - val_auc_46: 0.0000e+00 - val_loss: 0.5366\n",
      "Epoch 436/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9604 - loss: 0.5102 - val_auc_46: 0.0000e+00 - val_loss: 0.5361\n",
      "Epoch 437/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9588 - loss: 0.5104 - val_auc_46: 0.0000e+00 - val_loss: 0.5355\n",
      "Epoch 438/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9525 - loss: 0.5222 - val_auc_46: 0.0000e+00 - val_loss: 0.5345\n",
      "Epoch 439/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9541 - loss: 0.5207 - val_auc_46: 0.0000e+00 - val_loss: 0.5335\n",
      "Epoch 440/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9558 - loss: 0.5201 - val_auc_46: 0.0000e+00 - val_loss: 0.5329\n",
      "Epoch 441/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9631 - loss: 0.5028 - val_auc_46: 0.0000e+00 - val_loss: 0.5318\n",
      "Epoch 442/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9602 - loss: 0.5115 - val_auc_46: 0.0000e+00 - val_loss: 0.5313\n",
      "Epoch 443/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9646 - loss: 0.4964 - val_auc_46: 0.0000e+00 - val_loss: 0.5309\n",
      "Epoch 444/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9502 - loss: 0.5203 - val_auc_46: 0.0000e+00 - val_loss: 0.5298\n",
      "Epoch 445/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9586 - loss: 0.5092 - val_auc_46: 0.0000e+00 - val_loss: 0.5289\n",
      "Epoch 446/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9594 - loss: 0.5079 - val_auc_46: 0.0000e+00 - val_loss: 0.5283\n",
      "Epoch 447/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9604 - loss: 0.5028 - val_auc_46: 0.0000e+00 - val_loss: 0.5277\n",
      "Epoch 448/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9645 - loss: 0.4894 - val_auc_46: 0.0000e+00 - val_loss: 0.5274\n",
      "Epoch 449/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9592 - loss: 0.5007 - val_auc_46: 0.0000e+00 - val_loss: 0.5268\n",
      "Epoch 450/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9607 - loss: 0.5035 - val_auc_46: 0.0000e+00 - val_loss: 0.5264\n",
      "Epoch 451/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9616 - loss: 0.4939 - val_auc_46: 0.0000e+00 - val_loss: 0.5257\n",
      "Epoch 452/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9599 - loss: 0.5066 - val_auc_46: 0.0000e+00 - val_loss: 0.5253\n",
      "Epoch 453/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9592 - loss: 0.5092 - val_auc_46: 0.0000e+00 - val_loss: 0.5237\n",
      "Epoch 454/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9603 - loss: 0.5014 - val_auc_46: 0.0000e+00 - val_loss: 0.5235\n",
      "Epoch 455/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9547 - loss: 0.5061 - val_auc_46: 0.0000e+00 - val_loss: 0.5223\n",
      "Epoch 456/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9586 - loss: 0.5073 - val_auc_46: 0.0000e+00 - val_loss: 0.5220\n",
      "Epoch 457/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9621 - loss: 0.4939 - val_auc_46: 0.0000e+00 - val_loss: 0.5212\n",
      "Epoch 458/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9578 - loss: 0.5054 - val_auc_46: 0.0000e+00 - val_loss: 0.5208\n",
      "Epoch 459/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9588 - loss: 0.4971 - val_auc_46: 0.0000e+00 - val_loss: 0.5198\n",
      "Epoch 460/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9639 - loss: 0.4963 - val_auc_46: 0.0000e+00 - val_loss: 0.5188\n",
      "Epoch 461/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9623 - loss: 0.4983 - val_auc_46: 0.0000e+00 - val_loss: 0.5183\n",
      "Epoch 462/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9540 - loss: 0.5098 - val_auc_46: 0.0000e+00 - val_loss: 0.5177\n",
      "Epoch 463/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9668 - loss: 0.4849 - val_auc_46: 0.0000e+00 - val_loss: 0.5166\n",
      "Epoch 464/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9619 - loss: 0.4912 - val_auc_46: 0.0000e+00 - val_loss: 0.5163\n",
      "Epoch 465/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9603 - loss: 0.4956 - val_auc_46: 0.0000e+00 - val_loss: 0.5161\n",
      "Epoch 466/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9509 - loss: 0.5081 - val_auc_46: 0.0000e+00 - val_loss: 0.5160\n",
      "Epoch 467/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9543 - loss: 0.5026 - val_auc_46: 0.0000e+00 - val_loss: 0.5154\n",
      "Epoch 468/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9640 - loss: 0.4870 - val_auc_46: 0.0000e+00 - val_loss: 0.5148\n",
      "Epoch 469/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9591 - loss: 0.4932 - val_auc_46: 0.0000e+00 - val_loss: 0.5141\n",
      "Epoch 470/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9590 - loss: 0.4962 - val_auc_46: 0.0000e+00 - val_loss: 0.5136\n",
      "Epoch 471/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9597 - loss: 0.4938 - val_auc_46: 0.0000e+00 - val_loss: 0.5127\n",
      "Epoch 472/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9692 - loss: 0.4769 - val_auc_46: 0.0000e+00 - val_loss: 0.5122\n",
      "Epoch 473/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9647 - loss: 0.4857 - val_auc_46: 0.0000e+00 - val_loss: 0.5112\n",
      "Epoch 474/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9639 - loss: 0.4814 - val_auc_46: 0.0000e+00 - val_loss: 0.5108\n",
      "Epoch 475/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9646 - loss: 0.4855 - val_auc_46: 0.0000e+00 - val_loss: 0.5098\n",
      "Epoch 476/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9619 - loss: 0.4876 - val_auc_46: 0.0000e+00 - val_loss: 0.5104\n",
      "Epoch 477/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9593 - loss: 0.4954 - val_auc_46: 0.0000e+00 - val_loss: 0.5093\n",
      "Epoch 478/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9573 - loss: 0.5040 - val_auc_46: 0.0000e+00 - val_loss: 0.5086\n",
      "Epoch 479/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9672 - loss: 0.4798 - val_auc_46: 0.0000e+00 - val_loss: 0.5076\n",
      "Epoch 480/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9605 - loss: 0.4911 - val_auc_46: 0.0000e+00 - val_loss: 0.5065\n",
      "Epoch 481/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9643 - loss: 0.4821 - val_auc_46: 0.0000e+00 - val_loss: 0.5062\n",
      "Epoch 482/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9695 - loss: 0.4746 - val_auc_46: 0.0000e+00 - val_loss: 0.5053\n",
      "Epoch 483/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9651 - loss: 0.4789 - val_auc_46: 0.0000e+00 - val_loss: 0.5042\n",
      "Epoch 484/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9667 - loss: 0.4778 - val_auc_46: 0.0000e+00 - val_loss: 0.5042\n",
      "Epoch 485/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9621 - loss: 0.4805 - val_auc_46: 0.0000e+00 - val_loss: 0.5035\n",
      "Epoch 486/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9629 - loss: 0.4798 - val_auc_46: 0.0000e+00 - val_loss: 0.5031\n",
      "Epoch 487/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9728 - loss: 0.4664 - val_auc_46: 0.0000e+00 - val_loss: 0.5024\n",
      "Epoch 488/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9659 - loss: 0.4830 - val_auc_46: 0.0000e+00 - val_loss: 0.5015\n",
      "Epoch 489/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9681 - loss: 0.4712 - val_auc_46: 0.0000e+00 - val_loss: 0.5016\n",
      "Epoch 490/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9500 - loss: 0.4979 - val_auc_46: 0.0000e+00 - val_loss: 0.5014\n",
      "Epoch 491/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9615 - loss: 0.4844 - val_auc_46: 0.0000e+00 - val_loss: 0.5011\n",
      "Epoch 492/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9609 - loss: 0.4885 - val_auc_46: 0.0000e+00 - val_loss: 0.4997\n",
      "Epoch 493/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9641 - loss: 0.4800 - val_auc_46: 0.0000e+00 - val_loss: 0.4992\n",
      "Epoch 494/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9637 - loss: 0.4801 - val_auc_46: 0.0000e+00 - val_loss: 0.4988\n",
      "Epoch 495/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9625 - loss: 0.4796 - val_auc_46: 0.0000e+00 - val_loss: 0.4974\n",
      "Epoch 496/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9697 - loss: 0.4666 - val_auc_46: 0.0000e+00 - val_loss: 0.4964\n",
      "Epoch 497/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9630 - loss: 0.4775 - val_auc_46: 0.0000e+00 - val_loss: 0.4960\n",
      "Epoch 498/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9645 - loss: 0.4740 - val_auc_46: 0.0000e+00 - val_loss: 0.4953\n",
      "Epoch 499/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9664 - loss: 0.4688 - val_auc_46: 0.0000e+00 - val_loss: 0.4943\n",
      "Epoch 500/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9624 - loss: 0.4772 - val_auc_46: 0.0000e+00 - val_loss: 0.4938\n",
      "Epoch 501/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9683 - loss: 0.4664 - val_auc_46: 0.0000e+00 - val_loss: 0.4938\n",
      "Epoch 502/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9604 - loss: 0.4843 - val_auc_46: 0.0000e+00 - val_loss: 0.4934\n",
      "Epoch 503/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9627 - loss: 0.4790 - val_auc_46: 0.0000e+00 - val_loss: 0.4922\n",
      "Epoch 504/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9708 - loss: 0.4628 - val_auc_46: 0.0000e+00 - val_loss: 0.4923\n",
      "Epoch 505/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9647 - loss: 0.4622 - val_auc_46: 0.0000e+00 - val_loss: 0.4920\n",
      "Epoch 506/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9668 - loss: 0.4722 - val_auc_46: 0.0000e+00 - val_loss: 0.4910\n",
      "Epoch 507/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9590 - loss: 0.4849 - val_auc_46: 0.0000e+00 - val_loss: 0.4914\n",
      "Epoch 508/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9636 - loss: 0.4753 - val_auc_46: 0.0000e+00 - val_loss: 0.4903\n",
      "Epoch 509/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9662 - loss: 0.4664 - val_auc_46: 0.0000e+00 - val_loss: 0.4896\n",
      "Epoch 510/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9650 - loss: 0.4727 - val_auc_46: 0.0000e+00 - val_loss: 0.4893\n",
      "Epoch 511/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9626 - loss: 0.4707 - val_auc_46: 0.0000e+00 - val_loss: 0.4889\n",
      "Epoch 512/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9646 - loss: 0.4700 - val_auc_46: 0.0000e+00 - val_loss: 0.4887\n",
      "Epoch 513/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9657 - loss: 0.4667 - val_auc_46: 0.0000e+00 - val_loss: 0.4875\n",
      "Epoch 514/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9606 - loss: 0.4753 - val_auc_46: 0.0000e+00 - val_loss: 0.4866\n",
      "Epoch 515/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9749 - loss: 0.4462 - val_auc_46: 0.0000e+00 - val_loss: 0.4861\n",
      "Epoch 516/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9724 - loss: 0.4484 - val_auc_46: 0.0000e+00 - val_loss: 0.4859\n",
      "Epoch 517/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9588 - loss: 0.4705 - val_auc_46: 0.0000e+00 - val_loss: 0.4856\n",
      "Epoch 518/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9670 - loss: 0.4669 - val_auc_46: 0.0000e+00 - val_loss: 0.4847\n",
      "Epoch 519/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9654 - loss: 0.4644 - val_auc_46: 0.0000e+00 - val_loss: 0.4838\n",
      "Epoch 520/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9651 - loss: 0.4687 - val_auc_46: 0.0000e+00 - val_loss: 0.4833\n",
      "Epoch 521/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9662 - loss: 0.4646 - val_auc_46: 0.0000e+00 - val_loss: 0.4820\n",
      "Epoch 522/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9628 - loss: 0.4705 - val_auc_46: 0.0000e+00 - val_loss: 0.4823\n",
      "Epoch 523/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9700 - loss: 0.4535 - val_auc_46: 0.0000e+00 - val_loss: 0.4813\n",
      "Epoch 524/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9702 - loss: 0.4552 - val_auc_46: 0.0000e+00 - val_loss: 0.4808\n",
      "Epoch 525/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9679 - loss: 0.4623 - val_auc_46: 0.0000e+00 - val_loss: 0.4807\n",
      "Epoch 526/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9725 - loss: 0.4495 - val_auc_46: 0.0000e+00 - val_loss: 0.4801\n",
      "Epoch 527/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9467 - loss: 0.4908 - val_auc_46: 0.0000e+00 - val_loss: 0.4792\n",
      "Epoch 528/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9656 - loss: 0.4604 - val_auc_46: 0.0000e+00 - val_loss: 0.4786\n",
      "Epoch 529/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9699 - loss: 0.4558 - val_auc_46: 0.0000e+00 - val_loss: 0.4776\n",
      "Epoch 530/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9633 - loss: 0.4664 - val_auc_46: 0.0000e+00 - val_loss: 0.4775\n",
      "Epoch 531/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9666 - loss: 0.4609 - val_auc_46: 0.0000e+00 - val_loss: 0.4767\n",
      "Epoch 532/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9694 - loss: 0.4588 - val_auc_46: 0.0000e+00 - val_loss: 0.4771\n",
      "Epoch 533/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9689 - loss: 0.4564 - val_auc_46: 0.0000e+00 - val_loss: 0.4757\n",
      "Epoch 534/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9740 - loss: 0.4524 - val_auc_46: 0.0000e+00 - val_loss: 0.4754\n",
      "Epoch 535/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9681 - loss: 0.4574 - val_auc_46: 0.0000e+00 - val_loss: 0.4750\n",
      "Epoch 536/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9678 - loss: 0.4558 - val_auc_46: 0.0000e+00 - val_loss: 0.4746\n",
      "Epoch 537/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9662 - loss: 0.4609 - val_auc_46: 0.0000e+00 - val_loss: 0.4740\n",
      "Epoch 538/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9642 - loss: 0.4680 - val_auc_46: 0.0000e+00 - val_loss: 0.4733\n",
      "Epoch 539/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9701 - loss: 0.4507 - val_auc_46: 0.0000e+00 - val_loss: 0.4734\n",
      "Epoch 540/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9681 - loss: 0.4517 - val_auc_46: 0.0000e+00 - val_loss: 0.4724\n",
      "Epoch 541/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9668 - loss: 0.4572 - val_auc_46: 0.0000e+00 - val_loss: 0.4719\n",
      "Epoch 542/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9679 - loss: 0.4518 - val_auc_46: 0.0000e+00 - val_loss: 0.4709\n",
      "Epoch 543/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9661 - loss: 0.4600 - val_auc_46: 0.0000e+00 - val_loss: 0.4707\n",
      "Epoch 544/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9637 - loss: 0.4633 - val_auc_46: 0.0000e+00 - val_loss: 0.4706\n",
      "Epoch 545/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9705 - loss: 0.4500 - val_auc_46: 0.0000e+00 - val_loss: 0.4700\n",
      "Epoch 546/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9597 - loss: 0.4602 - val_auc_46: 0.0000e+00 - val_loss: 0.4692\n",
      "Epoch 547/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9758 - loss: 0.4331 - val_auc_46: 0.0000e+00 - val_loss: 0.4687\n",
      "Epoch 548/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9602 - loss: 0.4606 - val_auc_46: 0.0000e+00 - val_loss: 0.4681\n",
      "Epoch 549/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9728 - loss: 0.4454 - val_auc_46: 0.0000e+00 - val_loss: 0.4674\n",
      "Epoch 550/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9702 - loss: 0.4469 - val_auc_46: 0.0000e+00 - val_loss: 0.4679\n",
      "Epoch 551/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9657 - loss: 0.4562 - val_auc_46: 0.0000e+00 - val_loss: 0.4671\n",
      "Epoch 552/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9741 - loss: 0.4332 - val_auc_46: 0.0000e+00 - val_loss: 0.4664\n",
      "Epoch 553/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9664 - loss: 0.4543 - val_auc_46: 0.0000e+00 - val_loss: 0.4660\n",
      "Epoch 554/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9635 - loss: 0.4617 - val_auc_46: 0.0000e+00 - val_loss: 0.4664\n",
      "Epoch 555/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9595 - loss: 0.4636 - val_auc_46: 0.0000e+00 - val_loss: 0.4654\n",
      "Epoch 556/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9668 - loss: 0.4473 - val_auc_46: 0.0000e+00 - val_loss: 0.4647\n",
      "Epoch 557/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9685 - loss: 0.4479 - val_auc_46: 0.0000e+00 - val_loss: 0.4648\n",
      "Epoch 558/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9695 - loss: 0.4462 - val_auc_46: 0.0000e+00 - val_loss: 0.4639\n",
      "Epoch 559/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9679 - loss: 0.4482 - val_auc_46: 0.0000e+00 - val_loss: 0.4637\n",
      "Epoch 560/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9729 - loss: 0.4431 - val_auc_46: 0.0000e+00 - val_loss: 0.4629\n",
      "Epoch 561/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9683 - loss: 0.4432 - val_auc_46: 0.0000e+00 - val_loss: 0.4630\n",
      "Epoch 562/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9621 - loss: 0.4586 - val_auc_46: 0.0000e+00 - val_loss: 0.4619\n",
      "Epoch 563/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9653 - loss: 0.4467 - val_auc_46: 0.0000e+00 - val_loss: 0.4615\n",
      "Epoch 564/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9641 - loss: 0.4475 - val_auc_46: 0.0000e+00 - val_loss: 0.4620\n",
      "Epoch 565/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9711 - loss: 0.4360 - val_auc_46: 0.0000e+00 - val_loss: 0.4607\n",
      "Epoch 566/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9583 - loss: 0.4664 - val_auc_46: 0.0000e+00 - val_loss: 0.4607\n",
      "Epoch 567/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9659 - loss: 0.4465 - val_auc_46: 0.0000e+00 - val_loss: 0.4594\n",
      "Epoch 568/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9741 - loss: 0.4312 - val_auc_46: 0.0000e+00 - val_loss: 0.4599\n",
      "Epoch 569/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9680 - loss: 0.4415 - val_auc_46: 0.0000e+00 - val_loss: 0.4592\n",
      "Epoch 570/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9679 - loss: 0.4430 - val_auc_46: 0.0000e+00 - val_loss: 0.4583\n",
      "Epoch 571/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9654 - loss: 0.4490 - val_auc_46: 0.0000e+00 - val_loss: 0.4581\n",
      "Epoch 572/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9694 - loss: 0.4425 - val_auc_46: 0.0000e+00 - val_loss: 0.4573\n",
      "Epoch 573/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9608 - loss: 0.4545 - val_auc_46: 0.0000e+00 - val_loss: 0.4571\n",
      "Epoch 574/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9702 - loss: 0.4369 - val_auc_46: 0.0000e+00 - val_loss: 0.4561\n",
      "Epoch 575/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9691 - loss: 0.4441 - val_auc_46: 0.0000e+00 - val_loss: 0.4561\n",
      "Epoch 576/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9750 - loss: 0.4241 - val_auc_46: 0.0000e+00 - val_loss: 0.4558\n",
      "Epoch 577/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9629 - loss: 0.4462 - val_auc_46: 0.0000e+00 - val_loss: 0.4555\n",
      "Epoch 578/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9690 - loss: 0.4403 - val_auc_46: 0.0000e+00 - val_loss: 0.4543\n",
      "Epoch 579/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9601 - loss: 0.4545 - val_auc_46: 0.0000e+00 - val_loss: 0.4539\n",
      "Epoch 580/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9646 - loss: 0.4472 - val_auc_46: 0.0000e+00 - val_loss: 0.4533\n",
      "Epoch 581/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9651 - loss: 0.4488 - val_auc_46: 0.0000e+00 - val_loss: 0.4527\n",
      "Epoch 582/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9716 - loss: 0.4340 - val_auc_46: 0.0000e+00 - val_loss: 0.4529\n",
      "Epoch 583/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9628 - loss: 0.4518 - val_auc_46: 0.0000e+00 - val_loss: 0.4518\n",
      "Epoch 584/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9737 - loss: 0.4275 - val_auc_46: 0.0000e+00 - val_loss: 0.4517\n",
      "Epoch 585/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9732 - loss: 0.4271 - val_auc_46: 0.0000e+00 - val_loss: 0.4510\n",
      "Epoch 586/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9703 - loss: 0.4317 - val_auc_46: 0.0000e+00 - val_loss: 0.4501\n",
      "Epoch 587/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9686 - loss: 0.4429 - val_auc_46: 0.0000e+00 - val_loss: 0.4500\n",
      "Epoch 588/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9641 - loss: 0.4498 - val_auc_46: 0.0000e+00 - val_loss: 0.4499\n",
      "Epoch 589/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9648 - loss: 0.4420 - val_auc_46: 0.0000e+00 - val_loss: 0.4495\n",
      "Epoch 590/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9691 - loss: 0.4313 - val_auc_46: 0.0000e+00 - val_loss: 0.4486\n",
      "Epoch 591/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9749 - loss: 0.4268 - val_auc_46: 0.0000e+00 - val_loss: 0.4487\n",
      "Epoch 592/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9701 - loss: 0.4371 - val_auc_46: 0.0000e+00 - val_loss: 0.4474\n",
      "Epoch 593/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9714 - loss: 0.4298 - val_auc_46: 0.0000e+00 - val_loss: 0.4472\n",
      "Epoch 594/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9703 - loss: 0.4356 - val_auc_46: 0.0000e+00 - val_loss: 0.4467\n",
      "Epoch 595/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9720 - loss: 0.4322 - val_auc_46: 0.0000e+00 - val_loss: 0.4461\n",
      "Epoch 596/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9774 - loss: 0.4142 - val_auc_46: 0.0000e+00 - val_loss: 0.4465\n",
      "Epoch 597/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9661 - loss: 0.4386 - val_auc_46: 0.0000e+00 - val_loss: 0.4464\n",
      "Epoch 598/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9701 - loss: 0.4308 - val_auc_46: 0.0000e+00 - val_loss: 0.4458\n",
      "Epoch 599/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9667 - loss: 0.4382 - val_auc_46: 0.0000e+00 - val_loss: 0.4460\n",
      "Epoch 600/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9667 - loss: 0.4361 - val_auc_46: 0.0000e+00 - val_loss: 0.4459\n",
      "Epoch 601/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9662 - loss: 0.4378 - val_auc_46: 0.0000e+00 - val_loss: 0.4459\n",
      "Epoch 602/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9706 - loss: 0.4288 - val_auc_46: 0.0000e+00 - val_loss: 0.4460\n",
      "Epoch 603/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9691 - loss: 0.4342 - val_auc_46: 0.0000e+00 - val_loss: 0.4451\n",
      "Epoch 604/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9728 - loss: 0.4297 - val_auc_46: 0.0000e+00 - val_loss: 0.4455\n",
      "Epoch 605/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9732 - loss: 0.4277 - val_auc_46: 0.0000e+00 - val_loss: 0.4444\n",
      "Epoch 606/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9678 - loss: 0.4306 - val_auc_46: 0.0000e+00 - val_loss: 0.4442\n",
      "Epoch 607/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9670 - loss: 0.4396 - val_auc_46: 0.0000e+00 - val_loss: 0.4429\n",
      "Epoch 608/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9687 - loss: 0.4338 - val_auc_46: 0.0000e+00 - val_loss: 0.4431\n",
      "Epoch 609/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9746 - loss: 0.4273 - val_auc_46: 0.0000e+00 - val_loss: 0.4424\n",
      "Epoch 610/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9699 - loss: 0.4264 - val_auc_46: 0.0000e+00 - val_loss: 0.4418\n",
      "Epoch 611/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9668 - loss: 0.4366 - val_auc_46: 0.0000e+00 - val_loss: 0.4408\n",
      "Epoch 612/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9703 - loss: 0.4297 - val_auc_46: 0.0000e+00 - val_loss: 0.4401\n",
      "Epoch 613/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9708 - loss: 0.4303 - val_auc_46: 0.0000e+00 - val_loss: 0.4399\n",
      "Epoch 614/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9722 - loss: 0.4174 - val_auc_46: 0.0000e+00 - val_loss: 0.4399\n",
      "Epoch 615/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9783 - loss: 0.4126 - val_auc_46: 0.0000e+00 - val_loss: 0.4393\n",
      "Epoch 616/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9680 - loss: 0.4328 - val_auc_46: 0.0000e+00 - val_loss: 0.4389\n",
      "Epoch 617/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9729 - loss: 0.4227 - val_auc_46: 0.0000e+00 - val_loss: 0.4387\n",
      "Epoch 618/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9694 - loss: 0.4335 - val_auc_46: 0.0000e+00 - val_loss: 0.4380\n",
      "Epoch 619/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9747 - loss: 0.4168 - val_auc_46: 0.0000e+00 - val_loss: 0.4377\n",
      "Epoch 620/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9714 - loss: 0.4217 - val_auc_46: 0.0000e+00 - val_loss: 0.4375\n",
      "Epoch 621/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9747 - loss: 0.4196 - val_auc_46: 0.0000e+00 - val_loss: 0.4363\n",
      "Epoch 622/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9731 - loss: 0.4222 - val_auc_46: 0.0000e+00 - val_loss: 0.4361\n",
      "Epoch 623/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9746 - loss: 0.4118 - val_auc_46: 0.0000e+00 - val_loss: 0.4360\n",
      "Epoch 624/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9690 - loss: 0.4276 - val_auc_46: 0.0000e+00 - val_loss: 0.4364\n",
      "Epoch 625/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9715 - loss: 0.4204 - val_auc_46: 0.0000e+00 - val_loss: 0.4350\n",
      "Epoch 626/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9728 - loss: 0.4243 - val_auc_46: 0.0000e+00 - val_loss: 0.4350\n",
      "Epoch 627/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9738 - loss: 0.4137 - val_auc_46: 0.0000e+00 - val_loss: 0.4348\n",
      "Epoch 628/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9732 - loss: 0.4214 - val_auc_46: 0.0000e+00 - val_loss: 0.4340\n",
      "Epoch 629/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9689 - loss: 0.4347 - val_auc_46: 0.0000e+00 - val_loss: 0.4336\n",
      "Epoch 630/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9712 - loss: 0.4191 - val_auc_46: 0.0000e+00 - val_loss: 0.4337\n",
      "Epoch 631/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9741 - loss: 0.4200 - val_auc_46: 0.0000e+00 - val_loss: 0.4335\n",
      "Epoch 632/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9727 - loss: 0.4238 - val_auc_46: 0.0000e+00 - val_loss: 0.4325\n",
      "Epoch 633/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9683 - loss: 0.4330 - val_auc_46: 0.0000e+00 - val_loss: 0.4320\n",
      "Epoch 634/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9681 - loss: 0.4293 - val_auc_46: 0.0000e+00 - val_loss: 0.4319\n",
      "Epoch 635/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9767 - loss: 0.4068 - val_auc_46: 0.0000e+00 - val_loss: 0.4315\n",
      "Epoch 636/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9738 - loss: 0.4166 - val_auc_46: 0.0000e+00 - val_loss: 0.4305\n",
      "Epoch 637/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9641 - loss: 0.4317 - val_auc_46: 0.0000e+00 - val_loss: 0.4308\n",
      "Epoch 638/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9754 - loss: 0.4084 - val_auc_46: 0.0000e+00 - val_loss: 0.4306\n",
      "Epoch 639/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9739 - loss: 0.4125 - val_auc_46: 0.0000e+00 - val_loss: 0.4301\n",
      "Epoch 640/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9687 - loss: 0.4242 - val_auc_46: 0.0000e+00 - val_loss: 0.4300\n",
      "Epoch 641/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9734 - loss: 0.4177 - val_auc_46: 0.0000e+00 - val_loss: 0.4298\n",
      "Epoch 642/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9702 - loss: 0.4249 - val_auc_46: 0.0000e+00 - val_loss: 0.4286\n",
      "Epoch 643/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9730 - loss: 0.4110 - val_auc_46: 0.0000e+00 - val_loss: 0.4285\n",
      "Epoch 644/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9705 - loss: 0.4240 - val_auc_46: 0.0000e+00 - val_loss: 0.4288\n",
      "Epoch 645/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9737 - loss: 0.4117 - val_auc_46: 0.0000e+00 - val_loss: 0.4286\n",
      "Epoch 646/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9677 - loss: 0.4194 - val_auc_46: 0.0000e+00 - val_loss: 0.4289\n",
      "Epoch 647/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9732 - loss: 0.4146 - val_auc_46: 0.0000e+00 - val_loss: 0.4277\n",
      "Epoch 648/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9713 - loss: 0.4098 - val_auc_46: 0.0000e+00 - val_loss: 0.4272\n",
      "Epoch 649/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9731 - loss: 0.4121 - val_auc_46: 0.0000e+00 - val_loss: 0.4263\n",
      "Epoch 650/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9682 - loss: 0.4245 - val_auc_46: 0.0000e+00 - val_loss: 0.4261\n",
      "Epoch 651/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9768 - loss: 0.4036 - val_auc_46: 0.0000e+00 - val_loss: 0.4261\n",
      "Epoch 652/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9741 - loss: 0.4170 - val_auc_46: 0.0000e+00 - val_loss: 0.4252\n",
      "Epoch 653/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9685 - loss: 0.4232 - val_auc_46: 0.0000e+00 - val_loss: 0.4247\n",
      "Epoch 654/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9749 - loss: 0.4064 - val_auc_46: 0.0000e+00 - val_loss: 0.4247\n",
      "Epoch 655/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9751 - loss: 0.4088 - val_auc_46: 0.0000e+00 - val_loss: 0.4239\n",
      "Epoch 656/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9752 - loss: 0.4049 - val_auc_46: 0.0000e+00 - val_loss: 0.4234\n",
      "Epoch 657/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9675 - loss: 0.4238 - val_auc_46: 0.0000e+00 - val_loss: 0.4233\n",
      "Epoch 658/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9772 - loss: 0.4032 - val_auc_46: 0.0000e+00 - val_loss: 0.4229\n",
      "Epoch 659/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9764 - loss: 0.3997 - val_auc_46: 0.0000e+00 - val_loss: 0.4226\n",
      "Epoch 660/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9678 - loss: 0.4242 - val_auc_46: 0.0000e+00 - val_loss: 0.4221\n",
      "Epoch 661/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9720 - loss: 0.4140 - val_auc_46: 0.0000e+00 - val_loss: 0.4219\n",
      "Epoch 662/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9688 - loss: 0.4198 - val_auc_46: 0.0000e+00 - val_loss: 0.4214\n",
      "Epoch 663/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9646 - loss: 0.4249 - val_auc_46: 0.0000e+00 - val_loss: 0.4210\n",
      "Epoch 664/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9688 - loss: 0.4187 - val_auc_46: 0.0000e+00 - val_loss: 0.4208\n",
      "Epoch 665/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9754 - loss: 0.4066 - val_auc_46: 0.0000e+00 - val_loss: 0.4202\n",
      "Epoch 666/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9743 - loss: 0.4126 - val_auc_46: 0.0000e+00 - val_loss: 0.4202\n",
      "Epoch 667/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9686 - loss: 0.4121 - val_auc_46: 0.0000e+00 - val_loss: 0.4196\n",
      "Epoch 668/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9810 - loss: 0.3887 - val_auc_46: 0.0000e+00 - val_loss: 0.4192\n",
      "Epoch 669/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9773 - loss: 0.3988 - val_auc_46: 0.0000e+00 - val_loss: 0.4194\n",
      "Epoch 670/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9694 - loss: 0.4148 - val_auc_46: 0.0000e+00 - val_loss: 0.4189\n",
      "Epoch 671/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9729 - loss: 0.4092 - val_auc_46: 0.0000e+00 - val_loss: 0.4185\n",
      "Epoch 672/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - auc_46: 0.9734 - loss: 0.4095 - val_auc_46: 0.0000e+00 - val_loss: 0.4189\n",
      "Epoch 673/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9766 - loss: 0.3997 - val_auc_46: 0.0000e+00 - val_loss: 0.4182\n",
      "Epoch 674/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9741 - loss: 0.4059 - val_auc_46: 0.0000e+00 - val_loss: 0.4182\n",
      "Epoch 675/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9671 - loss: 0.4195 - val_auc_46: 0.0000e+00 - val_loss: 0.4180\n",
      "Epoch 676/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9756 - loss: 0.4027 - val_auc_46: 0.0000e+00 - val_loss: 0.4182\n",
      "Epoch 677/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9719 - loss: 0.4100 - val_auc_46: 0.0000e+00 - val_loss: 0.4175\n",
      "Epoch 678/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9723 - loss: 0.4053 - val_auc_46: 0.0000e+00 - val_loss: 0.4173\n",
      "Epoch 679/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9714 - loss: 0.4123 - val_auc_46: 0.0000e+00 - val_loss: 0.4173\n",
      "Epoch 680/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9752 - loss: 0.4002 - val_auc_46: 0.0000e+00 - val_loss: 0.4172\n",
      "Epoch 681/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9727 - loss: 0.4085 - val_auc_46: 0.0000e+00 - val_loss: 0.4174\n",
      "Epoch 682/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9725 - loss: 0.4072 - val_auc_46: 0.0000e+00 - val_loss: 0.4163\n",
      "Epoch 683/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9768 - loss: 0.3989 - val_auc_46: 0.0000e+00 - val_loss: 0.4163\n",
      "Epoch 684/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9697 - loss: 0.4127 - val_auc_46: 0.0000e+00 - val_loss: 0.4155\n",
      "Epoch 685/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9738 - loss: 0.4004 - val_auc_46: 0.0000e+00 - val_loss: 0.4159\n",
      "Epoch 686/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9711 - loss: 0.4126 - val_auc_46: 0.0000e+00 - val_loss: 0.4154\n",
      "Epoch 687/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9780 - loss: 0.3926 - val_auc_46: 0.0000e+00 - val_loss: 0.4159\n",
      "Epoch 688/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9732 - loss: 0.4050 - val_auc_46: 0.0000e+00 - val_loss: 0.4148\n",
      "Epoch 689/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9700 - loss: 0.4096 - val_auc_46: 0.0000e+00 - val_loss: 0.4148\n",
      "Epoch 690/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9681 - loss: 0.4159 - val_auc_46: 0.0000e+00 - val_loss: 0.4137\n",
      "Epoch 691/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9794 - loss: 0.3883 - val_auc_46: 0.0000e+00 - val_loss: 0.4138\n",
      "Epoch 692/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9731 - loss: 0.4067 - val_auc_46: 0.0000e+00 - val_loss: 0.4136\n",
      "Epoch 693/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9763 - loss: 0.4023 - val_auc_46: 0.0000e+00 - val_loss: 0.4127\n",
      "Epoch 694/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9764 - loss: 0.3901 - val_auc_46: 0.0000e+00 - val_loss: 0.4122\n",
      "Epoch 695/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9737 - loss: 0.4005 - val_auc_46: 0.0000e+00 - val_loss: 0.4118\n",
      "Epoch 696/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9779 - loss: 0.3827 - val_auc_46: 0.0000e+00 - val_loss: 0.4118\n",
      "Epoch 697/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9718 - loss: 0.4060 - val_auc_46: 0.0000e+00 - val_loss: 0.4119\n",
      "Epoch 698/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9705 - loss: 0.4126 - val_auc_46: 0.0000e+00 - val_loss: 0.4112\n",
      "Epoch 699/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9774 - loss: 0.3959 - val_auc_46: 0.0000e+00 - val_loss: 0.4111\n",
      "Epoch 700/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9693 - loss: 0.4057 - val_auc_46: 0.0000e+00 - val_loss: 0.4112\n",
      "Epoch 701/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9705 - loss: 0.4044 - val_auc_46: 0.0000e+00 - val_loss: 0.4104\n",
      "Epoch 702/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9740 - loss: 0.4007 - val_auc_46: 0.0000e+00 - val_loss: 0.4107\n",
      "Epoch 703/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9729 - loss: 0.4070 - val_auc_46: 0.0000e+00 - val_loss: 0.4099\n",
      "Epoch 704/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9794 - loss: 0.3844 - val_auc_46: 0.0000e+00 - val_loss: 0.4100\n",
      "Epoch 705/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9733 - loss: 0.3999 - val_auc_46: 0.0000e+00 - val_loss: 0.4103\n",
      "Epoch 706/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9776 - loss: 0.3873 - val_auc_46: 0.0000e+00 - val_loss: 0.4091\n",
      "Epoch 707/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9736 - loss: 0.4011 - val_auc_46: 0.0000e+00 - val_loss: 0.4088\n",
      "Epoch 708/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9736 - loss: 0.3974 - val_auc_46: 0.0000e+00 - val_loss: 0.4086\n",
      "Epoch 709/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9720 - loss: 0.3990 - val_auc_46: 0.0000e+00 - val_loss: 0.4083\n",
      "Epoch 710/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9761 - loss: 0.3931 - val_auc_46: 0.0000e+00 - val_loss: 0.4078\n",
      "Epoch 711/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9746 - loss: 0.3946 - val_auc_46: 0.0000e+00 - val_loss: 0.4083\n",
      "Epoch 712/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9652 - loss: 0.4158 - val_auc_46: 0.0000e+00 - val_loss: 0.4075\n",
      "Epoch 713/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9749 - loss: 0.3990 - val_auc_46: 0.0000e+00 - val_loss: 0.4066\n",
      "Epoch 714/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9770 - loss: 0.3934 - val_auc_46: 0.0000e+00 - val_loss: 0.4055\n",
      "Epoch 715/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9710 - loss: 0.4000 - val_auc_46: 0.0000e+00 - val_loss: 0.4056\n",
      "Epoch 716/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9691 - loss: 0.4033 - val_auc_46: 0.0000e+00 - val_loss: 0.4050\n",
      "Epoch 717/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9830 - loss: 0.3710 - val_auc_46: 0.0000e+00 - val_loss: 0.4053\n",
      "Epoch 718/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9733 - loss: 0.4009 - val_auc_46: 0.0000e+00 - val_loss: 0.4053\n",
      "Epoch 719/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9671 - loss: 0.4109 - val_auc_46: 0.0000e+00 - val_loss: 0.4053\n",
      "Epoch 720/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9738 - loss: 0.3925 - val_auc_46: 0.0000e+00 - val_loss: 0.4048\n",
      "Epoch 721/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9774 - loss: 0.3883 - val_auc_46: 0.0000e+00 - val_loss: 0.4046\n",
      "Epoch 722/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9746 - loss: 0.3882 - val_auc_46: 0.0000e+00 - val_loss: 0.4041\n",
      "Epoch 723/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9751 - loss: 0.3884 - val_auc_46: 0.0000e+00 - val_loss: 0.4043\n",
      "Epoch 724/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9758 - loss: 0.3885 - val_auc_46: 0.0000e+00 - val_loss: 0.4036\n",
      "Epoch 725/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9770 - loss: 0.3895 - val_auc_46: 0.0000e+00 - val_loss: 0.4033\n",
      "Epoch 726/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9783 - loss: 0.3870 - val_auc_46: 0.0000e+00 - val_loss: 0.4026\n",
      "Epoch 727/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9691 - loss: 0.4031 - val_auc_46: 0.0000e+00 - val_loss: 0.4027\n",
      "Epoch 728/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9815 - loss: 0.3793 - val_auc_46: 0.0000e+00 - val_loss: 0.4026\n",
      "Epoch 729/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9713 - loss: 0.4011 - val_auc_46: 0.0000e+00 - val_loss: 0.4015\n",
      "Epoch 730/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9695 - loss: 0.3967 - val_auc_46: 0.0000e+00 - val_loss: 0.4016\n",
      "Epoch 731/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9715 - loss: 0.3982 - val_auc_46: 0.0000e+00 - val_loss: 0.4017\n",
      "Epoch 732/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9758 - loss: 0.3860 - val_auc_46: 0.0000e+00 - val_loss: 0.4013\n",
      "Epoch 733/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9766 - loss: 0.3883 - val_auc_46: 0.0000e+00 - val_loss: 0.4010\n",
      "Epoch 734/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9779 - loss: 0.3861 - val_auc_46: 0.0000e+00 - val_loss: 0.4005\n",
      "Epoch 735/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9753 - loss: 0.3911 - val_auc_46: 0.0000e+00 - val_loss: 0.4002\n",
      "Epoch 736/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9741 - loss: 0.3897 - val_auc_46: 0.0000e+00 - val_loss: 0.3997\n",
      "Epoch 737/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9745 - loss: 0.3835 - val_auc_46: 0.0000e+00 - val_loss: 0.3992\n",
      "Epoch 738/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9712 - loss: 0.4006 - val_auc_46: 0.0000e+00 - val_loss: 0.3988\n",
      "Epoch 739/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9815 - loss: 0.3790 - val_auc_46: 0.0000e+00 - val_loss: 0.3986\n",
      "Epoch 740/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9771 - loss: 0.3860 - val_auc_46: 0.0000e+00 - val_loss: 0.3985\n",
      "Epoch 741/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9812 - loss: 0.3735 - val_auc_46: 0.0000e+00 - val_loss: 0.3977\n",
      "Epoch 742/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9687 - loss: 0.4009 - val_auc_46: 0.0000e+00 - val_loss: 0.3973\n",
      "Epoch 743/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9810 - loss: 0.3776 - val_auc_46: 0.0000e+00 - val_loss: 0.3977\n",
      "Epoch 744/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9740 - loss: 0.3921 - val_auc_46: 0.0000e+00 - val_loss: 0.3963\n",
      "Epoch 745/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9786 - loss: 0.3821 - val_auc_46: 0.0000e+00 - val_loss: 0.3962\n",
      "Epoch 746/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9768 - loss: 0.3884 - val_auc_46: 0.0000e+00 - val_loss: 0.3963\n",
      "Epoch 747/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9749 - loss: 0.3871 - val_auc_46: 0.0000e+00 - val_loss: 0.3962\n",
      "Epoch 748/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9771 - loss: 0.3826 - val_auc_46: 0.0000e+00 - val_loss: 0.3962\n",
      "Epoch 749/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9775 - loss: 0.3778 - val_auc_46: 0.0000e+00 - val_loss: 0.3964\n",
      "Epoch 750/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9729 - loss: 0.3877 - val_auc_46: 0.0000e+00 - val_loss: 0.3961\n",
      "Epoch 751/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9709 - loss: 0.3970 - val_auc_46: 0.0000e+00 - val_loss: 0.3960\n",
      "Epoch 752/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9763 - loss: 0.3818 - val_auc_46: 0.0000e+00 - val_loss: 0.3956\n",
      "Epoch 753/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9769 - loss: 0.3819 - val_auc_46: 0.0000e+00 - val_loss: 0.3952\n",
      "Epoch 754/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9700 - loss: 0.3953 - val_auc_46: 0.0000e+00 - val_loss: 0.3951\n",
      "Epoch 755/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3832 - val_auc_46: 0.0000e+00 - val_loss: 0.3954\n",
      "Epoch 756/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9790 - loss: 0.3745 - val_auc_46: 0.0000e+00 - val_loss: 0.3954\n",
      "Epoch 757/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9767 - loss: 0.3792 - val_auc_46: 0.0000e+00 - val_loss: 0.3947\n",
      "Epoch 758/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9789 - loss: 0.3772 - val_auc_46: 0.0000e+00 - val_loss: 0.3947\n",
      "Epoch 759/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9695 - loss: 0.4039 - val_auc_46: 0.0000e+00 - val_loss: 0.3934\n",
      "Epoch 760/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9717 - loss: 0.3822 - val_auc_46: 0.0000e+00 - val_loss: 0.3936\n",
      "Epoch 761/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9807 - loss: 0.3715 - val_auc_46: 0.0000e+00 - val_loss: 0.3936\n",
      "Epoch 762/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9696 - loss: 0.3951 - val_auc_46: 0.0000e+00 - val_loss: 0.3932\n",
      "Epoch 763/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9766 - loss: 0.3778 - val_auc_46: 0.0000e+00 - val_loss: 0.3929\n",
      "Epoch 764/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9806 - loss: 0.3668 - val_auc_46: 0.0000e+00 - val_loss: 0.3927\n",
      "Epoch 765/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9768 - loss: 0.3762 - val_auc_46: 0.0000e+00 - val_loss: 0.3923\n",
      "Epoch 766/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9820 - loss: 0.3666 - val_auc_46: 0.0000e+00 - val_loss: 0.3914\n",
      "Epoch 767/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9689 - loss: 0.3919 - val_auc_46: 0.0000e+00 - val_loss: 0.3913\n",
      "Epoch 768/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9686 - loss: 0.3976 - val_auc_46: 0.0000e+00 - val_loss: 0.3907\n",
      "Epoch 769/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9722 - loss: 0.3901 - val_auc_46: 0.0000e+00 - val_loss: 0.3912\n",
      "Epoch 770/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9775 - loss: 0.3747 - val_auc_46: 0.0000e+00 - val_loss: 0.3906\n",
      "Epoch 771/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9797 - loss: 0.3701 - val_auc_46: 0.0000e+00 - val_loss: 0.3902\n",
      "Epoch 772/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9771 - loss: 0.3771 - val_auc_46: 0.0000e+00 - val_loss: 0.3899\n",
      "Epoch 773/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9730 - loss: 0.3837 - val_auc_46: 0.0000e+00 - val_loss: 0.3899\n",
      "Epoch 774/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9779 - loss: 0.3755 - val_auc_46: 0.0000e+00 - val_loss: 0.3900\n",
      "Epoch 775/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9758 - loss: 0.3850 - val_auc_46: 0.0000e+00 - val_loss: 0.3892\n",
      "Epoch 776/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9693 - loss: 0.3960 - val_auc_46: 0.0000e+00 - val_loss: 0.3889\n",
      "Epoch 777/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9750 - loss: 0.3794 - val_auc_46: 0.0000e+00 - val_loss: 0.3886\n",
      "Epoch 778/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9723 - loss: 0.3889 - val_auc_46: 0.0000e+00 - val_loss: 0.3880\n",
      "Epoch 779/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9755 - loss: 0.3824 - val_auc_46: 0.0000e+00 - val_loss: 0.3879\n",
      "Epoch 780/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9784 - loss: 0.3727 - val_auc_46: 0.0000e+00 - val_loss: 0.3880\n",
      "Epoch 781/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9757 - loss: 0.3797 - val_auc_46: 0.0000e+00 - val_loss: 0.3880\n",
      "Epoch 782/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9817 - loss: 0.3670 - val_auc_46: 0.0000e+00 - val_loss: 0.3878\n",
      "Epoch 783/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9778 - loss: 0.3723 - val_auc_46: 0.0000e+00 - val_loss: 0.3870\n",
      "Epoch 784/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9741 - loss: 0.3806 - val_auc_46: 0.0000e+00 - val_loss: 0.3873\n",
      "Epoch 785/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9772 - loss: 0.3768 - val_auc_46: 0.0000e+00 - val_loss: 0.3867\n",
      "Epoch 786/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9726 - loss: 0.3889 - val_auc_46: 0.0000e+00 - val_loss: 0.3863\n",
      "Epoch 787/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9716 - loss: 0.3865 - val_auc_46: 0.0000e+00 - val_loss: 0.3866\n",
      "Epoch 788/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9754 - loss: 0.3797 - val_auc_46: 0.0000e+00 - val_loss: 0.3855\n",
      "Epoch 789/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9728 - loss: 0.3887 - val_auc_46: 0.0000e+00 - val_loss: 0.3859\n",
      "Epoch 790/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9789 - loss: 0.3700 - val_auc_46: 0.0000e+00 - val_loss: 0.3859\n",
      "Epoch 791/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9758 - loss: 0.3801 - val_auc_46: 0.0000e+00 - val_loss: 0.3853\n",
      "Epoch 792/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9714 - loss: 0.3870 - val_auc_46: 0.0000e+00 - val_loss: 0.3851\n",
      "Epoch 793/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9805 - loss: 0.3630 - val_auc_46: 0.0000e+00 - val_loss: 0.3848\n",
      "Epoch 794/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3780 - val_auc_46: 0.0000e+00 - val_loss: 0.3847\n",
      "Epoch 795/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9682 - loss: 0.3887 - val_auc_46: 0.0000e+00 - val_loss: 0.3844\n",
      "Epoch 796/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9775 - loss: 0.3709 - val_auc_46: 0.0000e+00 - val_loss: 0.3843\n",
      "Epoch 797/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9766 - loss: 0.3713 - val_auc_46: 0.0000e+00 - val_loss: 0.3844\n",
      "Epoch 798/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9751 - loss: 0.3757 - val_auc_46: 0.0000e+00 - val_loss: 0.3836\n",
      "Epoch 799/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9742 - loss: 0.3783 - val_auc_46: 0.0000e+00 - val_loss: 0.3830\n",
      "Epoch 800/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9754 - loss: 0.3734 - val_auc_46: 0.0000e+00 - val_loss: 0.3822\n",
      "Epoch 801/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9776 - loss: 0.3683 - val_auc_46: 0.0000e+00 - val_loss: 0.3823\n",
      "Epoch 802/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9721 - loss: 0.3807 - val_auc_46: 0.0000e+00 - val_loss: 0.3825\n",
      "Epoch 803/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9675 - loss: 0.3907 - val_auc_46: 0.0000e+00 - val_loss: 0.3826\n",
      "Epoch 804/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9801 - loss: 0.3646 - val_auc_46: 0.0000e+00 - val_loss: 0.3823\n",
      "Epoch 805/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9806 - loss: 0.3666 - val_auc_46: 0.0000e+00 - val_loss: 0.3817\n",
      "Epoch 806/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9777 - loss: 0.3686 - val_auc_46: 0.0000e+00 - val_loss: 0.3817\n",
      "Epoch 807/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9785 - loss: 0.3698 - val_auc_46: 0.0000e+00 - val_loss: 0.3812\n",
      "Epoch 808/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9795 - loss: 0.3705 - val_auc_46: 0.0000e+00 - val_loss: 0.3814\n",
      "Epoch 809/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9755 - loss: 0.3716 - val_auc_46: 0.0000e+00 - val_loss: 0.3811\n",
      "Epoch 810/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9799 - loss: 0.3626 - val_auc_46: 0.0000e+00 - val_loss: 0.3808\n",
      "Epoch 811/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9772 - loss: 0.3717 - val_auc_46: 0.0000e+00 - val_loss: 0.3810\n",
      "Epoch 812/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9776 - loss: 0.3770 - val_auc_46: 0.0000e+00 - val_loss: 0.3807\n",
      "Epoch 813/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9809 - loss: 0.3589 - val_auc_46: 0.0000e+00 - val_loss: 0.3804\n",
      "Epoch 814/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9788 - loss: 0.3621 - val_auc_46: 0.0000e+00 - val_loss: 0.3805\n",
      "Epoch 815/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9763 - loss: 0.3712 - val_auc_46: 0.0000e+00 - val_loss: 0.3803\n",
      "Epoch 816/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9764 - loss: 0.3724 - val_auc_46: 0.0000e+00 - val_loss: 0.3797\n",
      "Epoch 817/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3734 - val_auc_46: 0.0000e+00 - val_loss: 0.3799\n",
      "Epoch 818/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9789 - loss: 0.3643 - val_auc_46: 0.0000e+00 - val_loss: 0.3794\n",
      "Epoch 819/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9766 - loss: 0.3672 - val_auc_46: 0.0000e+00 - val_loss: 0.3791\n",
      "Epoch 820/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9799 - loss: 0.3601 - val_auc_46: 0.0000e+00 - val_loss: 0.3787\n",
      "Epoch 821/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9788 - loss: 0.3682 - val_auc_46: 0.0000e+00 - val_loss: 0.3783\n",
      "Epoch 822/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9764 - loss: 0.3711 - val_auc_46: 0.0000e+00 - val_loss: 0.3782\n",
      "Epoch 823/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9726 - loss: 0.3777 - val_auc_46: 0.0000e+00 - val_loss: 0.3779\n",
      "Epoch 824/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9745 - loss: 0.3737 - val_auc_46: 0.0000e+00 - val_loss: 0.3780\n",
      "Epoch 825/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9781 - loss: 0.3669 - val_auc_46: 0.0000e+00 - val_loss: 0.3781\n",
      "Epoch 826/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9796 - loss: 0.3638 - val_auc_46: 0.0000e+00 - val_loss: 0.3780\n",
      "Epoch 827/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9790 - loss: 0.3622 - val_auc_46: 0.0000e+00 - val_loss: 0.3777\n",
      "Epoch 828/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9810 - loss: 0.3587 - val_auc_46: 0.0000e+00 - val_loss: 0.3773\n",
      "Epoch 829/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9744 - loss: 0.3762 - val_auc_46: 0.0000e+00 - val_loss: 0.3774\n",
      "Epoch 830/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9773 - loss: 0.3720 - val_auc_46: 0.0000e+00 - val_loss: 0.3771\n",
      "Epoch 831/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9701 - loss: 0.3801 - val_auc_46: 0.0000e+00 - val_loss: 0.3767\n",
      "Epoch 832/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9818 - loss: 0.3527 - val_auc_46: 0.0000e+00 - val_loss: 0.3772\n",
      "Epoch 833/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9820 - loss: 0.3539 - val_auc_46: 0.0000e+00 - val_loss: 0.3769\n",
      "Epoch 834/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9791 - loss: 0.3604 - val_auc_46: 0.0000e+00 - val_loss: 0.3760\n",
      "Epoch 835/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9711 - loss: 0.3799 - val_auc_46: 0.0000e+00 - val_loss: 0.3761\n",
      "Epoch 836/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9798 - loss: 0.3604 - val_auc_46: 0.0000e+00 - val_loss: 0.3758\n",
      "Epoch 837/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9760 - loss: 0.3711 - val_auc_46: 0.0000e+00 - val_loss: 0.3757\n",
      "Epoch 838/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9811 - loss: 0.3542 - val_auc_46: 0.0000e+00 - val_loss: 0.3755\n",
      "Epoch 839/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9788 - loss: 0.3606 - val_auc_46: 0.0000e+00 - val_loss: 0.3751\n",
      "Epoch 840/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9769 - loss: 0.3644 - val_auc_46: 0.0000e+00 - val_loss: 0.3752\n",
      "Epoch 841/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9778 - loss: 0.3559 - val_auc_46: 0.0000e+00 - val_loss: 0.3753\n",
      "Epoch 842/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9827 - loss: 0.3494 - val_auc_46: 0.0000e+00 - val_loss: 0.3744\n",
      "Epoch 843/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9793 - loss: 0.3627 - val_auc_46: 0.0000e+00 - val_loss: 0.3739\n",
      "Epoch 844/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9766 - loss: 0.3622 - val_auc_46: 0.0000e+00 - val_loss: 0.3741\n",
      "Epoch 845/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9762 - loss: 0.3716 - val_auc_46: 0.0000e+00 - val_loss: 0.3739\n",
      "Epoch 846/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9756 - loss: 0.3683 - val_auc_46: 0.0000e+00 - val_loss: 0.3735\n",
      "Epoch 847/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9724 - loss: 0.3720 - val_auc_46: 0.0000e+00 - val_loss: 0.3735\n",
      "Epoch 848/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9754 - loss: 0.3683 - val_auc_46: 0.0000e+00 - val_loss: 0.3739\n",
      "Epoch 849/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9819 - loss: 0.3546 - val_auc_46: 0.0000e+00 - val_loss: 0.3738\n",
      "Epoch 850/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9777 - loss: 0.3583 - val_auc_46: 0.0000e+00 - val_loss: 0.3736\n",
      "Epoch 851/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9805 - loss: 0.3570 - val_auc_46: 0.0000e+00 - val_loss: 0.3729\n",
      "Epoch 852/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9811 - loss: 0.3530 - val_auc_46: 0.0000e+00 - val_loss: 0.3722\n",
      "Epoch 853/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9797 - loss: 0.3543 - val_auc_46: 0.0000e+00 - val_loss: 0.3723\n",
      "Epoch 854/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9808 - loss: 0.3561 - val_auc_46: 0.0000e+00 - val_loss: 0.3723\n",
      "Epoch 855/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3676 - val_auc_46: 0.0000e+00 - val_loss: 0.3716\n",
      "Epoch 856/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9817 - loss: 0.3512 - val_auc_46: 0.0000e+00 - val_loss: 0.3712\n",
      "Epoch 857/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9789 - loss: 0.3621 - val_auc_46: 0.0000e+00 - val_loss: 0.3710\n",
      "Epoch 858/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9760 - loss: 0.3644 - val_auc_46: 0.0000e+00 - val_loss: 0.3708\n",
      "Epoch 859/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9766 - loss: 0.3648 - val_auc_46: 0.0000e+00 - val_loss: 0.3709\n",
      "Epoch 860/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9759 - loss: 0.3675 - val_auc_46: 0.0000e+00 - val_loss: 0.3704\n",
      "Epoch 861/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9763 - loss: 0.3706 - val_auc_46: 0.0000e+00 - val_loss: 0.3699\n",
      "Epoch 862/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9793 - loss: 0.3554 - val_auc_46: 0.0000e+00 - val_loss: 0.3702\n",
      "Epoch 863/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9793 - loss: 0.3557 - val_auc_46: 0.0000e+00 - val_loss: 0.3699\n",
      "Epoch 864/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9759 - loss: 0.3662 - val_auc_46: 0.0000e+00 - val_loss: 0.3700\n",
      "Epoch 865/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9767 - loss: 0.3622 - val_auc_46: 0.0000e+00 - val_loss: 0.3697\n",
      "Epoch 866/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9818 - loss: 0.3583 - val_auc_46: 0.0000e+00 - val_loss: 0.3691\n",
      "Epoch 867/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9777 - loss: 0.3595 - val_auc_46: 0.0000e+00 - val_loss: 0.3694\n",
      "Epoch 868/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9771 - loss: 0.3630 - val_auc_46: 0.0000e+00 - val_loss: 0.3695\n",
      "Epoch 869/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9785 - loss: 0.3625 - val_auc_46: 0.0000e+00 - val_loss: 0.3691\n",
      "Epoch 870/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9793 - loss: 0.3476 - val_auc_46: 0.0000e+00 - val_loss: 0.3691\n",
      "Epoch 871/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9795 - loss: 0.3566 - val_auc_46: 0.0000e+00 - val_loss: 0.3689\n",
      "Epoch 872/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9835 - loss: 0.3494 - val_auc_46: 0.0000e+00 - val_loss: 0.3683\n",
      "Epoch 873/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9842 - loss: 0.3445 - val_auc_46: 0.0000e+00 - val_loss: 0.3688\n",
      "Epoch 874/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9818 - loss: 0.3503 - val_auc_46: 0.0000e+00 - val_loss: 0.3687\n",
      "Epoch 875/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9789 - loss: 0.3526 - val_auc_46: 0.0000e+00 - val_loss: 0.3682\n",
      "Epoch 876/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9717 - loss: 0.3713 - val_auc_46: 0.0000e+00 - val_loss: 0.3680\n",
      "Epoch 877/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9826 - loss: 0.3432 - val_auc_46: 0.0000e+00 - val_loss: 0.3682\n",
      "Epoch 878/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9792 - loss: 0.3577 - val_auc_46: 0.0000e+00 - val_loss: 0.3675\n",
      "Epoch 879/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9801 - loss: 0.3511 - val_auc_46: 0.0000e+00 - val_loss: 0.3675\n",
      "Epoch 880/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9801 - loss: 0.3610 - val_auc_46: 0.0000e+00 - val_loss: 0.3672\n",
      "Epoch 881/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9755 - loss: 0.3654 - val_auc_46: 0.0000e+00 - val_loss: 0.3672\n",
      "Epoch 882/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9793 - loss: 0.3475 - val_auc_46: 0.0000e+00 - val_loss: 0.3668\n",
      "Epoch 883/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9813 - loss: 0.3501 - val_auc_46: 0.0000e+00 - val_loss: 0.3670\n",
      "Epoch 884/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9808 - loss: 0.3525 - val_auc_46: 0.0000e+00 - val_loss: 0.3660\n",
      "Epoch 885/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9765 - loss: 0.3592 - val_auc_46: 0.0000e+00 - val_loss: 0.3659\n",
      "Epoch 886/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9760 - loss: 0.3687 - val_auc_46: 0.0000e+00 - val_loss: 0.3663\n",
      "Epoch 887/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9754 - loss: 0.3663 - val_auc_46: 0.0000e+00 - val_loss: 0.3660\n",
      "Epoch 888/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9801 - loss: 0.3544 - val_auc_46: 0.0000e+00 - val_loss: 0.3649\n",
      "Epoch 889/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9810 - loss: 0.3564 - val_auc_46: 0.0000e+00 - val_loss: 0.3646\n",
      "Epoch 890/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9792 - loss: 0.3547 - val_auc_46: 0.0000e+00 - val_loss: 0.3643\n",
      "Epoch 891/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9800 - loss: 0.3488 - val_auc_46: 0.0000e+00 - val_loss: 0.3645\n",
      "Epoch 892/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9784 - loss: 0.3527 - val_auc_46: 0.0000e+00 - val_loss: 0.3643\n",
      "Epoch 893/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9802 - loss: 0.3561 - val_auc_46: 0.0000e+00 - val_loss: 0.3638\n",
      "Epoch 894/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9806 - loss: 0.3511 - val_auc_46: 0.0000e+00 - val_loss: 0.3640\n",
      "Epoch 895/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9781 - loss: 0.3533 - val_auc_46: 0.0000e+00 - val_loss: 0.3639\n",
      "Epoch 896/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9779 - loss: 0.3528 - val_auc_46: 0.0000e+00 - val_loss: 0.3642\n",
      "Epoch 897/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9810 - loss: 0.3501 - val_auc_46: 0.0000e+00 - val_loss: 0.3635\n",
      "Epoch 898/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9775 - loss: 0.3529 - val_auc_46: 0.0000e+00 - val_loss: 0.3633\n",
      "Epoch 899/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9705 - loss: 0.3652 - val_auc_46: 0.0000e+00 - val_loss: 0.3636\n",
      "Epoch 900/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9794 - loss: 0.3557 - val_auc_46: 0.0000e+00 - val_loss: 0.3630\n",
      "Epoch 901/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9760 - loss: 0.3578 - val_auc_46: 0.0000e+00 - val_loss: 0.3628\n",
      "Epoch 902/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9788 - loss: 0.3552 - val_auc_46: 0.0000e+00 - val_loss: 0.3621\n",
      "Epoch 903/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9804 - loss: 0.3517 - val_auc_46: 0.0000e+00 - val_loss: 0.3612\n",
      "Epoch 904/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9787 - loss: 0.3540 - val_auc_46: 0.0000e+00 - val_loss: 0.3613\n",
      "Epoch 905/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9837 - loss: 0.3398 - val_auc_46: 0.0000e+00 - val_loss: 0.3612\n",
      "Epoch 906/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3545 - val_auc_46: 0.0000e+00 - val_loss: 0.3613\n",
      "Epoch 907/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9749 - loss: 0.3641 - val_auc_46: 0.0000e+00 - val_loss: 0.3616\n",
      "Epoch 908/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9802 - loss: 0.3445 - val_auc_46: 0.0000e+00 - val_loss: 0.3610\n",
      "Epoch 909/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9782 - loss: 0.3512 - val_auc_46: 0.0000e+00 - val_loss: 0.3608\n",
      "Epoch 910/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9694 - loss: 0.3763 - val_auc_46: 0.0000e+00 - val_loss: 0.3604\n",
      "Epoch 911/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9793 - loss: 0.3495 - val_auc_46: 0.0000e+00 - val_loss: 0.3610\n",
      "Epoch 912/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9782 - loss: 0.3556 - val_auc_46: 0.0000e+00 - val_loss: 0.3608\n",
      "Epoch 913/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9822 - loss: 0.3412 - val_auc_46: 0.0000e+00 - val_loss: 0.3607\n",
      "Epoch 914/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9751 - loss: 0.3655 - val_auc_46: 0.0000e+00 - val_loss: 0.3599\n",
      "Epoch 915/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9795 - loss: 0.3452 - val_auc_46: 0.0000e+00 - val_loss: 0.3601\n",
      "Epoch 916/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9738 - loss: 0.3607 - val_auc_46: 0.0000e+00 - val_loss: 0.3600\n",
      "Epoch 917/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9783 - loss: 0.3482 - val_auc_46: 0.0000e+00 - val_loss: 0.3598\n",
      "Epoch 918/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9782 - loss: 0.3521 - val_auc_46: 0.0000e+00 - val_loss: 0.3597\n",
      "Epoch 919/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9798 - loss: 0.3441 - val_auc_46: 0.0000e+00 - val_loss: 0.3596\n",
      "Epoch 920/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9776 - loss: 0.3515 - val_auc_46: 0.0000e+00 - val_loss: 0.3595\n",
      "Epoch 921/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9727 - loss: 0.3584 - val_auc_46: 0.0000e+00 - val_loss: 0.3594\n",
      "Epoch 922/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9800 - loss: 0.3437 - val_auc_46: 0.0000e+00 - val_loss: 0.3590\n",
      "Epoch 923/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9821 - loss: 0.3366 - val_auc_46: 0.0000e+00 - val_loss: 0.3594\n",
      "Epoch 924/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9772 - loss: 0.3567 - val_auc_46: 0.0000e+00 - val_loss: 0.3596\n",
      "Epoch 925/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9825 - loss: 0.3374 - val_auc_46: 0.0000e+00 - val_loss: 0.3588\n",
      "Epoch 926/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9783 - loss: 0.3533 - val_auc_46: 0.0000e+00 - val_loss: 0.3587\n",
      "Epoch 927/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9790 - loss: 0.3480 - val_auc_46: 0.0000e+00 - val_loss: 0.3583\n",
      "Epoch 928/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9791 - loss: 0.3448 - val_auc_46: 0.0000e+00 - val_loss: 0.3577\n",
      "Epoch 929/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9838 - loss: 0.3370 - val_auc_46: 0.0000e+00 - val_loss: 0.3575\n",
      "Epoch 930/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9779 - loss: 0.3556 - val_auc_46: 0.0000e+00 - val_loss: 0.3571\n",
      "Epoch 931/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9733 - loss: 0.3619 - val_auc_46: 0.0000e+00 - val_loss: 0.3570\n",
      "Epoch 932/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - auc_46: 0.9838 - loss: 0.3316 - val_auc_46: 0.0000e+00 - val_loss: 0.3575\n",
      "Epoch 933/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9757 - loss: 0.3575 - val_auc_46: 0.0000e+00 - val_loss: 0.3574\n",
      "Epoch 934/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9767 - loss: 0.3515 - val_auc_46: 0.0000e+00 - val_loss: 0.3572\n",
      "Epoch 935/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - auc_46: 0.9761 - loss: 0.3527 - val_auc_46: 0.0000e+00 - val_loss: 0.3571\n",
      "Epoch 936/1000\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - auc_46: 0.9807 - loss: 0.3401 - val_auc_46: 0.0000e+00 - val_loss: 0.3578\n"
     ]
    }
   ],
   "source": [
    "model = train_NN(X_train_w_models,y_train,neurons=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "76d1ad57-5e14-4c10-bc58-a8b23e9f9a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    }
   ],
   "source": [
    "nn_predict = model.predict(X_test_w_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a5e4f695-3878-48b8-9c58-14016c37a8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL ENSEMBLE NN:  0.822063900915219\n"
     ]
    }
   ],
   "source": [
    "print(\"FINAL ENSEMBLE NN: \",roc_auc_score(y_test,nn_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d667ffbc-ecc7-49fe-80a8-7302d23ffc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for running a random search over a keras model, acts as a first pass for model design\n",
    "auc = keras.metrics.AUC()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "def build_random_model(hp):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    j = 0\n",
    "    for i in range(1, hp.Int(\"num_layers\", 2, 4)):\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=32),\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l1=1e-2,l2=1e-2),\n",
    "                activation=\"relu\",\n",
    "                name='Hidden-Layer-'+str(i))\n",
    "            )\n",
    "        \n",
    "        # Tune dropout layer with values from 0 - 0.3 with stepsize of 0.1.\n",
    "        model.add(keras.layers.Dropout(0.5)) \n",
    "    \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid',name='Output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=opt, loss=loss,metrics=[auc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "1d43d51b-46c6-4114-94f3-db4ebbbd1074",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 300 Complete [00h 00m 22s]\n",
      "val_loss: 0.7240146398544312\n",
      "\n",
      "Best val_loss So Far: 0.6984740495681763\n",
      "Total elapsed time: 01h 53m 45s\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_random_model,\n",
    "    objective=keras_tuner.Objective('val_loss','min'),\n",
    "    max_trials=300,\n",
    "    overwrite=True,\n",
    "    directory=\"random_search\",\n",
    "    project_name=\"v1\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=200, validation_split=0.1,batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "89430b35-9c88-4827-aa80-66be02860085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden-Layer-1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">25,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden-Layer-1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │        \u001b[38;5;34m25,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m97\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,153</span> (98.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,153\u001b[0m (98.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,153</span> (98.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,153\u001b[0m (98.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = tuner.get_best_models(num_models=1)\n",
    "best_model = models[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf337ca2-0fbf-44aa-b5b6-b5b92c1bbe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9774d88c-33bf-4d7e-83c4-bf5ea8399537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2106, 260)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "de6c6c20-56be-40a9-a65f-2a4c48e72b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#L1L2 seems no good\n",
    "# learning rate = 1e-5 essential bigger learning rate val error just noise\n",
    "# Want later layers to be smaller than earlier ones\n",
    "# Not sure about optimal dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e4b6f69-77d7-422b-a22e-f2d99d1aa382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units_1': 512, 'dropout_1': 0.1, 'units_2': 288, 'dropout_2': 0.2, 'units_3': 320, 'dropout_3': 0.1, 'units_4': 96, 'dropout_4': 0.1, 'units_5': 128, 'dropout_5': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Second pass grid search centered on best params found in random search\n",
    "auc = keras.metrics.AUC()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "best_params = tuner.get_best_hyperparameters()[0].values\n",
    "print(best_params)\n",
    "def build_grid_model(hp):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    for i in range(1, best_params['num_layers']+1):\n",
    "        units = \"units_\" + str(i)\n",
    "        name = 'Hidden-Layer-'+str(i)\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(units, min_value=32, max_value=1024, step=32, default=best_params[units]),\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l1=1e-4,l2=1e-3),\n",
    "                activation=\"relu\",\n",
    "                name=name,\n",
    "            ))\n",
    "        \n",
    "        # Add dropout to all layers except last one\n",
    "        if i != 5:\n",
    "            model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid',name='Output'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss,metrics=[auc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1595e699-6141-4170-9ec5-130f2dd5e89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'num_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tune binary cross entropy directly as that is the loss function in the model and training AUC may lead to overfitting\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mGridSearch(\n\u001b[1;32m      3\u001b[0m     build_grid_model,\n\u001b[1;32m      4\u001b[0m     objective\u001b[38;5;241m=\u001b[39mkeras_tuner\u001b[38;5;241m.\u001b[39mObjective(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mloss\u001b[38;5;241m.\u001b[39mname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      6\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid_search\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/tuners/gridsearch.py:419\u001b[0m, in \u001b[0;36mGridSearch.__init__\u001b[0;34m(self, hypermodel, objective, max_trials, seed, hyperparameters, tune_new_entries, allow_new_entries, max_retries_per_trial, max_consecutive_failed_trials, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m    409\u001b[0m oracle \u001b[38;5;241m=\u001b[39m GridSearchOracle(\n\u001b[1;32m    410\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m    411\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39mmax_trials,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     max_consecutive_failed_trials\u001b[38;5;241m=\u001b[39mmax_consecutive_failed_trials,\n\u001b[1;32m    418\u001b[0m )\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(oracle, hypermodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:122\u001b[0m, in \u001b[0;36mTuner.__init__\u001b[0;34m(self, oracle, hypermodel, max_model_size, optimizer, loss, metrics, distribution_strategy, directory, project_name, logger, tuner_id, overwrite, executions_per_trial, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hypermodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial \u001b[38;5;129;01mis\u001b[39;00m Tuner\u001b[38;5;241m.\u001b[39mrun_trial:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived `hypermodel=None`. We only allow not specifying \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`hypermodel` if the user defines the search space in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Tuner.run_trial()` by subclassing a `Tuner` class without \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing a `HyperModel` instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    123\u001b[0m     oracle\u001b[38;5;241m=\u001b[39moracle,\n\u001b[1;32m    124\u001b[0m     hypermodel\u001b[38;5;241m=\u001b[39mhypermodel,\n\u001b[1;32m    125\u001b[0m     directory\u001b[38;5;241m=\u001b[39mdirectory,\n\u001b[1;32m    126\u001b[0m     project_name\u001b[38;5;241m=\u001b[39mproject_name,\n\u001b[1;32m    127\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m    128\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39moverwrite,\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_model_size \u001b[38;5;241m=\u001b[39m max_model_size\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optimizer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:132\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, oracle, hypermodel, directory, project_name, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreload()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Only populate initial space if not reloading.\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_populate_initial_space()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Run in distributed mode.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mhas_chief_oracle() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dist_utils\u001b[38;5;241m.\u001b[39mis_chief_oracle():\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Proxies requests to the chief oracle.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# Avoid import at the top, to avoid inconsistent protobuf versions.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:192\u001b[0m, in \u001b[0;36mBaseTuner._populate_initial_space\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mdeclare_hyperparameters(hp)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activate_all_conditions()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:149\u001b[0m, in \u001b[0;36mBaseTuner._activate_all_conditions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_space()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(hp)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mupdate_space(hp)\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Update the recorded scopes.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 10\u001b[0m, in \u001b[0;36mbuild_grid_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_grid_model\u001b[39m(hp):\n\u001b[1;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, best_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     11\u001b[0m         units \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)\n\u001b[1;32m     12\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHidden-Layer-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'num_layers'"
     ]
    }
   ],
   "source": [
    "# Tune binary cross entropy directly as that is the loss function in the model and training AUC may lead to overfitting\n",
    "tuner = keras_tuner.GridSearch(\n",
    "    build_grid_model,\n",
    "    objective=keras_tuner.Objective('val_'+loss.name,'min'),\n",
    "    max_trials=300,\n",
    "    overwrite=True,\n",
    "    directory=\"grid_search\",\n",
    "    project_name=\"v1\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2f4b54c-bb1d-4542-89b9-f922301dd777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 5, 'units_1': 512, 'l1': 0.0001, 'l2': 0.001, 'dropout_1': 0.1, 'units_2': 288, 'dropout_2': 0.2, 'units_3': 320, 'dropout_3': 0.1, 'units_4': 96, 'dropout_4': 0.1, 'units_5': 128, 'dropout_5': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Random search result :0.873 val AUC\n",
    "# {'num_layers': 5, 'units_1': 512, 'l1': 0.0001, 'l2': 0.001, 'dropout_1': 0.1, 'units_2': 288, 'dropout_2': 0.2, 'units_3': 320, 'dropout_3': 0.1, 'units_4': 96, 'dropout_4': 0.1, 'units_5': 128, 'dropout_5': 0.0}\n",
    "best_params = tuner.get_best_hyperparameters()\n",
    "\n",
    "for params in best_params:\n",
    "    print(params.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3deb17f-4d3a-499b-8f3d-390295dbd867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n",
      "/Users/benware/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:869: UserWarning: pandas.DataFrame with sparse columns found.It will be converted to a dense numpy array.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2742,) (2116,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAulklEQVR4nO3df1xVVb7/8fcBPQcQQU0BFRLTyTJNS5OwzB8xkprdmluRlqK3tPJnMU1qJqQ2ak16nTGVtPxxGxssr3Z9pKMZZT+UslAeU6E25s+ZBLUSCA2Es75/9PXMEKhgcA5n+Xo+Hvsxc9ZZa+/PXpnn3d5rn+MwxhgBAABYIsDXBQAAANQmwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDXAJW7FihRwOhw4ePOjrUlADW7dulcPh0NatW31dClAvEW4ACyxatEgOh0NxcXFeO+aIESPkcDg8W4MGDRQTE6P77rtPubm5XqvjXHJzc/XMM89UK7gZY9SrVy+1aNFC3377baX3H3nkETVs2FA5OTnVPv6iRYu0YsWK6hcMoNYQbgALrFq1SrGxsdqxY4f27dvnteO6XC69+uqrevXVV/Xyyy9rxIgRyszMVM+ePfXNN994rY6q5Obmavr06dUKNw6HQy+99JIKCgr0xBNPVHgvKytLS5Ys0cSJE9W1a9dqH78uw80tt9yi06dP65ZbbqmT/QP+jnAD+LkDBw5o+/btmjdvnlq0aKFVq1Z57dgNGjTQAw88oAceeEAjRozQzJkztWLFChUUFGjDhg1eq6M2dOzYUb/73e+0YsUKvf/++5KkM2fOaPTo0YqJidH06dPr7NjFxcU16h8QEKCgoCAFBPBXOFAV/s0A/NyqVavUtGlTDRo0SHffffc5w82XX36pfv36KTg4WNHR0Xr22Wfldrsr9fu///s/DRo0SK1atZLL5VK7du00c+ZMlZeXV6ueqKgoST8Fn3+3f/9+3XPPPWrWrJlCQkJ04403VhmAjh07pgcffFCRkZEKCgpSly5dtHLlykr9MjIy1K1bNzVu3FhhYWHq3Lmz/vjHP0r6aS3RPffcI0nq27ev59bZhdaoTJs2Te3atdPDDz+s0tJSzZ07V1988YVefPFFNWrUqFrnL0mxsbH68ssv9f7773uO3adPH09tDodD77//vsaMGaOIiAhFR0dLkg4dOqQxY8aoQ4cOCg4O1mWXXaZ77rmn0tWnqtbc9OnTR506dVJubq769u2rkJAQtW7dWs8//3y16wZs0eDCXQDUZ6tWrdJvfvMbOZ1ODRkyRIsXL9ann36qG264wdMnLy9Pffv2VVlZmSZPnqxGjRppyZIlCg4OrrS/FStWKDQ0VCkpKQoNDdW7776r1NRUFRYW6g9/+EOl/idOnJAklZeXa//+/Zo0aZIuu+wy3X777Z4++fn56tmzp06dOqUJEybosssu08qVK3XHHXdozZo1uuuuuyRJp0+fVp8+fbRv3z6NGzdObdu21RtvvKERI0bo5MmTmjhxoiRpy5YtGjJkiG699VY999xzkqTdu3dr27Ztmjhxom655RZNmDBBf/rTn/TUU0/p6quvliTP/55LUFCQFi1apMTERI0ZM0avvfaa7rrrLg0ePLgm/0g0f/58jR8/XqGhoZo6daokKTIyskKfMWPGqEWLFkpNTfVcufn000+1fft23XfffYqOjtbBgwe1ePFi9enTR7m5uQoJCTnvcb///nvddttt+s1vfqN7771Xa9as0aRJk9S5c2cNGDCgRucA+DUDwG999tlnRpLZsmWLMcYYt9ttoqOjzcSJEyv0e+yxx4wk88knn3jajh07ZsLDw40kc+DAAU/7qVOnKh3n4YcfNiEhIebHH3/0tCUnJxtJlbbWrVub7OzsKo//4YcfetqKiopM27ZtTWxsrCkvLzfGGDN//nwjyfz5z3/29CstLTXx8fEmNDTUFBYWGmOMmThxogkLCzNlZWXnnJs33njDSDLvvffeOfucy5AhQ4wk07hxY3PkyJEajzfGmGuuucb07t27Uvvy5cuNJHPzzTdXqr+quc/KyjKSzP/8z/942t57771K59a7d+9K/UpKSkxUVJT5z//8z4s6B8BfcVsK8GOrVq1SZGSk+vbtK+mnhbFJSUnKyMiocBtp48aNuvHGG9WjRw9PW4sWLXT//fdX2ue/X80pKirSiRMn1KtXL506dUp79uyp0DcoKEhbtmzRli1btHnzZr300ksKDQ3VwIED9dVXX1U4fo8ePXTzzTd72kJDQzV69GgdPHjQ83TVxo0bFRUVpSFDhnj6NWzYUBMmTNAPP/zgWQvTpEkTFRcXa8uWLRc1bxfSvHlzST+twzl7y6i2jRo1SoGBgRXa/n3uz5w5o2+//Vbt27dXkyZNtHPnzgvuMzQ0VA888IDntdPpVI8ePbR///7aKxzwA4QbwE+Vl5crIyNDffv21YEDB7Rv3z7t27dPcXFxys/PV2ZmpqfvoUOH9Ktf/arSPjp06FCp7csvv9Rdd92l8PBwhYWFqUWLFp4PzIKCggp9AwMDlZCQoISEBPXv31+jR4/WO++8o4KCAk2ZMqXC8as61tnbRIcOHapQ588Xyv6835gxY3TllVdqwIABio6O1n/9139p06ZNF560avjss8+0cOFCderUSZ988on+/Oc/18p+f65t27aV2k6fPq3U1FTFxMTI5XKpefPmatGihU6ePFlp7qsSHR0th8NRoa1p06b6/vvva61uwB+w5gbwU++++66OHj2qjIwMZWRkVHp/1apV6t+/f432efLkSfXu3VthYWGaMWOG2rVrp6CgIO3cuVOTJk2qcgHyz0VHR6tDhw764IMPanTsmoiIiFBOTo42b96sv/71r/rrX/+q5cuXa/jw4VUuPq6u8vJyjR49Wq1atdK2bdvUv39//fa3v9Xtt9+uJk2a1N4JSFWudxo/fryWL1+uxx57TPHx8QoPD5fD4dB9991Xrbn/+ZWgs4wxv7hewJ8QbgA/tWrVKkVERGjhwoWV3lu7dq3WrVun9PR0BQcHq02bNvr73/9eqd/evXsrvN66dau+/fZbrV27tsJ3qBw4cKBGtZWVlemHH37wvG7Tpk2lY0ny3OZq06aN53//9re/ye12V7h68/N+0k+3XAYPHqzBgwfL7XZrzJgxeumllzRt2jS1b9++0hWM6vjTn/6kXbt2ad26dQoLC1N6erq6d++uyZMnKz09vUb7upjjr1mzRsnJyZo7d66n7ccff9TJkydrvC/gUsZtKcAPnT59WmvXrtXtt9+uu+++u9I2btw4FRUVaf369ZKkgQMH6uOPP9aOHTs8+zh+/Hilx8bP/pf/v/+XfmlpqRYtWlTt2r766ivt3btXXbp08bQNHDhQO3bsUFZWlqetuLhYS5YsUWxsrDp27Ojpl5eXp9WrV3v6lZWVacGCBQoNDVXv3r0lqdK3CAcEBOjaa6+VJJWUlEiS59Ht6gaDI0eOKDU1VXfccYfuvPNOSVLXrl01YcIELV26VJ988km15+Ds8WsaSgIDAytdZVmwYEG1H8MH8BOu3AB+aP369SoqKtIdd9xR5fs33nij5wv9kpKS9OSTT+rVV1/VbbfdpokTJ3oeBT97peSsnj17qmnTpkpOTtaECRPkcDj06quvnvO2RllZmWdNitvt1sGDB5Weni632620tDRPv8mTJ+svf/mLBgwYoAkTJqhZs2ZauXKlDhw4oP/93//1XKUZPXq0XnrpJY0YMULZ2dmKjY3VmjVrtG3bNs2fP1+NGzeWJD300EP67rvv1K9fP0VHR+vQoUNasGCBunbt6lmf07VrVwUGBuq5555TQUGBXC6X+vXrp4iIiCrPZfz48TLGaMGCBRXap0+frtdff12PPPKIPvvss3Pe+vm5bt26afHixXr22WfVvn17RUREqF+/fucdc/vtt+vVV19VeHi4OnbsqKysLL3zzju67LLLqnVMAP+fT5/VAnBRBg8ebIKCgkxxcfE5+4wYMcI0bNjQnDhxwhhjzN/+9jfTu3dvExQUZFq3bm1mzpxpXnnllUqPgm/bts3ceOONJjg42LRq1co8+eSTZvPmzZUePa7qUfCwsDBz6623mnfeeadSPV9//bW5++67TZMmTUxQUJDp0aOHeeuttyr1y8/PNyNHjjTNmzc3TqfTdO7c2SxfvrxCnzVr1pj+/fubiIgI43Q6zeWXX24efvhhc/To0Qr9li5daq644goTGBh43sfC161bZySZF154ocr316xZYySZefPmVfl+VfLy8sygQYNM48aNjSTPY+FnHwX/9NNPK435/vvvPeceGhpqEhMTzZ49e0ybNm1McnKyp9+5HgW/5pprKu0zOTnZtGnTptp1AzZwGMNKMwAAYA/W3AAAAKuw5gYAauD48ePnXeDrdDrVrFkzL1YE4Oe4LQUANRAbG+v5MsGq9O7d+4I/0AmgbnHlBgBqYNWqVTp9+vQ532/atKkXqwFQFa7cAAAAq7CgGAAAWOWSuy3ldrv1zTffqHHjxhf19egAAMD7jDEqKipSq1atKv247s9dcuHmm2++UUxMjK/LAAAAF+HIkSOKjo4+b59LLtyc/fr2I0eOKCwszMfVAACA6igsLFRMTIznc/x8Lrlwc/ZWVFhYGOEGAAA/U50lJSwoBgAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACr+DTcfPDBBxo8eLBatWolh8OhN99884Jjtm7dquuvv14ul0vt27fXihUr6rxOAADgP3waboqLi9WlSxctXLiwWv0PHDigQYMGqW/fvsrJydFjjz2mhx56SJs3b67jSgEAgL/w6Q9nDhgwQAMGDKh2//T0dLVt21Zz586VJF199dX66KOP9N///d9KTEysqzKrzRij02fKfV0GAAA+F9wwsFo/clkX/OpXwbOyspSQkFChLTExUY899tg5x5SUlKikpMTzurCwsE5qM8bo7vQsZR/6vk72DwCAP8mdkagQp29ihl8tKM7Ly1NkZGSFtsjISBUWFur06dNVjpk9e7bCw8M9W0xMTJ3UdvpMOcEGAIB6wK+u3FyMKVOmKCUlxfO6sLCwzgLOWZ89naAQZ2CdHgMAgPosuKHvPgf9KtxERUUpPz+/Qlt+fr7CwsIUHBxc5RiXyyWXy+WN8jxCnIE+uxQHAMClzq9uS8XHxyszM7NC25YtWxQfH++jigAAQH3j03Dzww8/KCcnRzk5OZJ+etQ7JydHhw8flvTTLaXhw4d7+j/yyCPav3+/nnzySe3Zs0eLFi3S66+/rscff9wX5QMAgHrIp+Hms88+03XXXafrrrtOkpSSkqLrrrtOqampkqSjR496go4ktW3bVhs2bNCWLVvUpUsXzZ07Vy+//HK9eAwcAADUDz5dGNKnTx8ZY875flXfPtynTx/t2rWrDqsCAAD+zK/W3AAAAFwI4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBWfh5uFCxcqNjZWQUFBiouL044dO87bf/78+erQoYOCg4MVExOjxx9/XD/++KOXqgUAAPWdT8PN6tWrlZKSorS0NO3cuVNdunRRYmKijh07VmX/1157TZMnT1ZaWpp2796tV155RatXr9ZTTz3l5coBAEB95dNwM2/ePI0aNUojR45Ux44dlZ6erpCQEC1btqzK/tu3b9dNN92koUOHKjY2Vv3799eQIUMueLUHAABcOnwWbkpLS5Wdna2EhIR/FRMQoISEBGVlZVU5pmfPnsrOzvaEmf3792vjxo0aOHDgOY9TUlKiwsLCChsAALBXA18d+MSJEyovL1dkZGSF9sjISO3Zs6fKMUOHDtWJEyd08803yxijsrIyPfLII+e9LTV79mxNnz69VmsHAAD1l88XFNfE1q1bNWvWLC1atEg7d+7U2rVrtWHDBs2cOfOcY6ZMmaKCggLPduTIES9WDAAAvM1nV26aN2+uwMBA5efnV2jPz89XVFRUlWOmTZumYcOG6aGHHpIkde7cWcXFxRo9erSmTp2qgIDKWc3lcsnlctX+CQAAgHrJZ1dunE6nunXrpszMTE+b2+1WZmam4uPjqxxz6tSpSgEmMDBQkmSMqbtiAQCA3/DZlRtJSklJUXJysrp3764ePXpo/vz5Ki4u1siRIyVJw4cPV+vWrTV79mxJ0uDBgzVv3jxdd911iouL0759+zRt2jQNHjzYE3IAAMClzafhJikpScePH1dqaqry8vLUtWtXbdq0ybPI+PDhwxWu1Dz99NNyOBx6+umn9c9//lMtWrTQ4MGD9fvf/95XpwAAAOoZh7nE7ucUFhYqPDxcBQUFCgsLq7X9niotU8fUzZKk3BmJCnH6NDcCAGCVmnx++9XTUgAAABdCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWMXn4WbhwoWKjY1VUFCQ4uLitGPHjvP2P3nypMaOHauWLVvK5XLpyiuv1MaNG71ULQAAqO8a+PLgq1evVkpKitLT0xUXF6f58+crMTFRe/fuVURERKX+paWl+vWvf62IiAitWbNGrVu31qFDh9SkSRPvFw8AAOoln4abefPmadSoURo5cqQkKT09XRs2bNCyZcs0efLkSv2XLVum7777Ttu3b1fDhg0lSbGxsd4sGQAA1HM+uy1VWlqq7OxsJSQk/KuYgAAlJCQoKyuryjHr169XfHy8xo4dq8jISHXq1EmzZs1SeXn5OY9TUlKiwsLCChsAALCXz8LNiRMnVF5ersjIyArtkZGRysvLq3LM/v37tWbNGpWXl2vjxo2aNm2a5s6dq2efffacx5k9e7bCw8M9W0xMTK2eBwAAqF98vqC4JtxutyIiIrRkyRJ169ZNSUlJmjp1qtLT0885ZsqUKSooKPBsR44c8WLFAADA23y25qZ58+YKDAxUfn5+hfb8/HxFRUVVOaZly5Zq2LChAgMDPW1XX3218vLyVFpaKqfTWWmMy+WSy+Wq3eIBAEC95bMrN06nU926dVNmZqanze12KzMzU/Hx8VWOuemmm7Rv3z653W5P21dffaWWLVtWGWwAAMClx6e3pVJSUrR06VKtXLlSu3fv1qOPPqri4mLP01PDhw/XlClTPP0fffRRfffdd5o4caK++uorbdiwQbNmzdLYsWN9dQoAAKCe8emj4ElJSTp+/LhSU1OVl5enrl27atOmTZ5FxocPH1ZAwL/yV0xMjDZv3qzHH39c1157rVq3bq2JEydq0qRJvjoFAABQzziMMcbXRXhTYWGhwsPDVVBQoLCwsFrb76nSMnVM3SxJyp2RqBCnT3MjAABWqcnnt189LQUAAHAhhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAq9RauFm7dq2uvfba2todAADARalRuHnppZd09913a+jQofrkk08kSe+++66uu+46DRs2TDfddFOdFAkAAFBd1Q43c+bM0fjx43Xw4EGtX79e/fr106xZs3T//fcrKSlJ//jHP7R48eK6rBUAAOCCGlS34/Lly7V06VIlJyfrww8/VO/evbV9+3bt27dPjRo1qssaAQAAqq3aV24OHz6sfv36SZJ69eqlhg0bavr06QQbAABQr1Q73JSUlCgoKMjz2ul0qlmzZnVSFAAAwMWq9m0pSZo2bZpCQkIkSaWlpXr22WcVHh5eoc+8efNqrzoAAIAaqna4ueWWW7R3717P6549e2r//v0V+jgcjtqrDAAA4CJUO9xs3bq1DssAAACoHTW6LVVYWKhPPvlEpaWl6tGjh1q0aFFXdQEAAFyUaoebnJwcDRw4UHl5eZKkxo0b6/XXX1diYmKdFQcAAFBT1X5aatKkSWrbtq22bdum7Oxs3XrrrRo3blxd1gYAAFBj1b5yk52drbffflvXX3+9JGnZsmVq1qyZCgsLFRYWVmcFAgAA1ES1r9x89913io6O9rxu0qSJGjVqpG+//bZOCgMAALgYNVpQnJub61lzI0nGGO3evVtFRUWeNn4ZHAAA+FKNws2tt94qY0yFtttvv10Oh0PGGDkcDpWXl9dqgQAAADVR7XBz4MCBuqwDAACgVlQ73KxcuVJPPPGE5+cXAAAA6qNqLyiePn26fvjhh7qsBQAA4Berdrj5+VobAACA+qja4UbihzEBAED9V6Onpa688soLBpzvvvvuFxUEAADwS9Qo3EyfPl3h4eF1VQsAAMAvVqNwc9999ykiIqKuagEAAPjFqr3mhvU2AADAH/C0FAAAsEq1b0u53e66rAMAAKBW1OhRcAAAgPqOcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKvUi3CxcuFCxsbEKCgpSXFycduzYUa1xGRkZcjgcuvPOO+u2QAAA4Dd8Hm5Wr16tlJQUpaWlaefOnerSpYsSExN17Nix8447ePCgnnjiCfXq1ctLlQIAAH/g83Azb948jRo1SiNHjlTHjh2Vnp6ukJAQLVu27JxjysvLdf/992v69Om64oorvFgtAACo73wabkpLS5Wdna2EhARPW0BAgBISEpSVlXXOcTNmzFBERIQefPBBb5QJAAD8SANfHvzEiRMqLy9XZGRkhfbIyEjt2bOnyjEfffSRXnnlFeXk5FTrGCUlJSopKfG8LiwsvOh6AQBA/efz21I1UVRUpGHDhmnp0qVq3rx5tcbMnj1b4eHhni0mJqaOqwQAAL7k0ys3zZs3V2BgoPLz8yu05+fnKyoqqlL/r7/+WgcPHtTgwYM9bW63W5LUoEED7d27V+3ataswZsqUKUpJSfG8LiwsJOAAAGAxn4Ybp9Opbt26KTMz0/M4t9vtVmZmpsaNG1ep/1VXXaXPP/+8QtvTTz+toqIi/fGPf6wytLhcLrlcrjqpHwAA1D8+DTeSlJKSouTkZHXv3l09evTQ/PnzVVxcrJEjR0qShg8frtatW2v27NkKCgpSp06dKoxv0qSJJFVqBwAAlyafh5ukpCQdP35cqampysvLU9euXbVp0ybPIuPDhw8rIMCvlgYBAAAfchhjjK+L8KbCwkKFh4eroKBAYWFhtbbfU6Vl6pi6WZKUOyNRIU6f50YAAKxRk89vLokAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFapF+Fm4cKFio2NVVBQkOLi4rRjx45z9l26dKl69eqlpk2bqmnTpkpISDhvfwAAcGnxebhZvXq1UlJSlJaWpp07d6pLly5KTEzUsWPHquy/detWDRkyRO+9956ysrIUExOj/v3765///KeXKwcAAPWRwxhjfFlAXFycbrjhBr344ouSJLfbrZiYGI0fP16TJ0++4Pjy8nI1bdpUL774ooYPH37B/oWFhQoPD1dBQYHCwsJ+cf1nnSotU8fUzZKk3BmJCnE2qLV9AwBwqavJ57dPr9yUlpYqOztbCQkJnraAgAAlJCQoKyurWvs4deqUzpw5o2bNmtVVmQAAwI/49PLCiRMnVF5ersjIyArtkZGR2rNnT7X2MWnSJLVq1apCQPp3JSUlKikp8bwuLCy8+IIBAEC95/M1N7/EnDlzlJGRoXXr1ikoKKjKPrNnz1Z4eLhni4mJ8XKVAADAm3wabpo3b67AwEDl5+dXaM/Pz1dUVNR5x77wwguaM2eO3n77bV177bXn7DdlyhQVFBR4tiNHjtRK7QAAoH7yabhxOp3q1q2bMjMzPW1ut1uZmZmKj48/57jnn39eM2fO1KZNm9S9e/fzHsPlciksLKzCBgAA7OXzR3pSUlKUnJys7t27q0ePHpo/f76Ki4s1cuRISdLw4cPVunVrzZ49W5L03HPPKTU1Va+99ppiY2OVl5cnSQoNDVVoaKjPzgMAANQPPg83SUlJOn78uFJTU5WXl6euXbtq06ZNnkXGhw8fVkDAvy4wLV68WKWlpbr77rsr7CctLU3PPPOMN0sHAAD1kM+/58bb+J4bAAD8j998zw0AAEBtI9wAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCqEGwAAYBXCDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABWIdwAAACrEG4AAIBVCDcAAMAqhBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFapF+Fm4cKFio2NVVBQkOLi4rRjx47z9n/jjTd01VVXKSgoSJ07d9bGjRu9VCkAAKjvfB5uVq9erZSUFKWlpWnnzp3q0qWLEhMTdezYsSr7b9++XUOGDNGDDz6oXbt26c4779Sdd96pL774wsuVAwCA+shhjDG+LCAuLk433HCDXnzxRUmS2+1WTEyMxo8fr8mTJ1fqn5SUpOLiYr311luethtvvFFdu3ZVenr6BY9XWFio8PBwFRQUKCwsrNbO41RpmTqmbpYk5c5IVIizQa3tGwCAS11NPr99euWmtLRU2dnZSkhI8LQFBAQoISFBWVlZVY7Jysqq0F+SEhMTz9m/pKREhYWFFTYAAGAvn4abEydOqLy8XJGRkRXaIyMjlZeXV+WYvLy8GvWfPXu2wsPDPVtMTEztFA8AAOoln6+5qWtTpkxRQUGBZzty5EidHCe4YaByZyQqd0aighsG1skxAADAhfl0YUjz5s0VGBio/Pz8Cu35+fmKioqqckxUVFSN+rtcLrlcrtop+DwcDgfrbAAAqAd8euXG6XSqW7duyszM9LS53W5lZmYqPj6+yjHx8fEV+kvSli1bztkfAABcWnx+qSElJUXJycnq3r27evToofnz56u4uFgjR46UJA0fPlytW7fW7NmzJUkTJ05U7969NXfuXA0aNEgZGRn67LPPtGTJEl+eBgAAqCd8Hm6SkpJ0/PhxpaamKi8vT127dtWmTZs8i4YPHz6sgIB/XWDq2bOnXnvtNT399NN66qmn9Ktf/UpvvvmmOnXq5KtTAAAA9YjPv+fG2+rqe24AAEDd8ZvvuQEAAKhthBsAAGAVwg0AALAK4QYAAFiFcAMAAKxCuAEAAFYh3AAAAKsQbgAAgFUINwAAwCo+//kFbzv7hcyFhYU+rgQAAFTX2c/t6vywwiUXboqKiiRJMTExPq4EAADUVFFRkcLDw8/b55L7bSm3261vvvlGjRs3lsPhqNV9FxYWKiYmRkeOHOF3q+oQ8+wdzLN3MM/ew1x7R13NszFGRUVFatWqVYUf1K7KJXflJiAgQNHR0XV6jLCwMP7F8QLm2TuYZ+9gnr2HufaOupjnC12xOYsFxQAAwCqEGwAAYBXCTS1yuVxKS0uTy+XydSlWY569g3n2DubZe5hr76gP83zJLSgGAAB248oNAACwCuEGAABYhXADAACsQrgBAABWIdzU0MKFCxUbG6ugoCDFxcVpx44d5+3/xhtv6KqrrlJQUJA6d+6sjRs3eqlS/1aTeV66dKl69eqlpk2bqmnTpkpISLjgPxf8pKZ/ns/KyMiQw+HQnXfeWbcFWqKm83zy5EmNHTtWLVu2lMvl0pVXXsnfHdVQ03meP3++OnTooODgYMXExOjxxx/Xjz/+6KVq/dMHH3ygwYMHq1WrVnI4HHrzzTcvOGbr1q26/vrr5XK51L59e61YsaLO65RBtWVkZBin02mWLVtmvvzySzNq1CjTpEkTk5+fX2X/bdu2mcDAQPP888+b3Nxc8/TTT5uGDRuazz//3MuV+5eazvPQoUPNwoULza5du8zu3bvNiBEjTHh4uPnHP/7h5cr9S03n+awDBw6Y1q1bm169epn/+I//8E6xfqym81xSUmK6d+9uBg4caD766CNz4MABs3XrVpOTk+Plyv1LTed51apVxuVymVWrVpkDBw6YzZs3m5YtW5rHH3/cy5X7l40bN5qpU6eatWvXGklm3bp15+2/f/9+ExISYlJSUkxubq5ZsGCBCQwMNJs2barTOgk3NdCjRw8zduxYz+vy8nLTqlUrM3v27Cr733vvvWbQoEEV2uLi4szDDz9cp3X6u5rO88+VlZWZxo0bm5UrV9ZViVa4mHkuKyszPXv2NC+//LJJTk4m3FRDTed58eLF5oorrjClpaXeKtEKNZ3nsWPHmn79+lVoS0lJMTfddFOd1mmT6oSbJ5980lxzzTUV2pKSkkxiYmIdVmYMt6WqqbS0VNnZ2UpISPC0BQQEKCEhQVlZWVWOycrKqtBfkhITE8/ZHxc3zz936tQpnTlzRs2aNaurMv3exc7zjBkzFBERoQcffNAbZfq9i5nn9evXKz4+XmPHjlVkZKQ6deqkWbNmqby83Ftl+52LmeeePXsqOzvbc+tq//792rhxowYOHOiVmi8VvvocvOR+OPNinThxQuXl5YqMjKzQHhkZqT179lQ5Ji8vr8r+eXl5dVanv7uYef65SZMmqVWrVpX+hcK/XMw8f/TRR3rllVeUk5PjhQrtcDHzvH//fr377ru6//77tXHjRu3bt09jxozRmTNnlJaW5o2y/c7FzPPQoUN14sQJ3XzzzTLGqKysTI888oieeuopb5R8yTjX52BhYaFOnz6t4ODgOjkuV25glTlz5igjI0Pr1q1TUFCQr8uxRlFRkYYNG6alS5eqefPmvi7Ham63WxEREVqyZIm6deumpKQkTZ06Venp6b4uzSpbt27VrFmztGjRIu3cuVNr167Vhg0bNHPmTF+XhlrAlZtqat68uQIDA5Wfn1+hPT8/X1FRUVWOiYqKqlF/XNw8n/XCCy9ozpw5euedd3TttdfWZZl+r6bz/PXXX+vgwYMaPHiwp83tdkuSGjRooL1796pdu3Z1W7Qfupg/zy1btlTDhg0VGBjoabv66quVl5en0tJSOZ3OOq3ZH13MPE+bNk3Dhg3TQw89JEnq3LmziouLNXr0aE2dOlUBAfy3f2041+dgWFhYnV21kbhyU21Op1PdunVTZmamp83tdiszM1Px8fFVjomPj6/QX5K2bNlyzv64uHmWpOeff14zZ87Upk2b1L17d2+U6tdqOs9XXXWVPv/8c+Xk5Hi2O+64Q3379lVOTo5iYmK8Wb7fuJg/zzfddJP27dvnCY+S9NVXX6lly5YEm3O4mHk+depUpQBzNlAafnKx1vjsc7BOlytbJiMjw7hcLrNixQqTm5trRo8ebZo0aWLy8vKMMcYMGzbMTJ482dN/27ZtpkGDBuaFF14wu3fvNmlpaTwKXg01nec5c+YYp9Np1qxZY44ePerZioqKfHUKfqGm8/xzPC1VPTWd58OHD5vGjRubcePGmb1795q33nrLREREmGeffdZXp+AXajrPaWlppnHjxuYvf/mL2b9/v3n77bdNu3btzL333uurU/ALRUVFZteuXWbXrl1Gkpk3b57ZtWuXOXTokDHGmMmTJ5thw4Z5+p99FPx3v/ud2b17t1m4cCGPgtdHCxYsMJdffrlxOp2mR48e5uOPP/a817t3b5OcnFyh/+uvv26uvPJK43Q6zTXXXGM2bNjg5Yr9U03muU2bNkZSpS0tLc37hfuZmv55/neEm+qr6Txv377dxMXFGZfLZa644grz+9//3pSVlXm5av9Tk3k+c+aMeeaZZ0y7du1MUFCQiYmJMWPGjDHff/+99wv3I++9916Vf9+endvk5GTTu3fvSmO6du1qnE6nueKKK8zy5cvrvE6HMVx/AwAA9mDNDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINgHpvxIgRcjgclbZ9+/ZVeM/pdKp9+/aaMWOGysrKJP3068//PqZFixYaOHCgPv/8cx+fFYC6QrgB4Bduu+02HT16tMLWtm3bCu/9/e9/129/+1s988wz+sMf/lBh/N69e3X06FFt3rxZJSUlGjRokEpLS31xKgDqGOEGgF9wuVyKioqqsJ39Feez77Vp00aPPvqoEhIStH79+grjIyIiFBUVpeuvv16PPfaYjhw5oj179vjiVADUMcINAOsEBwef86pMQUGBMjIyJElOp9ObZQHwkga+LgAAquOtt95SaGio5/WAAQP0xhtvVOhjjFFmZqY2b96s8ePHV3gvOjpaklRcXCxJuuOOO3TVVVfVcdUAfIFwA8Av9O3bV4sXL/a8btSokef/nw0+Z86ckdvt1tChQ/XMM89UGP/hhx8qJCREH3/8sWbNmqX09HRvlQ7Aywg3APxCo0aN1L59+yrfOxt8nE6nWrVqpQYNKv/V1rZtWzVp0kQdOnTQsWPHlJSUpA8++KCuywbgA6y5AeD3zgafyy+/vMpg83Njx47VF198oXXr1nmhOgDeRrgBcMkJCQnRqFGjlJaWJmOMr8sBUMsINwAuSePGjdPu3bsrLUoG4P8chv9sAQAAFuHKDQAAsArhBgAAWIVwAwAArEK4AQAAViHcAAAAqxBuAACAVQg3AADAKoQbAABgFcINAACwCuEGAABYhXADAACsQrgBAABW+X/3L8czJFAk4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AdaBoost) Time to train 19.642624855041504\n",
      "(AdaBoost) Avg time to classify 2.9403740780619615e-05\n",
      "(AdaBoost) (X_test) accuracy, precision, recall, specificity, AUROC (0.7392414296134209, 0.22573099415204678, 0.7845528455284553, 0.7347756410256411, 0.8410823170731708)\n",
      "(AdaBoost) (X_train) accuracy, precision, recall, specificity, AUROC (1.0, 1.0, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8410823170731708"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10),n_estimators=100, algorithm=\"SAMME\", random_state=0)\n",
    "run_tests(clf, 'AdaBoost', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10edef76-7059-4d39-8d6d-1ebfd6225fa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot a loss chart\u001b[39;00m\n\u001b[1;32m      4\u001b[0m display(model\u001b[38;5;241m.\u001b[39msummary())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Plot a loss chart\n",
    "display(model.summary())\n",
    "fig, ax = plt.subplots(figsize=(16,9), dpi=300)\n",
    "plt.title(label='Model loss by Epoch', loc='center')\n",
    "ax.plot(history.history['loss'], label='Training Data', color='black')\n",
    "ax.plot(history.history['val_loss'], label='Test Data', color='red')\n",
    "ax.set(xlabel='Epoch', ylabel='Loss')\n",
    "plt.xticks(ticks=np.arange(len(history.history['loss'])), labels=np.arange(1, len(history.history['loss'])+1))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(\"train\")\n",
    "model.evaluate(X_train, y_train)\n",
    "print(\"prediction\")\n",
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb5e00-643d-4dbc-8547-08452df42fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

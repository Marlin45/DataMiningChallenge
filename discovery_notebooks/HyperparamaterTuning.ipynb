{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6e2472-feef-48f1-afff-dbd40e76c8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 16:08:01.296877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.layers import Dense, LeakyReLU, ReLU, Conv1D\n",
    "from tensorflow.keras.utils import plot_model \n",
    "from imblearn.over_sampling import SMOTE,SVMSMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d9eb5f-87ca-45a0-aeae-f030aeac189c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_demo_data(y_train, y_test):\n",
    "    X = pd.read_csv('train/train_demos.csv')\n",
    "    X = X.set_index('patient_id')\n",
    "    cat = ['gender', 'insurance', 'marital_status', 'ethnicity']\n",
    "    # ONE HOT\n",
    "    enc = OneHotEncoder()\n",
    "    X_encoded = enc.fit_transform(X[cat])\n",
    "    X_encoded = pd.DataFrame.sparse.from_spmatrix(X_encoded)\n",
    "    X_encoded.index = X.index\n",
    "\n",
    "    # LABEL\n",
    "    # encoder = LabelEncoder()\n",
    "    # X_encoded = X.copy()\n",
    "    # for var in cat:\n",
    "    #     X_encoded[var] = encoder.fit_transform(X_encoded[var])\n",
    "\n",
    "    \n",
    "    X = pd.concat([X.drop(cat, axis=1), X_encoded], axis=1)\n",
    "    X['admittime'] = X.apply(lambda x: time.mktime(pd.Timestamp(x['admittime']).timetuple()), axis=1)\n",
    "    X['admittime'] = X['admittime'] - X['admittime'].min()\n",
    "\n",
    "    X_train = split(X, y_train.index)\n",
    "    X_test = split(X, y_test.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[['age', 'admittime']])\n",
    "    X_train[['age', 'admittime']] = scaler.transform(X_train[['age', 'admittime']])\n",
    "    X_test[['age', 'admittime']] = scaler.transform(X_test[['age', 'admittime']])\n",
    "\n",
    "    X_train.columns = X_train.columns.astype('str')\n",
    "    X_test.columns = X_test.columns.astype('str')\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def prep_signs_data(y_train, y_test):\n",
    "    # CURRENTLY DROPS TIME COL\n",
    "    signs = pd.read_csv('train/train_signs.csv')\n",
    "    signs['charttime'] = pd.to_datetime(signs['charttime'])\n",
    "    \n",
    "    \n",
    "    first_time_row = signs.groupby('patient_id')['charttime'].first()\n",
    "    signs['firsttime'] = signs['patient_id'].map(first_time_row)\n",
    "    # Sets the index as the time from the first reading so all patients start at 0 and go toward 24 hours\n",
    "    signs['timediff'] = pd.to_numeric(signs['charttime'] - signs['firsttime'])\n",
    "    signs = signs.drop(['charttime','firsttime'],axis=1)\n",
    "    \n",
    "    aggs = signs.groupby('patient_id').agg(['mean', 'min', 'max', 'first', 'last'])\n",
    "    X_train = split(aggs, y_train.index)\n",
    "    X_test = split(aggs, y_test.index)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features = X_train.columns\n",
    "    id = X_train.index\n",
    "    id_test = X_test.index\n",
    "\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_train.columns = ['_'.join(x) for x in features]\n",
    "    X_train.index = id\n",
    "    X_test.columns = ['_'.join(x) for x in features]\n",
    "    X_test.index = id_test\n",
    "\n",
    "    # columns with more than 10% null values, drop these (10 columns, 2 metrics)\n",
    "    drop_cols = X_train.columns[X_train.isna().sum() / X_train.shape[0] > .1] # should be just train set\n",
    "    X_train = X_train.drop(columns=drop_cols)\n",
    "    X_test = X_test.drop(columns=drop_cols)\n",
    "\n",
    "    # is this the best way to do it??\n",
    "    X_train = X_train.fillna(X_train.mean())\n",
    "    X_test = X_test.fillna(X_train.mean()) # note train mean\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def prep_radiology_data(y_train, y_test):\n",
    "    df = pd.read_csv('train/train_radiology.csv')\n",
    "    df = df.groupby('patient_id').agg({'text': ['sum']})\n",
    "\n",
    "    \n",
    "    X_train = split(df, y_train.index)\n",
    "    X_test = split(df, y_test.index)    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, min_df=5, max_features=100, stop_words=\"english\")\n",
    "    vec_train = vectorizer.fit_transform(X_train['text']['sum'])\n",
    "    vec_test = vectorizer.transform(X_test['text']['sum'])\n",
    "    X_train = pd.concat([X_train.drop(columns=['text']), pd.DataFrame(vec_train.toarray(), index=y_train.index)], axis=1)\n",
    "    X_test = pd.concat([X_test.drop(columns=['text']), pd.DataFrame(vec_test.toarray(), index=y_test.index)], axis=1)\n",
    "    \n",
    "    # X_train[('charttime','first')] = pd.to_datetime(X_train[('charttime','first')])\n",
    "    # X_train[('charttime','last')] = pd.to_datetime(X_train[('charttime','last')])\n",
    "    # X_test[('charttime','first')] = pd.to_datetime(X_test[('charttime','first')])\n",
    "    # X_test[('charttime','last')] = pd.to_datetime(X_test[('charttime','last')])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features = X_train.columns\n",
    "    id = X_train.index\n",
    "    id_test = X_test.index\n",
    "\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_train.columns = features\n",
    "    X_train.index = id\n",
    "    X_test.columns = features\n",
    "    X_test.index = id_test\n",
    "\n",
    "    return X_train, X_test\n",
    "def train_test(balance_train=False):\n",
    "    # returns the train test split labels\n",
    "    y = pd.read_csv('train/train_labels.csv')\n",
    "    y = y.set_index('patient_id')\n",
    "    y_train = y.sample(n=int(y.shape[0] * .8))\n",
    "    y_test = y.drop(y_train.index)\n",
    "    if balance_train:\n",
    "        # RUN THIS CELL IF YOU WANT AN EVEN DATA CLASS BALANCE\n",
    "        keep_ids = y_train[y_train['label'] == 0].sample(n=y_train['label'].sum()).index\n",
    "        ys = y_train[y_train['label'] == 1].index\n",
    "        y_train = y_train.loc[[*keep_ids, *ys]]\n",
    "    return y_train, y_test\n",
    "\n",
    "def split(X, index):\n",
    "    return X.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a0f275-1ff3-4741-84f1-0de9385d383c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train, y_test = train_test(balance_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0bbef41-4a8c-4ee0-9e44-4ae2cdb6ef5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2090, 265) (2742, 265)\n"
     ]
    }
   ],
   "source": [
    "rad_train, rad_test = prep_radiology_data(y_train, y_test)\n",
    "demo_train, demo_test = prep_demo_data(y_train, y_test)\n",
    "sign_train,sign_test = prep_signs_data(y_train,y_test)\n",
    "\n",
    "X_train = pd.concat([rad_train,sign_train, demo_train], axis=1)\n",
    "X_test = pd.concat([rad_test,sign_test,  demo_test], axis=1)\n",
    "X_just_rad_train = rad_train\n",
    "X_jst_rad_test = rad_test\n",
    "X_train.columns = X_train.columns.astype('str')\n",
    "X_test.columns = X_test.columns.astype('str')\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11dd1961-4ed9-4f37-b374-a8ec487ec2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for running a random search over a keras model, acts as a first pass for model design\n",
    "auc = keras.metrics.AUC()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "def build_random_model(hp):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    j = 0\n",
    "    for i in range(1, hp.Int(\"num_layers\", 2, 4)):\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Choice(\"units_\" + str(i),[100,1000]),\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l1=1e-2,l2=1e-2),\n",
    "                activation=\"relu\",\n",
    "                name='Hidden-Layer-'+str(i))\n",
    "            )\n",
    "        model.add(keras.layers.Dropout(0.5)) \n",
    "    \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid',name='Output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=opt, loss=loss,metrics=[auc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82d5f98f-9df5-4739-8ca8-b3f9cd8bf959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 00m 27s]\n",
      "val_loss: 0.8213372230529785\n",
      "\n",
      "Best val_loss So Far: 0.6912960410118103\n",
      "Total elapsed time: 00h 18m 28s\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_random_model,\n",
    "    objective=keras_tuner.Objective('val_loss','min'),\n",
    "    max_trials=300,\n",
    "    overwrite=True,\n",
    "    directory=\"random_search\",\n",
    "    project_name=\"v1\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=200, validation_split=0.1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7de7b3-7fab-4266-9ed1-c98ca9b6644a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m auc \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mAUC()\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy()\n\u001b[0;32m----> 4\u001b[0m best_params \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_params)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_grid_model\u001b[39m(hp):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "# Second pass grid search centered on best params found in random search\n",
    "auc = keras.metrics.AUC()\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "best_params = tuner.get_best_hyperparameters()[0].values\n",
    "print(best_params)\n",
    "def build_grid_model(hp):\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    for i in range(1, best_params['num_layers']+1):\n",
    "        units = \"units_\" + str(i)\n",
    "        name = 'Hidden-Layer-'+str(i)\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(units, min_value=32, max_value=1024, step=32, default=best_params[units]),\n",
    "                kernel_regularizer=keras.regularizers.L1L2(l1=1e-4,l2=1e-3),\n",
    "                activation=\"relu\",\n",
    "                name=name,\n",
    "            ))\n",
    "        \n",
    "        # Add dropout to all layers except last one\n",
    "        if i != 5:\n",
    "            model.add(keras.layers.Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid',name='Output'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss,metrics=[auc])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052be52-e079-4238-b291-96cb7e594e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune binary cross entropy directly as that is the loss function in the model and training AUC may lead to overfitting\n",
    "tuner = keras_tuner.GridSearch(\n",
    "    build_grid_model,\n",
    "    objective=keras_tuner.Objective('val_'+loss.name,'min'),\n",
    "    max_trials=300,\n",
    "    overwrite=True,\n",
    "    directory=\"grid_search\",\n",
    "    project_name=\"v1\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=15, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf24da-f45b-49ba-921b-f2de2c725ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search result :0.873 val AUC\n",
    "# {'num_layers': 5, 'units_1': 512, 'l1': 0.0001, 'l2': 0.001, 'dropout_1': 0.1, 'units_2': 288, 'dropout_2': 0.2, 'units_3': 320, 'dropout_3': 0.1, 'units_4': 96, 'dropout_4': 0.1, 'units_5': 128, 'dropout_5': 0.0}\n",
    "best_params = tuner.get_best_hyperparameters()\n",
    "\n",
    "for params in best_params:\n",
    "    print(params.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
